{
  "query_title": "Self-Supervised Learning Approaches for 3D Point Cloud Understanding",
  "domain_terms": [
    "3d point clouds",
    "point cloud processing",
    "point cloud understanding",
    "point cloud representation",
    "unsupervised learning",
    "pretext tasks",
    "contrastive learning",
    "point cloud completion"
  ],
  "papers": [
    {
      "title": "Towards Understanding the Mechanism of Contrastive Learning via\n  Similarity Structure: A Theoretical Analysis",
      "authors": [
        "Hiroki Waida",
        "Yuichiro Wada",
        "Léo Andéol",
        "Takumi Nakagawa",
        "Yuhui Zhang",
        "Takafumi Kanamori"
      ],
      "abstract": "Contrastive learning is an efficient approach to self-supervised\nrepresentation learning. Although recent studies have made progress in the\ntheoretical understanding of contrastive learning, the investigation of how to\ncharacterize the clusters of the learned representations is still limited. In\nthis paper, we aim to elucidate the characterization from theoretical\nperspectives. To this end, we consider a kernel-based contrastive learning\nframework termed Kernel Contrastive Learning (KCL), where kernel functions play\nan important role when applying our theoretical results to other frameworks. We\nintroduce a formulation of the similarity structure of learned representations\nby utilizing a statistical dependency viewpoint. We investigate the theoretical\nproperties of the kernel-based contrastive loss via this formulation. We first\nprove that the formulation characterizes the structure of representations\nlearned with the kernel-based contrastive learning framework. We show a new\nupper bound of the classification error of a downstream task, which explains\nthat our theory is consistent with the empirical success of contrastive\nlearning. We also establish a generalization error bound of KCL. Finally, we\nshow a guarantee for the generalization ability of KCL to the downstream\nclassification task via a surrogate bound.",
      "doi": "arXiv:2304.00395v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Contrastive Abstraction for Reinforcement Learning",
      "authors": [
        "Vihang Patil",
        "Markus Hofmarcher",
        "Elisabeth Rumetshofer",
        "Sepp Hochreiter"
      ],
      "abstract": "Learning agents with reinforcement learning is difficult when dealing with\nlong trajectories that involve a large number of states. To address these\nlearning problems effectively, the number of states can be reduced by abstract\nrepresentations that cluster states. In principle, deep reinforcement learning\ncan find abstract states, but end-to-end learning is unstable. We propose\ncontrastive abstraction learning to find abstract states, where we assume that\nsuccessive states in a trajectory belong to the same abstract state. Such\nabstract states may be basic locations, achieved subgoals, inventory, or health\nconditions. Contrastive abstraction learning first constructs clusters of state\nrepresentations by contrastive learning and then applies modern Hopfield\nnetworks to determine the abstract states. The first phase of contrastive\nabstraction learning is self-supervised learning, where contrastive learning\nforces states with sequential proximity to have similar representations. The\nsecond phase uses modern Hopfield networks to map similar state representations\nto the same fixed point, i.e.\\ to an abstract state. The level of abstraction\ncan be adjusted by determining the number of fixed points of the modern\nHopfield network. Furthermore, \\textit{contrastive abstraction learning} does\nnot require rewards and facilitates efficient reinforcement learning for a wide\nrange of downstream tasks. Our experiments demonstrate the effectiveness of\ncontrastive abstraction learning for reinforcement learning.",
      "doi": "arXiv:2410.00704v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "C$^2$VAE: Gaussian Copula-based VAE Differing Disentangled from Coupled\n  Representations with Contrastive Posterior",
      "authors": [
        "Zhangkai Wu",
        "Longbing Cao"
      ],
      "abstract": "We present a self-supervised variational autoencoder (VAE) to jointly learn\ndisentangled and dependent hidden factors and then enhance disentangled\nrepresentation learning by a self-supervised classifier to eliminate coupled\nrepresentations in a contrastive manner. To this end, a Contrastive Copula VAE\n(C$^2$VAE) is introduced without relying on prior knowledge about data in the\nprobabilistic principle and involving strong modeling assumptions on the\nposterior in the neural architecture. C$^2$VAE simultaneously factorizes the\nposterior (evidence lower bound, ELBO) with total correlation (TC)-driven\ndecomposition for learning factorized disentangled representations and extracts\nthe dependencies between hidden features by a neural Gaussian copula for copula\ncoupled representations. Then, a self-supervised contrastive classifier\ndifferentiates the disentangled representations from the coupled\nrepresentations, where a contrastive loss regularizes this contrastive\nclassification together with the TC loss for eliminating entangled factors and\nstrengthening disentangled representations. C$^2$VAE demonstrates a strong\neffect in enhancing disentangled representation learning. C$^2$VAE further\ncontributes to improved optimization addressing the TC-based VAE instability\nand the trade-off between reconstruction and representation.",
      "doi": "arXiv:2309.13303v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Contrastive Representation Learning: A Framework and Review",
      "authors": [
        "Phuc H. Le-Khac",
        "Graham Healy",
        "Alan F. Smeaton"
      ],
      "abstract": "Contrastive Learning has recently received interest due to its success in\nself-supervised representation learning in the computer vision domain. However,\nthe origins of Contrastive Learning date as far back as the 1990s and its\ndevelopment has spanned across many fields and domains including Metric\nLearning and natural language processing. In this paper we provide a\ncomprehensive literature review and we propose a general Contrastive\nRepresentation Learning framework that simplifies and unifies many different\ncontrastive learning methods. We also provide a taxonomy for each of the\ncomponents of contrastive learning in order to summarise it and distinguish it\nfrom other forms of machine learning. We then discuss the inductive biases\nwhich are present in any contrastive learning system and we analyse our\nframework under different views from various sub-fields of Machine Learning.\nExamples of how contrastive learning has been applied in computer vision,\nnatural language processing, audio processing, and others, as well as in\nReinforcement Learning are also presented. Finally, we discuss the challenges\nand some of the most promising future research directions ahead.",
      "doi": "https://doi.org/10.1109/ACCESS.2020.3031549",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Contrastive self-representation learning for data clustering.",
      "authors": [
        "Zhao W",
        "Gao Q",
        "Mei S",
        "Yang M"
      ],
      "abstract": "This paper is concerned with self-representation subspace learning. It is one of the most representative subspace techniques, which has attracted considerable attention for clustering due to its good performance. Among these methods, low-rank representation (LRR) has achieved impressive results for subspace clustering. However, it only considers the similarity between the data itself, while neglecting the differences with other samples. Besides, it cannot well deal with noise and portray cluster-to-cluster relationships well. To solve these problems, we propose a Contrastive Self-representation model for Clustering (CSC). CSC simultaneously takes into account the similarity/dissimilarity between positive/negative pairs when learning the self-representation coefficient matrix of data while the form of the loss function can reduce the effect of noise on the results. Moreover, We use the ℓ",
      "doi": "https://doi.org/10.1016/j.neunet.2023.08.050",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Action-driven contrastive representation for reinforcement learning.",
      "authors": [
        "Kim M",
        "Rho K",
        "Kim YD",
        "Jung K"
      ],
      "abstract": "In reinforcement learning, reward-driven feature learning directly from high-dimensional images faces two challenges: sample-efficiency for solving control tasks and generalization to unseen observations. In prior works, these issues have been addressed through learning representation from pixel inputs. However, their representation faced the limitations of being vulnerable to the high diversity inherent in environments or not taking the characteristics for solving control tasks. To attenuate these phenomena, we propose the novel contrastive representation method, Action-Driven Auxiliary Task (ADAT), which forces a representation to concentrate on essential features for deciding actions and ignore control-irrelevant details. In the augmented state-action dictionary of ADAT, the agent learns representation to maximize agreement between observations sharing the same actions. The proposed method significantly outperforms model-free and model-based algorithms in the Atari and OpenAI ProcGen, widely used benchmarks for sample-efficiency and generalization.",
      "doi": "https://doi.org/10.1371/journal.pone.0265456",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Contrastive representation learning on dynamic networks.",
      "authors": [
        "Jiao P",
        "Chen H",
        "Tang H",
        "Bao Q",
        "Zhang L",
        "Zhao Z",
        "Wu H"
      ],
      "abstract": "Representation learning for dynamic networks is designed to learn the low-dimensional embeddings of nodes that can well preserve the snapshot structure, properties and temporal evolution of dynamic networks. However, current dynamic network representation learning methods tend to focus on estimating or generating observed snapshot structures, paying excessive attention to network details, and disregarding distinctions between snapshots with larger time intervals, resulting in less robustness for sparse or noisy networks. To alleviate these challenges, this paper proposes a contrastive mechanism for temporal representation learning on dynamic networks, inspired by the success of contrastive learning in visual and static network representation learning. This paper proposes a novel Dynamic Network Contrastive representation Learning (DNCL) model. Specifically, contrast objective functions are constructed using intra-snapshot and inter-snapshot contrasts to capture the network topology, node feature information, and network evolution information, respectively. Rather than estimating or generating ground-truth network features, the proposed approach maximizes mutual information between nodes from different time steps and views generated. The experimental results of link prediction, node classification, and clustering on several real-world and synthetic networks demonstrate the superiority of DNCL over state-of-the-art methods, indicating the effectiveness of the proposed approach for dynamic network representation learning.",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106240",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Consistent representation via contrastive learning for skin lesion diagnosis.",
      "authors": [
        "Wang Z",
        "Zhang L",
        "Shu X",
        "Wang Y",
        "Feng Y"
      ],
      "abstract": "Skin lesions are a prevalent ailment, with melanoma emerging as a particularly perilous variant. Encouragingly, artificial intelligence displays promising potential in early detection, yet its integration within clinical contexts, particularly involving multi-modal data, presents challenges. While multi-modal approaches enhance diagnostic efficacy, the influence of modal bias is often disregarded. In this investigation, a multi-modal feature learning technique termed \"Contrast-based Consistent Representation Disentanglement\" for dermatological diagnosis is introduced. This approach employs adversarial domain adaptation to disentangle features from distinct modalities, fostering a shared representation. Furthermore, a contrastive learning strategy is devised to incentivize the model to preserve uniformity in common lesion attributes across modalities. Emphasizing the learning of a uniform representation among models, this approach circumvents reliance on supplementary data. Assessment of the proposed technique on a 7-point criteria evaluation dataset yields an average accuracy of 76.1% for multi-classification tasks, surpassing researched state-of-the-art methods. The approach tackles modal bias, enabling the acquisition of a consistent representation of common lesion appearances across diverse modalities, which transcends modality boundaries. This study underscores the latent potential of multi-modal feature learning in dermatological diagnosis. In summation, a multi-modal feature learning strategy is posited for dermatological diagnosis. This approach outperforms other state-of-the-art methods, underscoring its capacity to enhance diagnostic precision for skin lesions.",
      "doi": "https://doi.org/10.1016/j.cmpb.2023.107826",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent\n  Reinforcement Learning",
      "authors": [
        "Violet Xiang",
        "Logan Cross",
        "Jan-Philipp Fränken",
        "Nick Haber"
      ],
      "abstract": "In real-world environments, autonomous agents rely on their egocentric\nobservations. They must learn adaptive strategies to interact with others who\npossess mixed motivations, discernible only through visible cues. Several\nMulti-Agent Reinforcement Learning (MARL) methods adopt centralized approaches\nthat involve either centralized training or reward-sharing, often violating the\nrealistic ways in which living organisms, like animals or humans, process\ninformation and interact. MARL strategies deploying decentralized training with\nintrinsic motivation offer a self-supervised approach, enable agents to develop\nflexible social strategies through the interaction of autonomous agents.\nHowever, by contrasting the self-supervised and centralized methods, we reveal\nthat populations trained with reward-sharing methods surpass those using\nself-supervised methods in a mixed-motive environment. We link this superiority\nto specialized role emergence and an agent's expertise in its role.\nInterestingly, this gap shrinks in pure-motive settings, emphasizing the need\nfor evaluations in more complex, realistic environments (mixed-motive). Our\npreliminary results suggest a gap in population performance that can be closed\nby improving self-supervised methods and thereby pushing MARL closer to\nreal-world readiness.",
      "doi": "arXiv:2312.08662v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled\n  Autoencoder for Mixed Tabular Datasets",
      "authors": [
        "Samuel Stocksieker",
        "Denys Pommeret",
        "Arthur Charpentier"
      ],
      "abstract": "The field of imbalanced self-supervised learning, especially in the context\nof tabular data, has not been extensively studied. Existing research has\npredominantly focused on image datasets. This paper aims to fill this gap by\nexamining the specific challenges posed by data imbalance in self-supervised\nlearning in the domain of tabular data, with a primary focus on autoencoders.\nAutoencoders are widely employed for learning and constructing a new\nrepresentation of a dataset, particularly for dimensionality reduction. They\nare also often used for generative model learning, as seen in variational\nautoencoders. When dealing with mixed tabular data, qualitative variables are\noften encoded using a one-hot encoder with a standard loss function (MSE or\nCross Entropy). In this paper, we analyze the drawbacks of this approach,\nespecially when categorical variables are imbalanced. We propose a novel metric\nto balance learning: a Multi-Supervised Balanced MSE. This approach reduces the\nreconstruction error by balancing the influence of variables. Finally, we\nempirically demonstrate that this new metric, compared to the standard MSE: i)\noutperforms when the dataset is imbalanced, especially when the learning\nprocess is insufficient, and ii) provides similar results in the opposite case.",
      "doi": "arXiv:2403.15790v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language\n  Models into Universal Lexical and Sentence Encoders",
      "authors": [
        "Fangyu Liu",
        "Ivan Vulić",
        "Anna Korhonen",
        "Nigel Collier"
      ],
      "abstract": "Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30\nseconds without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective universal lexical and sentence encoders.",
      "doi": "arXiv:2104.08027v2",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "An Effective Deployment of Contrastive Learning in Multi-label Text\n  Classification",
      "authors": [
        "Nankai Lin",
        "Guanqiu Qin",
        "Jigang Wang",
        "Aimin Yang",
        "Dong Zhou"
      ],
      "abstract": "The effectiveness of contrastive learning technology in natural language\nprocessing tasks is yet to be explored and analyzed. How to construct positive\nand negative samples correctly and reasonably is the core challenge of\ncontrastive learning. It is even harder to discover contrastive objects in\nmulti-label text classification tasks. There are very few contrastive losses\nproposed previously. In this paper, we investigate the problem from a different\nangle by proposing five novel contrastive losses for multi-label text\nclassification tasks. These are Strict Contrastive Loss (SCL), Intra-label\nContrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard\nSimilarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive\nLoss (SLCL). We explore the effectiveness of contrastive learning for\nmulti-label text classification tasks by the employment of these novel losses\nand provide a set of baseline models for deploying contrastive learning\ntechniques on specific tasks. We further perform an interpretable analysis of\nour approach to show how different components of contrastive learning losses\nplay their roles. The experimental results show that our proposed contrastive\nlosses can bring improvement to multi-label text classification tasks. Our work\nalso explores how contrastive learning should be adapted for multi-label text\nclassification tasks.",
      "doi": "arXiv:2212.00552v3",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Local contrastive loss with pseudo-label based self-training for semi-supervised medical image segmentation.",
      "authors": [
        "Chaitanya K",
        "Erdil E",
        "Karani N",
        "Konukoglu E"
      ],
      "abstract": "Supervised deep learning-based methods yield accurate results for medical image segmentation. However, they require large labeled datasets for this, and obtaining them is a laborious task that requires clinical expertise. Semi/self-supervised learning-based approaches address this limitation by exploiting unlabeled data along with limited annotated data. Recent self-supervised learning methods use contrastive loss to learn good global level representations from unlabeled images and achieve high performance in classification tasks on popular natural image datasets like ImageNet. In pixel-level prediction tasks such as segmentation, it is crucial to also learn good local level representations along with global representations to achieve better accuracy. However, the impact of the existing local contrastive loss-based methods remains limited for learning good local representations because similar and dissimilar local regions are defined based on random augmentations and spatial proximity; not based on the semantic label of local regions due to lack of large-scale expert annotations in the semi/self-supervised setting. In this paper, we propose a local contrastive loss to learn good pixel level features useful for segmentation by exploiting semantic label information obtained from pseudo-labels of unlabeled images alongside limited annotated images with ground truth (GT) labels. In particular, we define the proposed contrastive loss to encourage similar representations for the pixels that have the same pseudo-label/GT label while being dissimilar to the representation of pixels with different pseudo-label/GT label in the dataset. We perform pseudo-label based self-training and train the network by jointly optimizing the proposed contrastive loss on both labeled and unlabeled sets and segmentation loss on only the limited labeled set. We evaluated the proposed approach on three public medical datasets of cardiac and prostate anatomies, and obtain high segmentation performance with a limited labeled set of one or two 3D volumes. Extensive comparisons with the state-of-the-art semi-supervised and data augmentation methods and concurrent contrastive learning methods demonstrate the substantial improvement achieved by the proposed method. The code is made publicly available at https://github.com/krishnabits001/pseudo_label_contrastive_training.",
      "doi": "https://doi.org/10.1016/j.media.2023.102792",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Self-Supervised Contrastive Learning for Medical Time Series: A Systematic Review.",
      "authors": [
        "Liu Z",
        "Alavi A",
        "Li M",
        "Zhang X"
      ],
      "abstract": "Medical time series are sequential data collected over time that measures health-related signals, such as electroencephalography (EEG), electrocardiography (ECG), and intensive care unit (ICU) readings. Analyzing medical time series and identifying the latent patterns and trends that lead to uncovering highly valuable insights for enhancing diagnosis, treatment, risk assessment, and disease progression. However, data mining in medical time series is heavily limited by the sample annotation which is time-consuming and labor-intensive, and expert-depending. To mitigate this challenge, the emerging self-supervised contrastive learning, which has shown great success since 2020, is a promising solution. Contrastive learning aims to learn representative embeddings by contrasting positive and negative samples without the requirement for explicit labels. Here, we conducted a systematic review of how contrastive learning alleviates the label scarcity in medical time series based on PRISMA standards. We searched the studies in five scientific databases (IEEE, ACM, Scopus, Google Scholar, and PubMed) and retrieved 1908 papers based on the inclusion criteria. After applying excluding criteria, and screening at title, abstract, and full text levels, we carefully reviewed 43 papers in this area. Specifically, this paper outlines the pipeline of contrastive learning, including pre-training, fine-tuning, and testing. We provide a comprehensive summary of the various augmentations applied to medical time series data, the architectures of pre-training encoders, the types of fine-tuning classifiers and clusters, and the popular contrastive loss functions. Moreover, we present an overview of the different data types used in medical time series, highlight the medical applications of interest, and provide a comprehensive table of 51 public datasets that have been utilized in this field. In addition, this paper will provide a discussion on the promising future scopes such as providing guidance for effective augmentation design, developing a unified framework for analyzing hierarchical time series, and investigating methods for processing multimodal data. Despite being in its early stages, self-supervised contrastive learning has shown great potential in overcoming the need for expert-created annotations in the research of medical time series.",
      "doi": "https://doi.org/10.3390/s23094221",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Self-Supervised Contrastive Representation Learning for Semi-Supervised Time-Series Classification.",
      "authors": [
        "Eldele E",
        "Ragab M",
        "Chen Z",
        "Wu M",
        "Kwoh CK",
        "Li X",
        "Guan C"
      ],
      "abstract": "Learning time-series representations when only unlabeled data or few labeled samples are available can be a challenging task. Recently, contrastive self-supervised learning has shown great improvement in extracting useful representations from unlabeled data via contrasting different augmented views of data. In this work, we propose a novel Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC) that learns representations from unlabeled data with contrastive learning. Specifically, we propose time-series-specific weak and strong augmentations and use their views to learn robust temporal relations in the proposed temporal contrasting module, besides learning discriminative representations by our proposed contextual contrasting module. Additionally, we conduct a systematic study of time-series data augmentation selection, which is a key part of contrastive learning. We also extend TS-TCC to the semi-supervised learning settings and propose a Class-Aware TS-TCC (CA-TCC) that benefits from the available few labeled data to further improve representations learned by TS-TCC. Specifically, we leverage the robust pseudo labels produced by TS-TCC to realize a class-aware contrastive loss. Extensive experiments show that the linear evaluation of the features learned by our proposed framework performs comparably with the fully supervised training. Additionally, our framework shows high efficiency in few labeled data and transfer learning scenarios.",
      "doi": "https://doi.org/10.1109/tpami.2023.3308189",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "DC-SiamNet: Deep contrastive Siamese network for self-supervised MRI reconstruction.",
      "authors": [
        "Yan Y",
        "Yang T",
        "Zhao X",
        "Jiao C",
        "Yang A",
        "Miao J"
      ],
      "abstract": "Reconstruction methods based on deep learning have greatly shortened the data acquisition time of magnetic resonance imaging (MRI). However, these methods typically utilize massive fully sampled data for supervised training, restricting their application in certain clinical scenarios and posing challenges to the reconstruction effect when high-quality MR images are unavailable. Recently, self-supervised methods have been developed that only undersampled MRI images participate in the network training. Nevertheless, due to the lack of complete referable MR image data, self-supervised reconstruction is prone to produce incorrect structure contents, such as unnatural texture details and over-smoothed tissue sites. To solve this problem, we propose a self-supervised Deep Contrastive Siamese Network (DC-SiamNet) for fast MR imaging. First, DC-SiamNet performs the reconstruction with a Siamese unrolled structure and obtains visual representations in different iterative phases. Particularly, an attention-weighted average pooling module is employed at the bottleneck layer of the U-shape regularization unit, which can effectively aggregate valuable local information of the underlying feature map in the generated representation vector. Then, a novel hybrid loss function is designed to drive the self-supervised reconstruction and contrastive learning simultaneously by forcing the output consistency across different branches in the frequency domain, the image domain, and the latent space. The proposed method is extensively evaluated with different sampling patterns on the IXI brain dataset and the MRINet knee dataset. Experimental results show that DC-SiamNet can achieve 0.93 in structural similarity and 33.984 dB in peak signal-to-noise ratio on the IXI brain dataset under 8x acceleration. It has better reconstruction accuracy than other methods, and the performance is close to the corresponding model trained with full supervision, especially when the sampling rate is low. In addition, generalization experiments verify that our method has a strong cross-domain reconstruction ability for different contrast brain images.",
      "doi": "https://doi.org/10.1016/j.compbiomed.2023.107619",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "KnowAugNet: Multi-Source Medical Knowledge Augmented Medication\n  Prediction Network with Multi-Level Graph Contrastive Learning",
      "authors": [
        "Yang An",
        "Bo Jin",
        "Xiaopeng Wei"
      ],
      "abstract": "Predicting medications is a crucial task in many intelligent healthcare\nsystems. It can assist doctors in making informed medication decisions for\npatients according to electronic medical records (EMRs). However, medication\nprediction is a challenging data mining task due to the complex relations\nbetween medical codes. Most existing studies focus on utilizing inherent\nrelations between homogeneous codes of medical ontology graph to enhance their\nrepresentations using supervised methods, and few studies pay attention to the\nvaluable relations between heterogeneous or homogeneous medical codes from\nhistory EMRs, which further limits the prediction performance and application\nscenarios. Therefore, to address these limitations, this paper proposes\nKnowAugNet, a multi-sourced medical knowledge augmented medication prediction\nnetwork which can fully capture the diverse relations between medical codes via\nmulti-level graph contrastive learning framework. Specifically, KnowAugNet\nfirst leverages the graph contrastive learning using graph attention network as\nthe encoder to capture the implicit relations between homogeneous medical codes\nfrom the medical ontology graph and obtains the knowledge augmented medical\ncodes embedding vectors. Then, it utilizes the graph contrastive learning using\na weighted graph convolutional network as the encoder to capture the\ncorrelative relations between homogeneous or heterogeneous medical codes from\nthe constructed medical prior relation graph and obtains the relation augmented\nmedical codes embedding vectors. Finally, the augmented medical codes embedding\nvectors and the supervised medical codes embedding vectors are retrieved and\ninput to the sequential learning network to capture the temporal relations of\nmedical codes and predict medications for patients.",
      "doi": "arXiv:2204.11736v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Contrastive Difference Predictive Coding",
      "authors": [
        "Chongyi Zheng",
        "Ruslan Salakhutdinov",
        "Benjamin Eysenbach"
      ],
      "abstract": "Predicting and reasoning about the future lie at the heart of many\ntime-series questions. For example, goal-conditioned reinforcement learning can\nbe viewed as learning representations to predict which states are likely to be\nvisited in the future. While prior methods have used contrastive predictive\ncoding to model time series data, learning representations that encode\nlong-term dependencies usually requires large amounts of data. In this paper,\nwe introduce a temporal difference version of contrastive predictive coding\nthat stitches together pieces of different time series data to decrease the\namount of data required to learn predictions of future events. We apply this\nrepresentation learning method to derive an off-policy algorithm for\ngoal-conditioned RL. Experiments demonstrate that, compared with prior RL\nmethods, ours achieves $2 \\times$ median improvement in success rates and can\nbetter cope with stochastic environments. In tabular settings, we show that our\nmethod is about $20 \\times$ more sample efficient than the successor\nrepresentation and $1500 \\times$ more sample efficient than the standard (Monte\nCarlo) version of contrastive predictive coding.",
      "doi": "arXiv:2310.20141v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Contrastive Predictive Coding Based Feature for Automatic Speaker\n  Verification",
      "authors": [
        "Cheng-I Lai"
      ],
      "abstract": "This thesis describes our ongoing work on Contrastive Predictive Coding (CPC)\nfeatures for speaker verification. CPC is a recently proposed representation\nlearning framework based on predictive coding and noise contrastive estimation.\nWe focus on incorporating CPC features into the standard automatic speaker\nverification systems, and we present our methods, experiments, and analysis.\nThis thesis also details necessary background knowledge in past and recent work\non automatic speaker verification systems, conventional speech features, and\nthe motivation and techniques behind CPC.",
      "doi": "arXiv:1904.01575v1",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code\n  Representation",
      "authors": [
        "Xin Wang",
        "Yasheng Wang",
        "Fei Mi",
        "Pingyi Zhou",
        "Yao Wan",
        "Xiao Liu",
        "Li Li",
        "Hao Wu",
        "Jin Liu",
        "Xin Jiang"
      ],
      "abstract": "Code representation learning, which aims to encode the semantics of source\ncode into distributed vectors, plays an important role in recent\ndeep-learning-based models for code intelligence. Recently, many pre-trained\nlanguage models for source code (e.g., CuBERT and CodeBERT) have been proposed\nto model the context of code and serve as a basis for downstream code\nintelligence tasks such as code search, code clone detection, and program\ntranslation. Current approaches typically consider the source code as a plain\nsequence of tokens, or inject the structure information (e.g., AST and\ndata-flow) into the sequential model pre-training. To further explore the\nproperties of programming languages, this paper proposes SynCoBERT, a\nsyntax-guided multi-modal contrastive pre-training approach for better code\nrepresentations. Specially, we design two novel pre-training objectives\noriginating from the symbolic and syntactic properties of source code, i.e.,\nIdentifier Prediction (IP) and AST Edge Prediction (TEP), which are designed to\npredict identifiers, and edges between two nodes of AST, respectively.\nMeanwhile, to exploit the complementary information in semantically equivalent\nmodalities (i.e., code, comment, AST) of the code, we propose a multi-modal\ncontrastive learning strategy to maximize the mutual information among\ndifferent modalities. Extensive experiments on four downstream tasks related to\ncode intelligence show that SynCoBERT advances the state-of-the-art with the\nsame pre-training corpus and model size.",
      "doi": "arXiv:2108.04556v3",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Six-Minute Walk Test: Clinical Role, Technique, Coding, and Reimbursement.",
      "authors": [
        "Agarwala P",
        "Salzman SH"
      ],
      "abstract": "The 6-min walk test (6MWT) is a commonly used test for the objective assessment of functional exercise capacity for the management of patients with moderate-to-severe pulmonary disease. Unlike pulmonary function testing, the 6MWT captures the often coexisting extrapulmonary manifestations of chronic respiratory disease, including cardiovascular disease, frailty, sarcopenia, and cancer. In contrast with cardiopulmonary exercise stress testing, this test does not require complex equipment or technical expertise. In this low complexity, safe test, the patient is asked to walk as far as possible along a 30-m minimally trafficked corridor for a period of 6 min with the primary outcome measure being the 6-min walk distance (6MWD) measured in meters. There has been interest in other derived indexes, such as distance-desaturation product (the product of nadir oxygen saturation and walk distance), which in small studies has been predictive of morbidity and mortality in certain chronic respiratory conditions. Special attention to methodology is required to produce reliable and reproducible results. Factors that can affect walk distance include track layout (continuous vs straight), track length, oxygen amount and portability, learning effect, and verbal encouragement. The absolute 6MWD and change in 6MWD are predictive of morbidity and mortality in patients with COPD, pulmonary arterial hypertension, and idiopathic pulmonary fibrosis and patients awaiting lung transplant, highlighting its use in management decisions and clinical trials. As of January 2018, Current Procedural Terminology code 94620 (simple pulmonary stress test) has been deleted and replaced by two new codes, 94617 and 94618. Code 94617 includes exercise test for bronchospasm including pre- and postspirometry, ECG recordings, and pulse oximetry. Code 94618, pulmonary stress testing (eg, 6MWT), includes the measurement of heart rate, oximetry, and oxygen titration when performed. If 94620 is billed after January 2018 it will not be reimbursed.",
      "doi": "https://doi.org/10.1016/j.chest.2019.10.014",
      "year": 2020,
      "source": "PubMed"
    },
    {
      "title": "A Contrastive Predictive Coding-Based Classification Framework for Healthcare Sensor Data.",
      "authors": [
        "Ren C",
        "Sun L",
        "Peng D"
      ],
      "abstract": "Supervised learning technologies have been used in medical-data classification to improve diagnosis efficiency and reduce human diagnosis errors. A large amount of manually annotated data are required for the fully supervised learning process. However, annotating data information will consume a large amount of manpower and resources. Self-supervised learning has great advantages in solving this problem. Self-supervised learning mainly uses pretext tasks to mine its own supervised information from large-scale unsupervised data. And this constructed supervised information is used to train the network to learn valuable representations for downstream tasks. This study designs a general and efficient model for the diagnosis and classification of medical sensor data based on contrastive predictive coding (CPC) in self-supervised learning, called TCC, which consists of two steps. The first step is to design a pretext task based on the idea of CPC, which aims to extract effective features between different categories using its encoder. The second step designs a downstream classification task with lower time and space complexity to perform a supervised type of training using the features extracted by the encoder of the pretext task. Finally, to demonstrate the performance of the proposed framework in this paper, we compare the proposed framework with recent state-of-the-art works. Experiments comparing the proposed framework with supervised learning are also set up under the condition of different proportions of labeled data.",
      "doi": "https://doi.org/10.1155/2022/5649253",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Conditional Contrastive Predictive Coding for Assessment of Fetal Health From the Cardiotocogram.",
      "authors": [
        "de Vries IR",
        "Melaet R",
        "Huijben IAM",
        "van Laar JOEH",
        "Kok RD",
        "Oei SG",
        "van Sloun RJG",
        "Vullings R"
      ],
      "abstract": "Fetal well-being during labor is currently assessed by medical professionals through visual interpretation of the cardiotocogram, a simultaneous recording of Fetal Heart Rate and Uterine Activity. This method is disputed due to high inter- and intra-observer variability and a resulting high number of unnecessary interventions. Recently, an unsupervised deep learning model for automated anomaly detection in the cardiotocogram was presented. Anomalies were defined as out-of-distribution behaviour or deviations from subject-specific behaviour and the model was based on the WaveNet architecture, but required a two-step training. The current work improves this previous work by leveraging Contrastive Predictive Coding (CPC), which uses a contrastive loss to make latent predictions without requiring a decoder network. In this work, CPC was extended with a stochastic, recurrent, and conditioned (upon Uterine Activity) future predictor. We, moreover, introduce a new training objective that was found better suitable for the task of anomaly detection. Evaluated on annotations made by experienced gynecologists, all proposed extensions were shown to be beneficial, and the proposed method is shown to rival or outperform the WaveNet-based method on different annotation categories.",
      "doi": "https://doi.org/10.1109/jbhi.2025.3530610",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "Self-Supervised EEG Representation Learning with Contrastive Predictive Coding for Post-Stroke Patients.",
      "authors": [
        "Xu F",
        "Yan Y",
        "Zhu J",
        "Chen X",
        "Gao L",
        "Liu Y",
        "Shi W",
        "Lou Y",
        "Wang W",
        "Leng J",
        "Zhang Y"
      ],
      "abstract": "Stroke patients are prone to fatigue during the EEG acquisition procedure, and experiments have high requirements on cognition and physical limitations of subjects. Therefore, how to learn effective feature representation is very important. Deep learning networks have been widely used in motor imagery (MI) based brain-computer interface (BCI). This paper proposes a contrast predictive coding (CPC) framework based on the modified s-transform (MST) to generate MST-CPC feature representations. MST is used to acquire the temporal-frequency feature to improve the decoding performance for MI task recognition. EEG2Image is used to convert multi-channel one-dimensional EEG into two-dimensional EEG topography. High-level feature representations are generated by CPC which consists of an encoder and autoregressive model. Finally, the effectiveness of generated features is verified by the",
      "doi": "https://doi.org/10.1142/s0129065723500661",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Improving the Modality Representation with Multi-View Contrastive\n  Learning for Multimodal Sentiment Analysis",
      "authors": [
        "Peipei Liu",
        "Xin Zheng",
        "Hong Li",
        "Jie Liu",
        "Yimo Ren",
        "Hongsong Zhu",
        "Limin Sun"
      ],
      "abstract": "Modality representation learning is an important problem for multimodal\nsentiment analysis (MSA), since the highly distinguishable representations can\ncontribute to improving the analysis effect. Previous works of MSA have usually\nfocused on multimodal fusion strategies, and the deep study of modal\nrepresentation learning was given less attention. Recently, contrastive\nlearning has been confirmed effective at endowing the learned representation\nwith stronger discriminate ability. Inspired by this, we explore the\nimprovement approaches of modality representation with contrastive learning in\nthis study. To this end, we devise a three-stages framework with multi-view\ncontrastive learning to refine representations for the specific objectives. At\nthe first stage, for the improvement of unimodal representations, we employ the\nsupervised contrastive learning to pull samples within the same class together\nwhile the other samples are pushed apart. At the second stage, a\nself-supervised contrastive learning is designed for the improvement of the\ndistilled unimodal representations after cross-modal interaction. At last, we\nleverage again the supervised contrastive learning to enhance the fused\nmultimodal representation. After all the contrast trainings, we next achieve\nthe classification task based on frozen representations. We conduct experiments\non three open datasets, and results show the advance of our model.",
      "doi": "arXiv:2210.15824v3",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Task-Induced Representation Learning",
      "authors": [
        "Jun Yamada",
        "Karl Pertsch",
        "Anisha Gunjal",
        "Joseph J. Lim"
      ],
      "abstract": "In this work, we evaluate the effectiveness of representation learning\napproaches for decision making in visually complex environments. Representation\nlearning is essential for effective reinforcement learning (RL) from\nhigh-dimensional inputs. Unsupervised representation learning approaches based\non reconstruction, prediction or contrastive learning have shown substantial\nlearning efficiency gains. Yet, they have mostly been evaluated in clean\nlaboratory or simulated settings. In contrast, real environments are visually\ncomplex and contain substantial amounts of clutter and distractors.\nUnsupervised representations will learn to model such distractors, potentially\nimpairing the agent's learning efficiency. In contrast, an alternative class of\napproaches, which we call task-induced representation learning, leverages task\ninformation such as rewards or demonstrations from prior tasks to focus on\ntask-relevant parts of the scene and ignore distractors. We investigate the\neffectiveness of unsupervised and task-induced representation learning\napproaches on four visually complex environments, from Distracting DMControl to\nthe CARLA driving simulator. For both, RL and imitation learning, we find that\nrepresentation learning generally improves sample efficiency on unseen tasks\neven in visually complex scenes and that task-induced representations can\ndouble learning efficiency compared to unsupervised alternatives. Code is\navailable at https://clvrai.com/tarp.",
      "doi": "arXiv:2204.11827v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Transferability of Representations Learned using Supervised Contrastive\n  Learning Trained on a Multi-Domain Dataset",
      "authors": [
        "Alvin De Jun Tan",
        "Clement Tan",
        "Chai Kiat Yeo"
      ],
      "abstract": "Contrastive learning has shown to learn better quality representations than\nmodels trained using cross-entropy loss. They also transfer better to\ndownstream datasets from different domains. However, little work has been done\nto explore the transferability of representations learned using contrastive\nlearning when trained on a multi-domain dataset. In this paper, a study has\nbeen conducted using the Supervised Contrastive Learning framework to learn\nrepresentations from the multi-domain DomainNet dataset and then evaluate the\ntransferability of the representations learned on other downstream datasets.\nThe fixed feature linear evaluation protocol will be used to evaluate the\ntransferability on 7 downstream datasets that were chosen across different\ndomains. The results obtained are compared to a baseline model that was trained\nusing the widely used cross-entropy loss. Empirical results from the\nexperiments showed that on average, the Supervised Contrastive Learning model\nperformed 6.05% better than the baseline model on the 7 downstream datasets.\nThe findings suggest that Supervised Contrastive Learning models can\npotentially learn more robust representations that transfer better across\ndomains than cross-entropy models when trained on a multi-domain dataset.",
      "doi": "arXiv:2309.15486v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Local structure-aware graph contrastive representation learning.",
      "authors": [
        "Yang K",
        "Liu Y",
        "Zhao Z",
        "Ding P",
        "Zhao W"
      ],
      "abstract": "Traditional Graph Neural Network (GNN), as a graph representation learning method, is constrained by label information. However, Graph Contrastive Learning (GCL) methods, which tackles the label problem effectively, mainly focus on the feature information of the global graph or small subgraph structure (e.g., the first-order neighborhood). In this paper, we propose a Local Structure-aware Graph Contrastive representation Learning method (LS-GCL) to model the structural information of nodes from multiple views. Specifically, we construct the semantic subgraphs that are not limited to the first-order neighbors. For the local view, the semantic subgraph of each target node is input into a shared GNN encoder to obtain the target node embeddings at the subgraph-level. Then, we use a pooling function to generate the subgraph-level graph embeddings. For the global view, considering the original graph preserves indispensable semantic information of nodes, we leverage the shared GNN encoder to learn the target node embeddings at the global graph-level. The proposed LS-GCL model is optimized to maximize the common information among similar instances at three various perspectives through a multi-level contrastive loss function. Experimental results on six datasets illustrate that our method outperforms state-of-the-art graph representation learning approaches for both node classification and link prediction tasks.",
      "doi": "https://doi.org/10.1016/j.neunet.2023.12.037",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "X-Invariant Contrastive Augmentation and Representation Learning for Semi-Supervised Skeleton-Based Action Recognition.",
      "authors": [
        "Xu B",
        "Shu X",
        "Song Y"
      ],
      "abstract": "Semi-supervised skeleton-based action recognition is a challenging problem due to insufficient labeled data. For addressing this problem, some representative methods leverage contrastive learning to obtain more features from the pre-augmented skeleton actions. Such methods usually adopt a two-stage way: first randomly augment samples, and then learn their representations via contrastive learning. Since skeleton samples have already been randomly augmented, the representation ability of the subsequent contrastive learning is limited due to the inconsistency between the augmentations and representations. Thus, we propose a novel X-invariant Contrastive Augmentation and Representation learning (X-CAR) framework to thoroughly obtain rotate-shear-scale (X for short) invariant features by learning augmentations and representations of skeleton sequences in a one-stage way. In X-CAR, a new Adaptive-combination Augmentation (AA) mechanism is designed to rotate, shear, and scale the skeletons by learnable controlling factors in an adaptive way rather than a random way. Here, such controlling factors are also learned in the whole contrastive learning process, which can facilitate the consistency between the learned augmentations and representations of skeleton sequences. In addition, we relax the pre-definition of positive and negative samples to avoid the confusing allocation of ambiguous samples, and present a new Pull-Push Contrastive Loss (PPCL) to pull the augmenting skeleton close to the original skeleton, while push far away from the other skeletons. Experimental results on both NTU RGB+D and North-Western UCLA datasets show that the proposed X-CAR achieves better accuracy compared with other competitive methods in the semi-supervised scenario.",
      "doi": "https://doi.org/10.1109/tip.2022.3175605",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Contrastive and adversarial regularized multi-level representation learning for incomplete multi-view clustering.",
      "authors": [
        "Wang H",
        "Zhang W",
        "Ma X"
      ],
      "abstract": "Incomplete multi-view clustering is a significant task in machine learning, given that complex systems in nature and society cannot be fully observed; it provides an opportunity to exploit the structure and functions of underlying systems. Current algorithms are criticized for failing either to balance data restoration and clustering or to capture the consistency of the representation of various views. To address these problems, a novel Multi-level Representation Learning Contrastive and Adversarial Learning (aka MRL_CAL) for incomplete multi-view clustering is proposed, in which data restoration, consistent representation, and clustering are jointly learned by exploiting features in various subspaces. Specifically, MRL_CAL employs v auto-encoder to obtain a low-level specific-view representation of instances, which restores data by estimating the distribution of the original incomplete data with adversarial learning. Then, MRL_CAL extracts a high-level representation of instances, in which the consistency of various views and labels of clusters is incorporated with contrastive learning. In this case, MRL_CAL simultaneously learns multi-level features of instances in various subspaces, which not only overcomes the confliction of representations but also improves the quality of features. Finally, MRL_CAL transforms incomplete multi-view clustering into an overall objective, where features are learned under the guidance of clustering. Extensive experimental results indicate that MRL_CAL outperforms state-of-the-art algorithms in terms of various measurements, implying that the proposed method is promising for incomplete multi-view clustering.",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106102",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Semi-Supervised Contrastive Learning with Generalized Contrastive Loss\n  and Its Application to Speaker Recognition",
      "authors": [
        "Nakamasa Inoue",
        "Keita Goto"
      ],
      "abstract": "This paper introduces a semi-supervised contrastive learning framework and\nits application to text-independent speaker verification. The proposed\nframework employs generalized contrastive loss (GCL). GCL unifies losses from\ntwo different learning frameworks, supervised metric learning and unsupervised\ncontrastive learning, and thus it naturally determines the loss for\nsemi-supervised learning. In experiments, we applied the proposed framework to\ntext-independent speaker verification on the VoxCeleb dataset. We demonstrate\nthat GCL enables the learning of speaker embeddings in three manners,\nsupervised learning, semi-supervised learning, and unsupervised learning,\nwithout any changes in the definition of the loss function.",
      "doi": "arXiv:2006.04326v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Tuned Contrastive Learning",
      "authors": [
        "Chaitanya Animesh",
        "Manmohan Chandraker"
      ],
      "abstract": "In recent times, contrastive learning based loss functions have become\nincreasingly popular for visual self-supervised representation learning owing\nto their state-of-the-art (SOTA) performance. Most of the modern contrastive\nlearning methods generalize only to one positive and multiple negatives per\nanchor. A recent state-of-the-art, supervised contrastive (SupCon) loss,\nextends self-supervised contrastive learning to supervised setting by\ngeneralizing to multiple positives and negatives in a batch and improves upon\nthe cross-entropy loss. In this paper, we propose a novel contrastive loss\nfunction -- Tuned Contrastive Learning (TCL) loss, that generalizes to multiple\npositives and negatives in a batch and offers parameters to tune and improve\nthe gradient responses from hard positives and hard negatives. We provide\ntheoretical analysis of our loss function's gradient response and show\nmathematically how it is better than that of SupCon loss. We empirically\ncompare our loss function with SupCon loss and cross-entropy loss in supervised\nsetting on multiple classification-task datasets to show its effectiveness. We\nalso show the stability of our loss function to a range of hyper-parameter\nsettings. Unlike SupCon loss which is only applied to supervised setting, we\nshow how to extend TCL to self-supervised setting and empirically compare it\nwith various SOTA self-supervised learning methods. Hence, we show that TCL\nloss achieves performance on par with SOTA methods in both supervised and\nself-supervised settings.",
      "doi": "arXiv:2305.10675v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Probabilistic Contrastive Loss for Self-Supervised Learning",
      "authors": [
        "Shen Li",
        "Jianqing Xu",
        "Bryan Hooi"
      ],
      "abstract": "This paper proposes a probabilistic contrastive loss function for\nself-supervised learning. The well-known contrastive loss is deterministic and\ninvolves a temperature hyperparameter that scales the inner product between two\nnormed feature embeddings. By reinterpreting the temperature hyperparameter as\na quantity related to the radius of the hypersphere, we derive a new loss\nfunction that involves a confidence measure which quantifies uncertainty in a\nmathematically grounding manner. Some intriguing properties of the proposed\nloss function are empirically demonstrated, which agree with human-like\npredictions. We believe the present work brings up a new prospective to the\narea of contrastive learning.",
      "doi": "arXiv:2112.01642v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "An Inclusive Theoretical Framework of Robust Supervised Contrastive Loss\n  against Label Noise",
      "authors": [
        "Jingyi Cui",
        "Yi-Ge Zhang",
        "Hengyu Liu",
        "Yisen Wang"
      ],
      "abstract": "Learning from noisy labels is a critical challenge in machine learning, with\nvast implications for numerous real-world scenarios. While supervised\ncontrastive learning has recently emerged as a powerful tool for navigating\nlabel noise, many existing solutions remain heuristic, often devoid of a\nsystematic theoretical foundation for crafting robust supervised contrastive\nlosses. To address the gap, in this paper, we propose a unified theoretical\nframework for robust losses under the pairwise contrastive paradigm. In\nparticular, we for the first time derive a general robust condition for\narbitrary contrastive losses, which serves as a criterion to verify the\ntheoretical robustness of a supervised contrastive loss against label noise.\nThe theory indicates that the popular InfoNCE loss is in fact non-robust, and\naccordingly inspires us to develop a robust version of InfoNCE, termed\nSymmetric InfoNCE (SymNCE). Moreover, we highlight that our theory is an\ninclusive framework that provides explanations to prior robust techniques such\nas nearest-neighbor (NN) sample selection and robust contrastive loss.\nValidation experiments on benchmark datasets demonstrate the superiority of\nSymNCE against label noise.",
      "doi": "arXiv:2501.01130v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "Self-Supervised Deep Correlation Tracking.",
      "authors": [
        "Yuan D",
        "Chang X",
        "Huang PY",
        "Liu Q",
        "He Z"
      ],
      "abstract": "The training of a feature extraction network typically requires abundant manually annotated training samples, making this a time-consuming and costly process. Accordingly, we propose an effective self-supervised learning-based tracker in a deep correlation framework (named: self-SDCT). Motivated by the forward-backward tracking consistency of a robust tracker, we propose a multi-cycle consistency loss as self-supervised information for learning feature extraction network from adjacent video frames. At the training stage, we generate pseudo-labels of consecutive video frames by forward-backward prediction under a Siamese correlation tracking framework and utilize the proposed multi-cycle consistency loss to learn a feature extraction network. Furthermore, we propose a similarity dropout strategy to enable some low-quality training sample pairs to be dropped and also adopt a cycle trajectory consistency loss in each sample pair to improve the training loss function. At the tracking stage, we employ the pre-trained feature extraction network to extract features and utilize a Siamese correlation tracking framework to locate the target using forward tracking alone. Extensive experimental results indicate that the proposed self-supervised deep correlation tracker (self-SDCT) achieves competitive tracking performance contrasted to state-of-the-art supervised and unsupervised tracking methods on standard evaluation benchmarks.",
      "doi": "https://doi.org/10.1109/tip.2020.3037518",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "SCL: Self-supervised contrastive learning for few-shot image classification.",
      "authors": [
        "Lim JY",
        "Lim KM",
        "Lee CP",
        "Tan YX"
      ],
      "abstract": "Few-shot learning aims to train a model with a limited number of base class samples to classify the novel class samples. However, to attain generalization with a limited number of samples is not a trivial task. This paper proposed a novel few-shot learning approach named Self-supervised Contrastive Learning (SCL) that enriched the model representation with multiple self-supervision objectives. Given the base class samples, the model is trained with the base class loss. Subsequently, contrastive-based self-supervision is introduced to minimize the distance between each training sample with their augmented variants to improve the sample discrimination. To recognize the distant sample, rotation-based self-supervision is proposed to enable the model to learn to recognize the rotation degree of the samples for better sample diversity. The multitask environment is introduced where each training sample is assigned with two class labels: base class label and rotation class label. Complex augmentation is put forth to help the model learn a deeper understanding of the object. The image structure of the training samples are augmented independent of the base class information. The proposed SCL is trained to minimize the base class loss, contrastive distance loss, and rotation class loss simultaneously to learn the generic features and improve the novel class performance. With the multiple self-supervision objectives, the proposed SCL outperforms state-of-the-art few-shot approaches on few-shot image classification benchmark datasets.",
      "doi": "https://doi.org/10.1016/j.neunet.2023.05.037",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Self-supervised contrastive learning with random walks for medical image segmentation with limited annotations.",
      "authors": [
        "Fischer M",
        "Hepp T",
        "Gatidis S",
        "Yang B"
      ],
      "abstract": "Medical image segmentation has seen significant progress through the use of supervised deep learning. Hereby, large annotated datasets were employed to reliably segment anatomical structures. To reduce the requirement for annotated training data, self-supervised pre-training strategies on non-annotated data were designed. Especially contrastive learning schemes operating on dense pixel-wise representations have been introduced as an effective tool. In this work, we expand on this strategy and leverage inherent anatomical similarities in medical imaging data. We apply our approach to the task of semantic segmentation in a semi-supervised setting with limited amounts of annotated volumes. Trained alongside a segmentation loss in one single training stage, a contrastive loss aids to differentiate between salient anatomical regions that conform to the available annotations. Our approach builds upon the work of Jabri et al. (2020), who proposed cyclical contrastive random walks (CCRW) for self-supervision on palindromes of video frames. We adapt this scheme to operate on entries of paired embedded image slices. Using paths of cyclical random walks bypasses the need for negative samples, as commonly used in contrastive approaches, enabling the algorithm to discriminate among relevant salient (anatomical) regions implicitly. Further, a multi-level supervision strategy is employed, ensuring adequate representations of local and global characteristics of anatomical structures. The effectiveness of reducing the amount of required annotations is shown on three MRI datasets. A median increase of 8.01 and 5.90 pp in the Dice Similarity Coefficient (DSC) compared to our baseline could be achieved across all three datasets in the case of one and two available annotated examples per dataset.",
      "doi": "https://doi.org/10.1016/j.compmedimag.2022.102174",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Rethinking Patch Dependence for Masked Autoencoders",
      "authors": [
        "Letian Fu",
        "Long Lian",
        "Renhao Wang",
        "Baifeng Shi",
        "Xudong Wang",
        "Adam Yala",
        "Trevor Darrell",
        "Alexei A. Efros",
        "Ken Goldberg"
      ],
      "abstract": "In this work, we examine the impact of inter-patch dependencies in the\ndecoder of masked autoencoders (MAE) on representation learning. We decompose\nthe decoding mechanism for masked reconstruction into self-attention between\nmask tokens and cross-attention between masked and visible tokens. Our findings\nreveal that MAE reconstructs coherent images from visible patches not through\ninteractions between patches in the decoder but by learning a global\nrepresentation within the encoder. This discovery leads us to propose a simple\nvisual pretraining framework: cross-attention masked autoencoders (CrossMAE).\nThis framework employs only cross-attention in the decoder to independently\nread out reconstructions for a small subset of masked patches from encoder\noutputs. This approach achieves comparable or superior performance to\ntraditional MAE across models ranging from ViT-S to ViT-H and significantly\nreduces computational requirements. By its design, CrossMAE challenges the\nnecessity of interaction between mask tokens for effective masked pretraining.\nCode and models are publicly available: https://crossmae.github.io",
      "doi": "arXiv:2401.14391v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Downstream Task Guided Masking Learning in Masked Autoencoders Using\n  Multi-Level Optimization",
      "authors": [
        "Han Guo",
        "Ramtin Hosseini",
        "Ruiyi Zhang",
        "Sai Ashish Somayajula",
        "Ranak Roy Chowdhury",
        "Rajesh K. Gupta",
        "Pengtao Xie"
      ],
      "abstract": "Masked Autoencoder (MAE) is a notable method for self-supervised pretraining\nin visual representation learning. It operates by randomly masking image\npatches and reconstructing these masked patches using the unmasked ones. A key\nlimitation of MAE lies in its disregard for the varying informativeness of\ndifferent patches, as it uniformly selects patches to mask. To overcome this,\nsome approaches propose masking based on patch informativeness. However, these\nmethods often do not consider the specific requirements of downstream tasks,\npotentially leading to suboptimal representations for these tasks. In response,\nwe introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel\nframework that leverages end-to-end feedback from downstream tasks to learn an\noptimal masking strategy during pretraining. Our experimental findings\nhighlight MLO-MAE's significant advancements in visual representation learning.\nCompared to existing methods, it demonstrates remarkable improvements across\ndiverse datasets and tasks, showcasing its adaptability and efficiency.",
      "doi": "arXiv:2402.18128v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation",
      "authors": [
        "Shengyi Qian",
        "Kaichun Mo",
        "Valts Blukis",
        "David F. Fouhey",
        "Dieter Fox",
        "Ankit Goyal"
      ],
      "abstract": "Recent works have shown that visual pretraining on egocentric datasets using\nmasked autoencoders (MAE) can improve generalization for downstream robotics\ntasks. However, these approaches pretrain only on 2D images, while many\nrobotics applications require 3D scene understanding. In this work, we propose\n3D-MVP, a novel approach for 3D Multi-View Pretraining using masked\nautoencoders. We leverage Robotic View Transformer (RVT), which uses a\nmulti-view transformer to understand the 3D scene and predict gripper pose\nactions. We split RVT's multi-view transformer into visual encoder and action\ndecoder, and pretrain its visual encoder using masked autoencoding on\nlarge-scale 3D datasets such as Objaverse. We evaluate 3D-MVP on a suite of\nvirtual robot manipulation tasks and demonstrate improved performance over\nbaselines. Our results suggest that 3D-aware pretraining is a promising\napproach to improve generalization of vision-based robotic manipulation\npolicies. Project site: https://jasonqsy.github.io/3DMVP",
      "doi": "arXiv:2406.18158v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "MAE-AST: Masked Autoencoding Audio Spectrogram Transformer",
      "authors": [
        "Alan Baade",
        "Puyuan Peng",
        "David Harwath"
      ],
      "abstract": "In this paper, we propose a simple yet powerful improvement over the recent\nSelf-Supervised Audio Spectrogram Transformer (SSAST) model for speech and\naudio classification. Specifically, we leverage the insight that the SSAST uses\na very high masking ratio (75%) during pretraining, meaning that the vast\nmajority of self-attention compute is performed on mask tokens. We address this\nby integrating the encoder-decoder architecture from Masked Autoencoders are\nScalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on\nonly unmasked input, and a shallow decoder operates on encoder outputs and mask\ntokens. We find that MAE-like pretraining can provide a 3x speedup and 2x\nmemory usage reduction over the vanilla SSAST using current audio pretraining\nstrategies with ordinary model and input sizes. When fine-tuning on downstream\ntasks, which only uses the encoder, we find that our approach outperforms the\nSSAST on a variety of downstream tasks. We further conduct comprehensive\nevaluations into different strategies of pretraining and explore differences in\nMAE-style pretraining between the visual and audio domains.",
      "doi": "arXiv:2203.16691v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "ProteinMAE: masked autoencoder for protein surface self-supervised learning.",
      "authors": [
        "Yuan M",
        "Shen A",
        "Fu K",
        "Guan J",
        "Ma Y",
        "Qiao Q",
        "Wang M"
      ],
      "abstract": "The biological functions of proteins are determined by the chemical and geometric properties of their surfaces. Recently, with the booming progress of deep learning, a series of learning-based surface descriptors have been proposed and achieved inspirational performance in many tasks such as protein design, protein-protein interaction prediction, etc. However, they are still limited by the problem of label scarcity, since the labels are typically obtained through wet experiments. Inspired by the great success of self-supervised learning in natural language processing and computer vision, we introduce ProteinMAE, a self-supervised framework specifically designed for protein surface representation to mitigate label scarcity. Specifically, we propose an efficient network and utilize a large number of accessible unlabeled protein data to pretrain it by self-supervised learning. Then we use the pretrained weights as initialization and fine-tune the network on downstream tasks. To demonstrate the effectiveness of our method, we conduct experiments on three different downstream tasks including binding site identification in protein surface, ligand-binding protein pocket classification, and protein-protein interaction prediction. The extensive experiments show that our method not only successfully improves the network's performance on all downstream tasks, but also achieves competitive performance with state-of-the-art methods. Moreover, our proposed network also exhibits significant advantages in terms of computational cost, which only requires less than a tenth of memory cost of previous methods. https://github.com/phdymz/ProteinMAE.",
      "doi": "https://doi.org/10.1093/bioinformatics/btad724",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Applying masked autoencoder-based self-supervised learning for high-capability vision transformers of electrocardiographies.",
      "authors": [
        "Sawano S",
        "Kodera S",
        "Setoguchi N",
        "Tanabe K",
        "Kushida S",
        "Kanda J",
        "Saji M",
        "Nanasato M",
        "Maki H",
        "Fujita H",
        "Kato N",
        "Watanabe H",
        "Suzuki M",
        "Takahashi M",
        "Sawada N",
        "Yamasaki M",
        "Sato M",
        "Katsushika S",
        "Shinohara H",
        "Takeda N",
        "Fujiu K",
        "Daimon M",
        "Akazawa H",
        "Morita H",
        "Komuro I"
      ],
      "abstract": "The generalization of deep neural network algorithms to a broader population is an important challenge in the medical field. We aimed to apply self-supervised learning using masked autoencoders (MAEs) to improve the performance of the 12-lead electrocardiography (ECG) analysis model using limited ECG data. We pretrained Vision Transformer (ViT) models by reconstructing the masked ECG data with MAE. We fine-tuned this MAE-based ECG pretrained model on ECG-echocardiography data from The University of Tokyo Hospital (UTokyo) for the detection of left ventricular systolic dysfunction (LVSD), and then evaluated it using multi-center external validation data from seven institutions, employing the area under the receiver operating characteristic curve (AUROC) for assessment. We included 38,245 ECG-echocardiography pairs from UTokyo and 229,439 pairs from all institutions. The performances of MAE-based ECG models pretrained using ECG data from UTokyo were significantly higher than that of other Deep Neural Network models across all external validation cohorts (AUROC, 0.913-0.962 for LVSD, p < 0.001). Moreover, we also found improvements for the MAE-based ECG analysis model depending on the model capacity and the amount of training data. Additionally, the MAE-based ECG analysis model maintained high performance even on the ECG benchmark dataset (PTB-XL). Our proposed method developed high performance MAE-based ECG analysis models using limited ECG data.",
      "doi": "https://doi.org/10.1371/journal.pone.0307978",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Regularized Denoising Masked Visual Pretraining for Robust Embodied PointGoal Navigation.",
      "authors": [
        "Peng J",
        "Xu Y",
        "Luo L",
        "Liu H",
        "Lu K",
        "Liu J"
      ],
      "abstract": "Embodied PointGoal navigation is a fundamental task for embodied agents. Recent works have shown that the performance of the embodied navigation agent degrades significantly in the presence of visual corruption, including Spatter, Speckle Noise, and Defocus Blur, showing the weak robustness of the agent. To improve the robustness of embodied navigation agents to various visual corruptions, we propose a navigation framework called Regularized Denoising Masked AutoEncoders Navigation (RDMAE-Nav). In a nutshell, RDMAE-Nav mainly consists of two modules: a visual module and a policy module. In the visual module, a self-supervised pretraining method, dubbed Regularized Denoising Masked AutoEncoders (RDMAE), is designed to enable the Vision Transformers (ViT)-based visual encoder to learn robust representations. The bidirectional Kullback-Leibler divergence is introduced in RDMAE as the regularization term for a denoising masked modeling task. Specifically, RDMAE mitigates the gap between clean and noisy image representations by minimizing the bidirectional Kullback-Leibler divergence. Then, the visual encoder is pretrained by RDMAE. In contrast to existing works, RDMAE-Nav applies denoising masked visual pretraining for PointGoal navigation to improve robustness to various visual corruptions. Finally, the pretrained visual encoder with frozen weights is applied to extract robust visual representations for policy learning in the RDMAE-Nav. Extensive experiments show that RDMAE-Nav performs competitively compared with state of the arts (SOTAs) on various visual corruptions. In detail, RDMAE-Nav performs the absolute improvement: 28.2% in SR and 23.68% in SPL under Spatter; 2.28% in SR and 6.41% in SPL under Speckle Noise; and 9.46% in SR and 9.55% in SPL under Defocus Blur.",
      "doi": "https://doi.org/10.3390/s23073553",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "SpectralMAE: Spectral Masked Autoencoder for Hyperspectral Remote Sensing Image Reconstruction.",
      "authors": [
        "Zhu L",
        "Wu J",
        "Biao W",
        "Liao Y",
        "Gu D"
      ],
      "abstract": "Accurate hyperspectral remote sensing information is essential for feature identification and detection. Nevertheless, the hyperspectral imaging mechanism poses challenges in balancing the trade-off between spatial and spectral resolution. Hardware improvements are cost-intensive and depend on strict environmental conditions and extra equipment. Recent spectral imaging methods have attempted to directly reconstruct hyperspectral information from widely available multispectral images. However, fixed mapping approaches used in previous spectral reconstruction models limit their reconstruction quality and generalizability, especially dealing with missing or contaminated bands. Moreover, data-hungry issues plague increasingly complex data-driven spectral reconstruction methods. This paper proposes SpectralMAE, a novel spectral reconstruction model that can take arbitrary combinations of bands as input and improve the utilization of data sources. In contrast to previous spectral reconstruction techniques, SpectralMAE explores the application of a self-supervised learning paradigm and proposes a masked autoencoder architecture for spectral dimensions. To further enhance the performance for specific sensor inputs, we propose a training strategy by combining random masking pre-training and fixed masking fine-tuning. Empirical evaluations on five remote sensing datasets demonstrate that SpectralMAE outperforms state-of-the-art methods in both qualitative and quantitative metrics.",
      "doi": "https://doi.org/10.3390/s23073728",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Masked autoencoder: influence of self-supervised pretraining on object segmentation in industrial images",
      "authors": [
        "Anja Witte",
        "Sascha Lange",
        "Christian Lins"
      ],
      "abstract": "<jats:title>Abstract</jats:title><jats:p>The amount of labelled data in industrial use cases is limited because the annotation process is time-consuming and costly. As in research, self-supervised pretraining such as MAE resulted in training segmentation models with fewer labels, this is also an interesting direction for industry. The reduction of required labels is achieved with large amounts of unlabelled images for the pretraining that aims to learn image features. This paper analyses the influence of MAE pretraining on the efficiency of label usage for semantic segmentation with UNETR. This is investigated for the use case of log-yard cranes. Additionally, two transfer learning cases with respect to crane type and perspective are considered in the context of label-efficiency. The results show that MAE is successfully applicable to the use case. With respect to the segmentation, an IoU improvement of 3.26% is reached while using 2000 labels. The strongest positive influence is found for all experiments in the lower label amounts. The highest effect is achieved with transfer learning regarding cranes, where IoU and Recall increase about 4.31% and 8.58%, respectively. Further analyses show that improvements result from a better distinction between the background and the segmented crane objects.</jats:p>",
      "doi": "https://doi.org/10.1007/s44244-024-00020-y",
      "year": 2024,
      "source": "Crossref"
    },
    {
      "title": "Learned Camera Gain and Exposure Control for Improved Visual Feature\n  Detection and Matching",
      "authors": [
        "Justin Tomasi",
        "Brandon Wagstaff",
        "Steven L. Waslander",
        "Jonathan Kelly"
      ],
      "abstract": "Successful visual navigation depends upon capturing images that contain\nsufficient useful information. In this letter, we explore a data-driven\napproach to account for environmental lighting changes, improving the quality\nof images for use in visual odometry (VO) or visual simultaneous localization\nand mapping (SLAM). We train a deep convolutional neural network model to\npredictively adjust camera gain and exposure time parameters such that\nconsecutive images contain a maximal number of matchable features. The training\nprocess is fully self-supervised: our training signal is derived from an\nunderlying VO or SLAM pipeline and, as a result, the model is optimized to\nperform well with that specific pipeline. We demonstrate through extensive\nreal-world experiments that our network can anticipate and compensate for\ndramatic lighting changes (e.g., transitions into and out of road tunnels),\nmaintaining a substantially higher number of inlier feature matches than\ncompeting camera parameter control algorithms.",
      "doi": "https://doi.org/10.1109/LRA.2021.3058909",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Mask Scoring R-CNN",
      "authors": [
        "Zhaojin Huang",
        "Lichao Huang",
        "Yongchao Gong",
        "Chang Huang",
        "Xinggang Wang"
      ],
      "abstract": "Letting a deep network be aware of the quality of its own predictions is an\ninteresting yet important problem. In the task of instance segmentation, the\nconfidence of instance classification is used as mask quality score in most\ninstance segmentation frameworks. However, the mask quality, quantified as the\nIoU between the instance mask and its ground truth, is usually not well\ncorrelated with classification score. In this paper, we study this problem and\npropose Mask Scoring R-CNN which contains a network block to learn the quality\nof the predicted instance masks. The proposed network block takes the instance\nfeature and the corresponding predicted mask together to regress the mask IoU.\nThe mask scoring strategy calibrates the misalignment between mask quality and\nmask score, and improves instance segmentation performance by prioritizing more\naccurate mask predictions during COCO AP evaluation. By extensive evaluations\non the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain\nwith different models, and outperforms the state-of-the-art Mask R-CNN. We hope\nour simple and effective approach will provide a new direction for improving\ninstance segmentation. The source code of our method is available at\n\\url{https://github.com/zjhuang22/maskscoring_rcnn}.",
      "doi": "arXiv:1903.00241v1",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Mask-Adapter: The Devil is in the Masks for Open-Vocabulary Segmentation",
      "authors": [
        "Yongkang Li",
        "Tianheng Cheng",
        "Bin Feng",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "abstract": "Recent open-vocabulary segmentation methods adopt mask generators to predict\nsegmentation masks and leverage pre-trained vision-language models, e.g., CLIP,\nto classify these masks via mask pooling. Although these approaches show\npromising results, it is counterintuitive that accurate masks often fail to\nyield accurate classification results through pooling CLIP image embeddings\nwithin the mask regions. In this paper, we reveal the performance limitations\nof mask pooling and introduce Mask-Adapter, a simple yet effective method to\naddress these challenges in open-vocabulary segmentation. Compared to directly\nusing proposal masks, our proposed Mask-Adapter extracts semantic activation\nmaps from proposal masks, providing richer contextual information and ensuring\nalignment between masks and CLIP. Additionally, we propose a mask consistency\nloss that encourages proposal masks with similar IoUs to obtain similar CLIP\nembeddings to enhance models' robustness to varying predicted masks.\nMask-Adapter integrates seamlessly into open-vocabulary segmentation methods\nbased on mask pooling in a plug-and-play manner, delivering more accurate\nclassification results. Extensive experiments across several zero-shot\nbenchmarks demonstrate significant performance gains for the proposed\nMask-Adapter on several well-established methods. Notably, Mask-Adapter also\nextends effectively to SAM and achieves impressive results on several\nopen-vocabulary segmentation datasets. Code and models are available at\nhttps://github.com/hustvl/MaskAdapter.",
      "doi": "arXiv:2412.04533v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT).",
      "authors": [
        "Jiang J",
        "Tyagi N",
        "Tringale K",
        "Crane C",
        "Veeraraghavan H"
      ],
      "abstract": "Vision transformers efficiently model long-range context and thus have demonstrated impressive accuracy gains in several image analysis tasks including segmentation. However, such methods need large labeled datasets for training, which is hard to obtain for medical image analysis. Self-supervised learning (SSL) has demonstrated success in medical image segmentation using convolutional networks. In this work, we developed a self-distillation learning with masked image modeling method to perform SSL for vision transformers (SMIT) applied to 3D multi-organ segmentation from CT and MRI. Our contribution combines a dense pixel-wise regression pretext task performed within masked patches called masked image prediction with masked patch token distillation to pre-train vision transformers. Our approach is more accurate and requires fewer fine tuning datasets than other pretext tasks. Unlike prior methods, which typically used image sets arising from disease sites and imaging modalities corresponding to the target tasks, we used 3,643 CT scans (602,708 images) arising from head and neck, lung, and kidney cancers as well as COVID-19 for pre-training and applied it to abdominal organs segmentation from MRI pancreatic cancer patients as well as publicly available 13 different abdominal organs segmentation from CT. Our method showed clear accuracy improvement (average DSC of 0.875 from MRI and 0.878 from CT) with reduced requirement for fine-tuning datasets over commonly used pretext tasks. Extensive comparisons against multiple current SSL methods were done. Our code is available at: https://github.com/harveerar/SMIT.git.",
      "doi": "https://doi.org/10.1007/978-3-031-16440-8_53",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Point-MPP: Point Cloud Self-Supervised Learning From Masked Position Prediction.",
      "authors": [
        "Fan S",
        "Gao W",
        "Li G"
      ],
      "abstract": "Masked autoencoding has gained momentum for improving fine-tuning performance in many downstream tasks. However, it tends to focus on low-level reconstruction details, lacking high-level semantics and resulting in weak transfer capability. This article presents a novel jigsaw puzzle solver inspired by the idea that predicting the positions of disordered point cloud patches provides more semantic information, similar to how children learn by solving jigsaw puzzles. Our method adopts the mask-then-predict paradigm, erasing the positions of selected point patches rather than their contents. We first partition input point clouds into irregular patches and randomly erase the positions of some patches. Then, a Transformer-based model is used to learn high-level semantic features and regress the positions of the masked patches. This approach forces the model to focus on learning transfer-robust semantics while paying less attention to low-level details. To tie the predictions within the encoding space, we further introduce a consistency constraint on their latent representations to encourage the encoded features to contain more semantic cues. We demonstrate that a standard Transformer backbone with our pretraining scheme can capture discriminative point cloud semantic information. Furthermore, extensive experiments indicate that our method outperforms the previous best competitor across six popular downstream vision tasks, achieving new state-of-the-art performance. Codes will be available at https://git.openi.org.cn/OpenPointCloud/Point-MPP.",
      "doi": "https://doi.org/10.1109/tnnls.2024.3479309",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "SMG: self-supervised masked graph learning for cancer gene identification.",
      "authors": [
        "Cui Y",
        "Wang Z",
        "Wang X",
        "Zhang Y",
        "Zhang Y",
        "Pan T",
        "Zhang Z",
        "Li S",
        "Guo Y",
        "Akutsu T",
        "Song J"
      ],
      "abstract": "Cancer genomics is dedicated to elucidating the genes and pathways that contribute to cancer progression and development. Identifying cancer genes (CGs) associated with the initiation and progression of cancer is critical for characterization of molecular-level mechanism in cancer research. In recent years, the growing availability of high-throughput molecular data and advancements in deep learning technologies has enabled the modelling of complex interactions and topological information within genomic data. Nevertheless, because of the limited labelled data, pinpointing CGs from a multitude of potential mutations remains an exceptionally challenging task. To address this, we propose a novel deep learning framework, termed self-supervised masked graph learning (SMG), which comprises SMG reconstruction (pretext task) and task-specific fine-tuning (downstream task). In the pretext task, the nodes of multi-omic featured protein-protein interaction (PPI) networks are randomly substituted with a defined mask token. The PPI networks are then reconstructed using the graph neural network (GNN)-based autoencoder, which explores the node correlations in a self-prediction manner. In the downstream tasks, the pre-trained GNN encoder embeds the input networks into feature graphs, whereas a task-specific layer proceeds with the final prediction. To assess the performance of the proposed SMG method, benchmarking experiments are performed on three node-level tasks (identification of CGs, essential genes and healthy driver genes) and one graph-level task (identification of disease subnetwork) across eight PPI networks. Benchmarking experiments and performance comparison with existing state-of-the-art methods demonstrate the superiority of SMG on multi-omic feature engineering.",
      "doi": "https://doi.org/10.1093/bib/bbad406",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "A Survey on Masked Autoencoder for Self-supervised Learning in Vision\n  and Beyond",
      "authors": [
        "Chaoning Zhang",
        "Chenshuang Zhang",
        "Junha Song",
        "John Seon Keun Yi",
        "Kang Zhang",
        "In So Kweon"
      ],
      "abstract": "Masked autoencoders are scalable vision learners, as the title of MAE\n\\cite{he2022masked}, which suggests that self-supervised learning (SSL) in\nvision might undertake a similar trajectory as in NLP. Specifically, generative\npretext tasks with the masked prediction (e.g., BERT) have become a de facto\nstandard SSL practice in NLP. By contrast, early attempts at generative methods\nin vision have been buried by their discriminative counterparts (like\ncontrastive learning); however, the success of mask image modeling has revived\nthe masking autoencoder (often termed denoising autoencoder in the past). As a\nmilestone to bridge the gap with BERT in NLP, masked autoencoder has attracted\nunprecedented attention for SSL in vision and beyond. This work conducts a\ncomprehensive survey of masked autoencoders to shed insight on a promising\ndirection of SSL. As the first to review SSL with masked autoencoders, this\nwork focuses on its application in vision by discussing its historical\ndevelopments, recent progress, and implications for diverse applications.",
      "doi": "arXiv:2208.00173v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "MIRAM: Masked Image Reconstruction Across Multiple Scales for Breast\n  Lesion Risk Prediction",
      "authors": [
        "Hung Q. Vo",
        "Pengyu Yuan",
        "Zheng Yin",
        "Kelvin K. Wong",
        "Chika F. Ezeana",
        "Son T. Ly",
        "Stephen T. C. Wong",
        "Hien V. Nguyen"
      ],
      "abstract": "Self-supervised learning (SSL) has garnered substantial interest within the\nmachine learning and computer vision communities. Two prominent approaches in\nSSL include contrastive-based learning and self-distillation utilizing cropping\naugmentation. Lately, masked image modeling (MIM) has emerged as a more potent\nSSL technique, employing image inpainting as a pretext task. MIM creates a\nstrong inductive bias toward meaningful spatial and semantic understanding.\nThis has opened up new opportunities for SSL to contribute not only to\nclassification tasks but also to more complex applications like object\ndetection and image segmentation. Building upon this progress, our research\npaper introduces a scalable and practical SSL approach centered around more\nchallenging pretext tasks that facilitate the acquisition of robust features.\nSpecifically, we leverage multi-scale image reconstruction from randomly masked\ninput images as the foundation for feature learning. Our hypothesis posits that\nreconstructing high-resolution images enables the model to attend to finer\nspatial details, particularly beneficial for discerning subtle intricacies\nwithin medical images. The proposed SSL features help improve classification\nperformance on the Curated Breast Imaging Subset of Digital Database for\nScreening Mammography (CBIS-DDSM) dataset. In pathology classification, our\nmethod demonstrates a 3\\% increase in average precision (AP) and a 1\\% increase\nin the area under the receiver operating characteristic curve (AUC) when\ncompared to state-of-the-art (SOTA) algorithms. Moreover, in mass margins\nclassification, our approach achieves a 4\\% increase in AP and a 2\\% increase\nin AUC.",
      "doi": "arXiv:2503.07157v2",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "DailyMAE: Towards Pretraining Masked Autoencoders in One Day",
      "authors": [
        "Jiantao Wu",
        "Shentong Mo",
        "Sara Atito",
        "Zhenhua Feng",
        "Josef Kittler",
        "Muhammad Awais"
      ],
      "abstract": "Recently, masked image modeling (MIM), an important self-supervised learning\n(SSL) method, has drawn attention for its effectiveness in learning data\nrepresentation from unlabeled data. Numerous studies underscore the advantages\nof MIM, highlighting how models pretrained on extensive datasets can enhance\nthe performance of downstream tasks. However, the high computational demands of\npretraining pose significant challenges, particularly within academic\nenvironments, thereby impeding the SSL research progress. In this study, we\npropose efficient training recipes for MIM based SSL that focuses on mitigating\ndata loading bottlenecks and employing progressive training techniques and\nother tricks to closely maintain pretraining performance. Our library enables\nthe training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs\nwithin just 18 hours, using a single machine equipped with 8 A100 GPUs. By\nachieving speed gains of up to 5.8 times, this work not only demonstrates the\nfeasibility of conducting high-efficiency SSL training but also paves the way\nfor broader accessibility and promotes advancement in SSL research particularly\nfor prototyping and initial testing of SSL ideas. The code is available in\nhttps://github.com/erow/FastSSL.",
      "doi": "arXiv:2404.00509v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Adversarial Masking for Self-Supervised Learning",
      "authors": [
        "Yuge Shi",
        "N. Siddharth",
        "Philip H. S. Torr",
        "Adam R. Kosiorek"
      ],
      "abstract": "We propose ADIOS, a masked image model (MIM) framework for self-supervised\nlearning, which simultaneously learns a masking function and an image encoder\nusing an adversarial objective. The image encoder is trained to minimise the\ndistance between representations of the original and that of a masked image.\nThe masking function, conversely, aims at maximising this distance. ADIOS\nconsistently improves on state-of-the-art self-supervised learning (SSL)\nmethods on a variety of tasks and datasets -- including classification on\nImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and\niNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao\net al., 2021) -- while generating semantically meaningful masks. Unlike modern\nMIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch\ntokenisation construction of Vision Transformers, and can be implemented with\nconvolutional backbones. We further demonstrate that the masks learned by ADIOS\nare more effective in improving representation learning of SSL methods than\nmasking schemes used in popular MIM models. Code is available at\nhttps://github.com/YugeTen/adios.",
      "doi": "arXiv:2201.13100v3",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Kernel Masked Image Modeling Through the Lens of Theoretical Understanding.",
      "authors": [
        "Qian Y",
        "Wang Y",
        "Zou J",
        "Lin J",
        "Pan Y",
        "Yao T",
        "Sun Q",
        "Mei T"
      ],
      "abstract": "Masked image modeling (MIM) has been considered as the state-of-the-art (SOTA) self-supervised learning (SSL) technique in terms of visual pretraining. The impressive generalization ability of MIM also paves the way for the remarkable success of large-scale vision foundation models. In this article, we further discuss the validity and advantages of implementing MIM techniques in the reproducing kernel Hilbert spaces (RKHSs) and we associate the analysis with a novel MIM method named R-MIM (short for RKHS-MIM). Through the careful construction of an augmentation graph and by using spectral decomposition techniques, we establish a systematic theoretical understanding between the proposed R-MIM's generalization ability and the choice of kernel function used during training. Specifically, we reach a conclusion that both of the local Lipschitz constant of the resultant R-MIM model and the corresponding expected pretraining error can have a strong composite effect on bounding downstream task error, depending on the kernel options. We demonstrate that under mild mathematical assumptions, R-MIM method is guaranteed to return a lower bound on downstream tasks in comparison to vanilla MIM techniques, such as masked autoencoder (MAE) and SimMIM. Empirical justification well corroborates our theoretical hypothesis and analysis in showing the superior generalization of the proposed R-MIM and the theoretical link to kernel choices. The code is available at: https://github.com/yurui-q/R-MIM.",
      "doi": "https://doi.org/10.1109/tnnls.2024.3443088",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Uni4Eye++: A General Masked Image Modeling Multi-Modal Pre-Training Framework for Ophthalmic Image Classification and Segmentation.",
      "authors": [
        "Cai Z",
        "Lin L",
        "He H",
        "Cheng P",
        "Tang X"
      ],
      "abstract": "A large-scale labeled dataset is a key factor for the success of supervised deep learning in most ophthalmic image analysis scenarios. However, limited annotated data is very common in ophthalmic image analysis, since manual annotation is time-consuming and labor-intensive. Self-supervised learning (SSL) methods bring huge opportunities for better utilizing unlabeled data, as they do not require massive annotations. To utilize as many unlabeled ophthalmic images as possible, it is necessary to break the dimension barrier, simultaneously making use of both 2D and 3D images as well as alleviating the issue of catastrophic forgetting. In this paper, we propose a universal self-supervised Transformer framework named Uni4Eye++ to discover the intrinsic image characteristic and capture domain-specific feature embedding in ophthalmic images. Uni4Eye++ can serve as a global feature extractor, which builds its basis on a Masked Image Modeling task with a Vision Transformer architecture. On the basis of our previous work Uni4Eye, we further employ an image entropy guided masking strategy to reconstruct more-informative patches and a dynamic head generator module to alleviate modality confusion. We evaluate the performance of our pre-trained Uni4Eye++ encoder by fine-tuning it on multiple downstream ophthalmic image classification and segmentation tasks. The superiority of Uni4Eye++ is successfully established through comparisons to other state-of-the-art SSL pre-training methods. Our code is available at https://github.com/Davidczy/Uni4Eye++.",
      "doi": "https://doi.org/10.1109/tmi.2024.3422102",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Cervical OCT image classification using contrastive masked autoencoders with Swin Transformer.",
      "authors": [
        "Wang Q",
        "Xiong Y",
        "Zhu H",
        "Mu X",
        "Zhang Y",
        "Ma Y"
      ],
      "abstract": "Cervical cancer poses a major health threat to women globally. Optical coherence tomography (OCT) imaging has recently shown promise for non-invasive cervical lesion diagnosis. However, obtaining high-quality labeled cervical OCT images is challenging and time-consuming as they must correspond precisely with pathological results. The scarcity of such high-quality labeled data hinders the application of supervised deep-learning models in practical clinical settings. This study addresses the above challenge by proposing CMSwin, a novel self-supervised learning (SSL) framework combining masked image modeling (MIM) with contrastive learning based on the Swin-Transformer architecture to utilize abundant unlabeled cervical OCT images. In this contrastive-MIM framework, mixed image encoding is combined with a latent contextual regressor to solve the inconsistency problem between pre-training and fine-tuning and separate the encoder's feature extraction task from the decoder's reconstruction task, allowing the encoder to extract better image representations. Besides, contrastive losses at the patch and image levels are elaborately designed to leverage massive unlabeled data. We validated the superiority of CMSwin over the state-of-the-art SSL approaches with five-fold cross-validation on an OCT image dataset containing 1,452 patients from a multi-center clinical study in China, plus two external validation sets from top-ranked Chinese hospitals: the Huaxi dataset from the West China Hospital of Sichuan University and the Xiangya dataset from the Xiangya Second Hospital of Central South University. A human-machine comparison experiment on the Huaxi and Xiangya datasets for volume-level binary classification also indicates that CMSwin can match or exceed the average level of four skilled medical experts, especially in identifying high-risk cervical lesions. Our work has great potential to assist gynecologists in intelligently interpreting cervical OCT images in clinical settings. Additionally, the integrated GradCAM module of CMSwin enables cervical lesion visualization and interpretation, providing good interpretability for gynecologists to diagnose cervical diseases efficiently.",
      "doi": "https://doi.org/10.1016/j.compmedimag.2024.102469",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Multi-Scale Neighborhood Occupancy Masked Autoencoder for\n  Self-Supervised Learning in LiDAR Point Clouds",
      "authors": [
        "Mohamed Abdelsamad",
        "Michael Ulrich",
        "Claudius Gläser",
        "Abhinav Valada"
      ],
      "abstract": "Masked autoencoders (MAE) have shown tremendous potential for self-supervised\nlearning (SSL) in vision and beyond. However, point clouds from LiDARs used in\nautomated driving are particularly challenging for MAEs since large areas of\nthe 3D volume are empty. Consequently, existing work suffers from leaking\noccupancy information into the decoder and has significant computational\ncomplexity, thereby limiting the SSL pre-training to only 2D bird's eye view\nencoders in practice. In this work, we propose the novel neighborhood occupancy\nMAE (NOMAE) that overcomes the aforementioned challenges by employing masked\noccupancy reconstruction only in the neighborhood of non-masked voxels. We\nincorporate voxel masking and occupancy reconstruction at multiple scales with\nour proposed hierarchical mask generation technique to capture features of\nobjects of different sizes in the point cloud. NOMAEs are extremely flexible\nand can be directly employed for SSL in existing 3D architectures. We perform\nextensive evaluations on the nuScenes and Waymo Open datasets for the\ndownstream perception tasks of semantic segmentation and 3D object detection,\ncomparing with both discriminative and generative SSL methods. The results\ndemonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks for\nmultiple point cloud perception tasks.",
      "doi": "arXiv:2502.20316v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "MET: Masked Encoding for Tabular Data",
      "authors": [
        "Kushal Majmundar",
        "Sachin Goyal",
        "Praneeth Netrapalli",
        "Prateek Jain"
      ],
      "abstract": "We consider the task of self-supervised representation learning (SSL) for\ntabular data: tabular-SSL. Typical contrastive learning based SSL methods\nrequire instance-wise data augmentations which are difficult to design for\nunstructured tabular data. Existing tabular-SSL methods design such\naugmentations in a relatively ad-hoc fashion and can fail to capture the\nunderlying data manifold. Instead of augmentations based approaches for\ntabular-SSL, we propose a new reconstruction based method, called Masked\nEncoding for Tabular Data (MET), that does not require augmentations. MET is\nbased on the popular MAE approach for vision-SSL [He et al., 2021] and uses two\nkey ideas: (i) since each coordinate in a tabular dataset has a distinct\nmeaning, we need to use separate representations for all coordinates, and (ii)\nusing an adversarial reconstruction loss in addition to the standard one.\nEmpirical results on five diverse tabular datasets show that MET achieves a new\nstate of the art (SOTA) on all of these datasets and improves up to 9% over\ncurrent SOTA methods. We shed more light on the working of MET via experiments\non carefully designed simple datasets.",
      "doi": "arXiv:2206.08564v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner",
      "authors": [
        "Zhenyu Hou",
        "Yufei He",
        "Yukuo Cen",
        "Xiao Liu",
        "Yuxiao Dong",
        "Evgeny Kharlamov",
        "Jie Tang"
      ],
      "abstract": "Graph self-supervised learning (SSL), including contrastive and generative\napproaches, offers great potential to address the fundamental challenge of\nlabel scarcity in real-world graph data. Among both sets of graph SSL\ntechniques, the masked graph autoencoders (e.g., GraphMAE)--one type of\ngenerative method--have recently produced promising results. The idea behind\nthis is to reconstruct the node features (or structures)--that are randomly\nmasked from the input--with the autoencoder architecture. However, the\nperformance of masked feature reconstruction naturally relies on the\ndiscriminability of the input features and is usually vulnerable to disturbance\nin the features. In this paper, we present a masked self-supervised learning\nframework GraphMAE2 with the goal of overcoming this issue. The idea is to\nimpose regularization on feature reconstruction for graph SSL. Specifically, we\ndesign the strategies of multi-view random re-mask decoding and latent\nrepresentation prediction to regularize the feature reconstruction. The\nmulti-view random re-mask decoding is to introduce randomness into\nreconstruction in the feature space, while the latent representation prediction\nis to enforce the reconstruction in the embedding space. Extensive experiments\nshow that GraphMAE2 can consistently generate top results on various public\ndatasets, including at least 2.45% improvements over state-of-the-art baselines\non ogbn-Papers100M with 111M nodes and 1.6B edges.",
      "doi": "arXiv:2304.04779v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Self-Supervised Learning Improves Accuracy and Data Efficiency for IMU-Based Ground Reaction Force Estimation.",
      "authors": [
        "Tan T",
        "Shull PB",
        "Hicks JL",
        "Uhlrich SD",
        "Chaudhari AS"
      ],
      "abstract": "Recent deep learning techniques hold promise to enable IMU-driven kinetic assessment; however, they require large extents of ground reaction force (GRF) data to serve as labels for supervised model training. We thus propose using existing self-supervised learning (SSL) techniques to leverage large IMU datasets to pre-train deep learning models, which can improve the accuracy and data efficiency of IMU-based GRF estimation. We performed SSL by masking a random portion of the input IMU data and training a transformer model to reconstruct the masked portion. We systematically compared a series of masking ratios across three pre-training datasets that included real IMU data, synthetic IMU data, or a combination of the two. Finally, we built models that used pre-training and labeled data to estimate GRF during three prediction tasks: overground walking, treadmill walking, and drop landing. When using the same amount of labeled data, SSL pre-training significantly improved the accuracy of 3-axis GRF estimation during walking compared to baseline models trained by conventional supervised learning. Fine-tuning SSL model with 1-10% of walking data yielded comparable accuracy to training baseline model with 100% of walking data. The optimal masking ratio for SSL is 6.25-12.5%. SSL leveraged large real and synthetic IMU datasets to increase the accuracy and data efficiency of deep-learning-based GRF estimation, reducing the need for labeled data. This work, with its open-source code and models, may unlock broader use cases of IMU-driven kinetic assessment by mitigating the scarcity of GRF measurements in practical applications.",
      "doi": "https://doi.org/10.1109/tbme.2024.3361888",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Self-supervised learning for classifying paranasal anomalies in the maxillary sinus.",
      "authors": [
        "Bhattacharya D",
        "Behrendt F",
        "Becker BT",
        "Maack L",
        "Beyersdorff D",
        "Petersen E",
        "Petersen M",
        "Cheng B",
        "Eggert D",
        "Betz C",
        "Hoffmann AS",
        "Schlaefer A"
      ],
      "abstract": "Paranasal anomalies, frequently identified in routine radiological screenings, exhibit diverse morphological characteristics. Due to the diversity of anomalies, supervised learning methods require large labelled dataset exhibiting diverse anomaly morphology. Self-supervised learning (SSL) can be used to learn representations from unlabelled data. However, there are no SSL methods designed for the downstream task of classifying paranasal anomalies in the maxillary sinus (MS). Our approach uses a 3D convolutional autoencoder (CAE) trained in an unsupervised anomaly detection (UAD) framework. Initially, we train the 3D CAE to reduce reconstruction errors when reconstructing normal maxillary sinus (MS) image. Then, this CAE is applied to an unlabelled dataset to generate coarse anomaly locations by creating residual MS images. Following this, a 3D convolutional neural network (CNN) reconstructs these residual images, which forms our SSL task. Lastly, we fine-tune the encoder part of the 3D CNN on a labelled dataset of normal and anomalous MS images. The proposed SSL technique exhibits superior performance compared to existing generic self-supervised methods, especially in scenarios with limited annotated data. When trained on just 10% of the annotated dataset, our method achieves an area under the precision-recall curve (AUPRC) of 0.79 for the downstream classification task. This performance surpasses other methods, with BYOL attaining an AUPRC of 0.75, SimSiam at 0.74, SimCLR at 0.73 and masked autoencoding using SparK at 0.75. A self-supervised learning approach that inherently focuses on localizing paranasal anomalies proves to be advantageous, particularly when the subsequent task involves differentiating normal from anomalous maxillary sinuses. Access our code at https://github.com/mtec-tuhh/self-supervised-paranasal-anomaly .",
      "doi": "https://doi.org/10.1007/s11548-024-03172-5",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Self-Supervised Learning Improves Accuracy and Data Efficiency for IMU-Based Ground Reaction Force Estimation.",
      "authors": [
        "Tan T",
        "Shull PB",
        "Hicks JL",
        "Uhlrich SD",
        "Chaudhari AS"
      ],
      "abstract": "Recent deep learning techniques hold promise to enable IMU-driven kinetic assessment; however, they require large extents of ground reaction force (GRF) data to serve as labels for supervised model training. We thus propose using existing self-supervised learning (SSL) techniques to leverage large IMU datasets to pre-train deep learning models, which can improve the accuracy and data efficiency of IMU-based GRF estimation. We performed SSL by masking a random portion of the input IMU data and training a transformer model to reconstruct the masked portion. We systematically compared a series of masking ratios across three pre-training datasets that included real IMU data, synthetic IMU data, or a combination of the two. Finally, we built models that used pre-training and labeled data to estimate GRF during three prediction tasks: overground walking, treadmill walking, and drop landing. When using the same amount of labeled data, SSL pre-training significantly improved the accuracy of 3-axis GRF estimation during walking compared to baseline models trained by conventional supervised learning. Fine-tuning SSL model with 1-10% of walking data yielded comparable accuracy to training baseline model with 100% of walking data. The optimal masking ratio for SSL is 6.25-12.5%. SSL leveraged large real and synthetic IMU datasets to increase the accuracy and data efficiency of deep-learning-based GRF estimation, reducing the need for labeled data. This work, with its open-source code and models, may unlock broader use cases of IMU-driven kinetic assessment by mitigating the scarcity of GRF measurements in practical applications.",
      "doi": "https://pubmed.ncbi.nlm.nih.gov/38328126/",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning",
      "authors": [
        "Rex Liu",
        "Xin Liu"
      ],
      "abstract": "With the exponential growth of multimedia data, leveraging multimodal sensors\npresents a promising approach for improving accuracy in human activity\nrecognition. Nevertheless, accurately identifying these activities using both\nvideo data and wearable sensor data presents challenges due to the\nlabor-intensive data annotation, and reliance on external pretrained models or\nadditional data. To address these challenges, we introduce Multimodal Masked\nAutoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal\nmasked autoencoder with a synchronized masking strategy tailored for wearable\nsensors. This masking strategy compels the networks to capture more meaningful\nspatiotemporal features, which enables effective self-supervised pretraining\nwithout the need for external data. Furthermore, Mu-MAE leverages the\nrepresentation extracted from multimodal masked autoencoders as prior\ninformation input to a cross-attention multimodal fusion layer. This fusion\nlayer emphasizes spatiotemporal features requiring attention across different\nmodalities while highlighting differences from other classes, aiding in the\nclassification of various classes in metric-based one-shot learning.\nComprehensive evaluations on MMAct one-shot classification show that Mu-MAE\noutperforms all the evaluated approaches, achieving up to an 80.17% accuracy\nfor five-way one-shot multimodal classification, without the use of additional\ndata.",
      "doi": "arXiv:2408.04243v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Bootstrapped Masked Autoencoders for Vision BERT Pretraining",
      "authors": [
        "Xiaoyi Dong",
        "Jianmin Bao",
        "Ting Zhang",
        "Dongdong Chen",
        "Weiming Zhang",
        "Lu Yuan",
        "Dong Chen",
        "Fang Wen",
        "Nenghai Yu"
      ],
      "abstract": "We propose bootstrapped masked autoencoders (BootMAE), a new approach for\nvision BERT pretraining. BootMAE improves the original masked autoencoders\n(MAE) with two core designs: 1) momentum encoder that provides online feature\nas extra BERT prediction targets; 2) target-aware decoder that tries to reduce\nthe pressure on the encoder to memorize target-specific information in BERT\npretraining. The first design is motivated by the observation that using a\npretrained MAE to extract the features as the BERT prediction target for masked\ntokens can achieve better pretraining performance. Therefore, we add a momentum\nencoder in parallel with the original MAE encoder, which bootstraps the\npretraining performance by using its own representation as the BERT prediction\ntarget. In the second design, we introduce target-specific information (e.g.,\npixel values of unmasked patches) from the encoder directly to the decoder to\nreduce the pressure on the encoder of memorizing the target-specific\ninformation. Thus, the encoder focuses on semantic modeling, which is the goal\nof BERT pretraining, and does not need to waste its capacity in memorizing the\ninformation of unmasked tokens related to the prediction target. Through\nextensive experiments, our BootMAE achieves $84.2\\%$ Top-1 accuracy on\nImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\\%$ under the same\npre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic\nsegmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object\ndetection and segmentation on COCO dataset. Code is released at\nhttps://github.com/LightDXY/BootMAE.",
      "doi": "arXiv:2207.07116v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical\n  Flow Estimation",
      "authors": [
        "Xiaoyu Shi",
        "Zhaoyang Huang",
        "Dasong Li",
        "Manyuan Zhang",
        "Ka Chun Cheung",
        "Simon See",
        "Hongwei Qin",
        "Jifeng Dai",
        "Hongsheng Li"
      ],
      "abstract": "FlowFormer introduces a transformer architecture into optical flow estimation\nand achieves state-of-the-art performance. The core component of FlowFormer is\nthe transformer-based cost-volume encoder. Inspired by the recent success of\nmasked autoencoding (MAE) pretraining in unleashing transformers' capacity of\nencoding visual representation, we propose Masked Cost Volume Autoencoding\n(MCVA) to enhance FlowFormer by pretraining the cost-volume encoder with a\nnovel MAE scheme. Firstly, we introduce a block-sharing masking strategy to\nprevent masked information leakage, as the cost maps of neighboring source\npixels are highly correlated. Secondly, we propose a novel pre-text\nreconstruction task, which encourages the cost-volume encoder to aggregate\nlong-range information and ensures pretraining-finetuning consistency. We also\nshow how to modify the FlowFormer architecture to accommodate masks during\npretraining. Pretrained with MCVA, FlowFormer++ ranks 1st among published\nmethods on both Sintel and KITTI-2015 benchmarks. Specifically, FlowFormer++\nachieves 1.07 and 1.94 average end-point error (AEPE) on the clean and final\npass of Sintel benchmark, leading to 7.76\\% and 7.18\\% error reductions from\nFlowFormer. FlowFormer++ obtains 4.52 F1-all on the KITTI-2015 test set,\nimproving FlowFormer by 0.16.",
      "doi": "arXiv:2303.01237v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential\n  with Masked Autoregressive Pretraining",
      "authors": [
        "Yunze Liu",
        "Li Yi"
      ],
      "abstract": "Hybrid Mamba-Transformer networks have recently garnered broad attention.\nThese networks can leverage the scalability of Transformers while capitalizing\non Mamba's strengths in long-context modeling and computational efficiency.\nHowever, the challenge of effectively pretraining such hybrid networks remains\nan open question. Existing methods, such as Masked Autoencoders (MAE) or\nautoregressive (AR) pretraining, primarily focus on single-type network\narchitectures. In contrast, pretraining strategies for hybrid architectures\nmust be effective for both Mamba and Transformer components. Based on this, we\npropose Masked Autoregressive Pretraining (MAP) to pretrain a hybrid\nMamba-Transformer vision backbone network. This strategy combines the strengths\nof both MAE and Autoregressive pretraining, improving the performance of Mamba\nand Transformer modules within a unified paradigm. Experimental results show\nthat the hybrid Mamba-Transformer vision backbone network pretrained with MAP\nsignificantly outperforms other pretraining strategies, achieving\nstate-of-the-art performance. We validate the method's effectiveness on both 2D\nand 3D datasets and provide detailed ablation studies to support the design\nchoices for each component. The code and checkpoints are available at\nhttps://github.com/yunzeliu/MAP",
      "doi": "arXiv:2410.00871v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "BatmanNet: bi-branch masked graph transformer autoencoder for molecular representation.",
      "authors": [
        "Wang Z",
        "Feng Z",
        "Li Y",
        "Li B",
        "Wang Y",
        "Sha C",
        "He M",
        "Li X"
      ],
      "abstract": "Although substantial efforts have been made using graph neural networks (GNNs) for artificial intelligence (AI)-driven drug discovery, effective molecular representation learning remains an open challenge, especially in the case of insufficient labeled molecules. Recent studies suggest that big GNN models pre-trained by self-supervised learning on unlabeled datasets enable better transfer performance in downstream molecular property prediction tasks. However, the approaches in these studies require multiple complex self-supervised tasks and large-scale datasets , which are time-consuming, computationally expensive and difficult to pre-train end-to-end. Here, we design a simple yet effective self-supervised strategy to simultaneously learn local and global information about molecules, and further propose a novel bi-branch masked graph transformer autoencoder (BatmanNet) to learn molecular representations. BatmanNet features two tailored complementary and asymmetric graph autoencoders to reconstruct the missing nodes and edges, respectively, from a masked molecular graph. With this design, BatmanNet can effectively capture the underlying structure and semantic information of molecules, thus improving the performance of molecular representation. BatmanNet achieves state-of-the-art results for multiple drug discovery tasks, including molecular properties prediction, drug-drug interaction and drug-target interaction, on 13 benchmark datasets, demonstrating its great potential and superiority in molecular representation learning.",
      "doi": "https://doi.org/10.1093/bib/bbad400",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Teaching Masked Autoencoder With Strong Augmentations.",
      "authors": [
        "Zhu R",
        "Bai Y",
        "Yao T",
        "Liu J",
        "Sun Z",
        "Mei T",
        "Wen Chen C"
      ],
      "abstract": "Masked autoencoder (MAE) has been regarded as a capable self-supervised learner for various downstream tasks. Nevertheless, the model still lacks high-level discriminability, which results in poor linear probing performance. In view of the fact that strong augmentation plays an essential role in contrastive learning, can we capitalize on strong augmentation in MAE? The difficulty originates from the pixel uncertainty caused by strong augmentation that may affect the reconstruction, and thus, directly introducing strong augmentation into MAE often hurts the performance. In this article, we delve into the potential of strong augmented views to enhance MAE while maintaining MAE's advantages. To this end, we propose a simple yet effective masked Siamese autoencoder (MSA) model, which consists of a student branch and a teacher branch. The student branch derives MAE's advanced architecture, and the teacher branch treats the unmasked strong view as an exemplary teacher to impose high-level discrimination onto the student branch. We demonstrate that our MSA can improve the model's spatial perception capability and, therefore, globally favors interimage discrimination. Empirical evidence shows that the model pretrained by MSA provides superior performances across different downstream tasks. Notably, linear probing performance on frozen features extracted from MSA leads to 6.1% gains over MAE on ImageNet-1k. Fine-tuning (FT) the network on VQAv2 task finally achieves 67.4% accuracy, outperforming 1.6% of the supervised method DeiT and 1.2% of MAE. Codes and models are available at https://github.com/KimSoybean/MSA.",
      "doi": "https://doi.org/10.1109/tnnls.2024.3419898",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "GMAEEG: A Self-Supervised Graph Masked Autoencoder for EEG Representation Learning.",
      "authors": [
        "Fu Z",
        "Zhu H",
        "Zhao Y",
        "Huan R",
        "Zhang Y",
        "Chen S",
        "Pan Y"
      ],
      "abstract": "Annotated electroencephalogram (EEG) data is the prerequisite for artificial intelligence-driven EEG autoanalysis. However, the scarcity of annotated data due to its high-cost and the resulted insufficient training limits the development of EEG autoanalysis. Generative self-supervised learning, represented by masked autoencoder, offers potential but struggles with non-Euclidean structures. To alleviate these challenges, this work proposes a self-supervised graph masked autoencoder for EEG representation learning, named GMAEEG. Concretely, a pretrained model is enriched with temporal and spatial representations through a masked signal reconstruction pretext task. A learnable dynamic adjacency matrix, initialized with prior knowledge, adapts to brain characteristics. Downstream tasks are achieved by finetuning pretrained parameters, with the adjacency matrix transferred based on task functional similarity. Experimental results demonstrate that with emotion recognition as the pretext task, GMAEEG reaches superior performance on various downstream tasks, including emotion, major depressive disorder, Parkinson's disease, and pain recognition. This study is the first to tailor the masked autoencoder specifically for EEG representation learning considering its non-Euclidean characteristics. Further, graph connection analysis based on GMAEEG may provide insights for future clinical studies.",
      "doi": "https://doi.org/10.1109/jbhi.2024.3443651",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Swin MAE: Masked autoencoders for small datasets.",
      "authors": [
        "Xu Z",
        "Dai Y",
        "Liu F",
        "Chen W",
        "Liu Y",
        "Shi L",
        "Liu S",
        "Zhou Y"
      ],
      "abstract": "The development of deep learning models in medical image analysis is majorly limited by the lack of large-sized and well-annotated datasets. Unsupervised learning does not require labels and is more suitable for solving medical image analysis problems. However, most unsupervised learning methods must be applied to large datasets. To make unsupervised learning applicable to small datasets, we proposed Swin MAE, a masked autoencoder with Swin Transformer as its backbone. Even on a dataset of only a few thousand medical images, Swin MAE can still learn useful semantic features purely from images without using any pre-trained models. It can equal or even slightly outperform the supervised model obtained by Swin Transformer trained on ImageNet in the transfer learning results of downstream tasks. Compared to MAE, Swin MAE brought a performance improvement of twice and five times for downstream tasks on BTCV and our parotid dataset, respectively. The code is publicly available at https://github.com/Zian-Xu/Swin-MAE.",
      "doi": "https://doi.org/10.1016/j.compbiomed.2023.107037",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Look Ahead or Look Around? A Theoretical Comparison Between\n  Autoregressive and Masked Pretraining",
      "authors": [
        "Qi Zhang",
        "Tianqi Du",
        "Haotian Huang",
        "Yifei Wang",
        "Yisen Wang"
      ],
      "abstract": "In recent years, the rise of generative self-supervised learning (SSL)\nparadigms has exhibited impressive performance across visual, language, and\nmulti-modal domains. While the varied designs of generative SSL objectives lead\nto distinct properties in downstream tasks, a theoretical understanding of\nthese differences remains largely unexplored. In this paper, we establish the\nfirst theoretical comparisons between two leading generative SSL paradigms:\nautoregressive SSL and masked SSL. Through establishing theoretical frameworks,\nwe elucidate the strengths and limitations of autoregressive and masked SSL\nwithin the primary evaluation tasks of classification and content generation.\nOur findings demonstrate that in classification tasks, the flexibility of\ntargeted tokens in masked SSL fosters more inter-sample connections compared to\nthe fixed position of target tokens in autoregressive SSL, which yields\nsuperior clustering performance. In content generation tasks, the misalignment\nbetween the flexible lengths of test samples and the fixed length of unmasked\ntexts in masked SSL (vs. flexible lengths of conditional texts in\nautoregressive SSL) hinders its generation performance. To leverage each\nother's strengths and mitigate weaknesses, we propose diversity-enhanced\nautoregressive and variable-length masked objectives, which substantially\nimprove the classification performance of autoregressive SSL and the generation\nperformance of masked SSL. Code is available at\nhttps://github.com/PKU-ML/LookAheadLookAround.",
      "doi": "arXiv:2407.00935v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "End-to-End Lyrics Recognition with Self-supervised Learning",
      "authors": [
        "Xiangyu Zhang",
        "Shuyue Stella Li",
        "Zhanhong He",
        "Roberto Togneri",
        "Leibny Paola Garcia"
      ],
      "abstract": "Lyrics recognition is an important task in music processing. Despite\ntraditional algorithms such as the hybrid HMM- TDNN model achieving good\nperformance, studies on applying end-to-end models and self-supervised learning\n(SSL) are limited. In this paper, we first establish an end-to-end baseline for\nlyrics recognition and then explore the performance of SSL models on lyrics\nrecognition task. We evaluate a variety of upstream SSL models with different\ntraining methods (masked reconstruction, masked prediction, autoregressive\nreconstruction, and contrastive learning). Our end-to-end self-supervised\nmodels, evaluated on the DAMP music dataset, outperform the previous\nstate-of-the-art (SOTA) system by 5.23% for the dev set and 2.4% for the test\nset even without a language model trained by a large corpus. Moreover, we\ninvestigate the effect of background music on the performance of\nself-supervised learning models and conclude that the SSL models cannot extract\nfeatures efficiently in the presence of background music. Finally, we study the\nout-of-domain generalization ability of the SSL features considering that those\nmodels were not trained on music datasets.",
      "doi": "arXiv:2209.12702v4",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Towards Better Domain Adaptation for Self-supervised Models: A Case\n  Study of Child ASR",
      "authors": [
        "Ruchao Fan",
        "Yunzheng Zhu",
        "Jinhan Wang",
        "Abeer Alwan"
      ],
      "abstract": "Recently, self-supervised learning (SSL) from unlabelled speech data has\ngained increased attention in the automatic speech recognition (ASR) community.\nTypical SSL methods include autoregressive predictive coding (APC), Wav2vec2.0,\nand hidden unit BERT (HuBERT). However, SSL models are biased to the\npretraining data. When SSL models are finetuned with data from another domain,\ndomain shifting occurs and might cause limited knowledge transfer for\ndownstream tasks. In this paper, we propose a novel framework, domain\nresponsible adaptation and finetuning (DRAFT), to reduce domain shifting in\npretrained speech models, and evaluate it for a causal and non-causal\ntransformer. For the causal transformer, an extension of APC (E-APC) is\nproposed to learn richer information from unlabelled data by using multiple\ntemporally-shifted sequences to perform prediction. For the non-causal\ntransformer, various solutions for using the bidirectional APC (Bi-APC) are\ninvestigated. In addition, the DRAFT framework is examined for Wav2vec2.0 and\nHuBERT methods, which use non-causal transformers as the backbone. The\nexperiments are conducted on child ASR (using the OGI and MyST databases) using\nSSL models trained with unlabelled adult speech data from Librispeech. The\nrelative WER improvements of up to 19.7% on the two child tasks are observed\nwhen compared to the pretrained models without adaptation. With the proposed\nmethods (E-APC and DRAFT), the relative WER improvements are even larger (30%\nand 19% on the OGI and MyST data, respectively) when compared to the models\nwithout using pretraining methods.",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3200910",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec\n  Transformer",
      "authors": [
        "Yuancheng Wang",
        "Haoyue Zhan",
        "Liwei Liu",
        "Ruihong Zeng",
        "Haotian Guo",
        "Jiachen Zheng",
        "Qiang Zhang",
        "Xueyao Zhang",
        "Shunsi Zhang",
        "Zhizheng Wu"
      ],
      "abstract": "The recent large-scale text-to-speech (TTS) systems are usually grouped as\nautoregressive and non-autoregressive systems. The autoregressive systems\nimplicitly model duration but exhibit certain deficiencies in robustness and\nlack of duration controllability. Non-autoregressive systems require explicit\nalignment information between text and speech during training and predict\ndurations for linguistic units (e.g. phone), which may compromise their\nnaturalness. In this paper, we introduce Masked Generative Codec Transformer\n(MaskGCT), a fully non-autoregressive TTS model that eliminates the need for\nexplicit alignment information between text and speech supervision, as well as\nphone-level duration prediction. MaskGCT is a two-stage model: in the first\nstage, the model uses text to predict semantic tokens extracted from a speech\nself-supervised learning (SSL) model, and in the second stage, the model\npredicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows\nthe mask-and-predict learning paradigm. During training, MaskGCT learns to\npredict masked semantic or acoustic tokens based on given conditions and\nprompts. During inference, the model generates tokens of a specified length in\na parallel manner. Experiments with 100K hours of in-the-wild speech\ndemonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS\nsystems in terms of quality, similarity, and intelligibility. Audio samples are\navailable at https://maskgct.github.io/. We release our code and model\ncheckpoints at\nhttps://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "doi": "arXiv:2409.00750v3",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Incorporating spatial structure into inclusion probabilities for Bayesian variable selection in generalized linear models with the spike-and-slab elastic net.",
      "authors": [
        "Leach JM",
        "Aban I",
        "Yi N",
        "Alzheimer’s Disease Neuroimaging Initiative"
      ],
      "abstract": "Spike-and-slab priors model predictors as arising from a mixture of distributions: those that should (slab) or should not (spike) remain in the model. The spike-and-slab lasso (SSL) is a mixture of double exponentials, extending the single lasso penalty by imposing different penalties on parameters based on their inclusion probabilities. The SSL was extended to Generalized Linear Models (GLM) for application in genetics/genomics, and can handle many highly correlated predictors of a scalar outcome, but does not incorporate these relationships into variable selection. When images/spatial data are used to model a scalar outcome, relevant parameters tend to cluster spatially, and model performance may benefit from incorporating spatial structure into variable selection. We propose to incorporate spatial information by assigning intrinsic autoregressive priors to the logit prior probabilities of inclusion, which results in more similar shrinkage penalties among spatially adjacent parameters. Using MCMC to fit Bayesian models can be computationally prohibitive for large-scale data, but we fit the model by adapting a computationally efficient coordinate-descent-based EM algorithm. A simulation study and an application to Alzheimer's Disease imaging data show that incorporating spatial information can improve model fitness.",
      "doi": "https://doi.org/10.1016/j.jspi.2021.07.010",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Representation Uncertainty in Self-Supervised Learning as Variational\n  Inference",
      "authors": [
        "Hiroki Nakamura",
        "Masashi Okada",
        "Tadahiro Taniguchi"
      ],
      "abstract": "In this study, a novel self-supervised learning (SSL) method is proposed,\nwhich considers SSL in terms of variational inference to learn not only\nrepresentation but also representation uncertainties. SSL is a method of\nlearning representations without labels by maximizing the similarity between\nimage representations of different augmented views of an image. Meanwhile,\nvariational autoencoder (VAE) is an unsupervised representation learning method\nthat trains a probabilistic generative model with variational inference. Both\nVAE and SSL can learn representations without labels, but their relationship\nhas not been investigated in the past. Herein, the theoretical relationship\nbetween SSL and variational inference has been clarified. Furthermore, a novel\nmethod, namely variational inference SimSiam (VI-SimSiam), has been proposed.\nVI-SimSiam can predict the representation uncertainty by interpreting SimSiam\nwith variational inference and defining the latent space distribution. The\npresent experiments qualitatively show that VI- SimSiam could learn uncertainty\nby comparing input images and predicted uncertainties. Additionally, we\ndescribed a relationship between estimated uncertainty and classification\naccuracy.",
      "doi": "arXiv:2203.11437v4",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Universal Semi-Supervised Learning for Medical Image Classification",
      "authors": [
        "Lie Ju",
        "Yicheng Wu",
        "Wei Feng",
        "Zhen Yu",
        "Lin Wang",
        "Zhuoting Zhu",
        "Zongyuan Ge"
      ],
      "abstract": "Semi-supervised learning (SSL) has attracted much attention since it reduces\nthe expensive costs of collecting adequate well-labeled training data,\nespecially for deep learning methods. However, traditional SSL is built upon an\nassumption that labeled and unlabeled data should be from the same distribution\n\\textit{e.g.,} classes and domains. However, in practical scenarios, unlabeled\ndata would be from unseen classes or unseen domains, and it is still\nchallenging to exploit them by existing SSL methods. Therefore, in this paper,\nwe proposed a unified framework to leverage these unseen unlabeled data for\nopen-scenario semi-supervised medical image classification. We first design a\nnovel scoring mechanism, called dual-path outliers estimation, to identify\nsamples from unseen classes. Meanwhile, to extract unseen-domain samples, we\nthen apply an effective variational autoencoder (VAE) pre-training. After that,\nwe conduct domain adaptation to fully exploit the value of the detected\nunseen-domain samples to boost semi-supervised training. We evaluated our\nproposed framework on dermatology and ophthalmology tasks. Extensive\nexperiments demonstrate our model can achieve superior classification\nperformance in various medical SSL scenarios. The code implementations are\naccessible at: https://github.com/PyJulie/USSL4MIC.",
      "doi": "arXiv:2304.04059v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Weakly Augmented Variational Autoencoder in Time Series Anomaly\n  Detection",
      "authors": [
        "Zhangkai Wu",
        "Longbing Cao",
        "Qi Zhang",
        "Junxian Zhou",
        "Hui Chen"
      ],
      "abstract": "Due to their unsupervised training and uncertainty estimation, deep\nVariational Autoencoders (VAEs) have become powerful tools for\nreconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based\nTSAD methods, either statistical or deep, tune meta-priors to estimate the\nlikelihood probability for effectively capturing spatiotemporal dependencies in\nthe data. However, these methods confront the challenge of inherent data\nscarcity, which is often the case in anomaly detection tasks. Such scarcity\neasily leads to latent holes, discontinuous regions in latent space, resulting\nin non-robust reconstructions on these discontinuous spaces. We propose a novel\ngenerative framework that combines VAEs with self-supervised learning (SSL) to\naddress this issue.",
      "doi": "arXiv:2401.03341v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Semi-supervised source localization in reverberant environments with\n  deep generative modeling",
      "authors": [
        "Michael J. Bianco",
        "Sharon Gannot",
        "Efren Fernandez-Grande",
        "Peter Gerstoft"
      ],
      "abstract": "We propose a semi-supervised approach to acoustic source localization in\nreverberant environments based on deep generative modeling. Localization in\nreverberant environments remains an open challenge. Even with large data\nvolumes, the number of labels available for supervised learning in reverberant\nenvironments is usually small. We address this issue by performing\nsemi-supervised learning (SSL) with convolutional variational autoencoders\n(VAEs) on reverberant speech signals recorded with microphone arrays. The VAE\nis trained to generate the phase of relative transfer functions (RTFs) between\nmicrophones, in parallel with a direction of arrival (DOA) classifier based on\nRTF-phase. These models are trained using both labeled and unlabeled RTF-phase\nsequences. In learning to perform these tasks, the VAE-SSL explicitly learns to\nseparate the physical causes of the RTF-phase (i.e., source location) from\ndistracting signal characteristics such as noise and speech activity. Relative\nto existing semi-supervised localization methods in acoustics, VAE-SSL is\neffectively an end-to-end processing approach which relies on minimal\npreprocessing of RTF-phase features. As far as we are aware, our paper presents\nthe first approach to modeling the physics of acoustic propagation using deep\ngenerative modeling. The VAE-SSL approach is compared with two signal\nprocessing-based approaches, steered response power with phase transform\n(SRP-PHAT) and MUltiple SIgnal Classification (MUSIC), as well as fully\nsupervised CNNs. We find that VAE-SSL can outperform the conventional\napproaches and the CNN in label-limited scenarios. Further, the trained VAE-SSL\nsystem can generate new RTF-phase samples, which shows the VAE-SSL approach\nlearns the physics of the acoustic environment. The generative modeling in\nVAE-SSL thus provides a means of interpreting the learned representations.",
      "doi": "arXiv:2101.10636v2",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Predicting Drug-Target Interaction Via Self-Supervised Learning.",
      "authors": [
        "Chen J",
        "Zhang L",
        "Cheng K",
        "Jin B",
        "Lu X",
        "Che C"
      ],
      "abstract": "Recent advances in graph representation learning provide new opportunities for computational drug-target interaction (DTI) prediction. However, it still suffers from deficiencies of dependence on manual labels and vulnerability to attacks. Inspired by the success of self-supervised learning (SSL) algorithms, which can leverage input data itself as supervision,we propose SupDTI, a SSL-enhanced drug-target interaction prediction framework based on a heterogeneous network (i.e., drug-protein, drug-drug, and protein-protein interaction network; drug-disease, drug-side-effect, and protein-disease association network; drug-structure and protein-sequence similarity network). Specifically, SupDTI is an end-to-end learning framework consisting of five components. First, localized and globalized graph convolutions are designed to capture the nodes' information from both local and global perspectives, respectively. Then, we develop a variational autoencoder to constrain the nodes' representation to have desired statistical characteristics. Finally, a unified self-supervised learning strategy is leveraged to enhance the nodes' representation, namely, a contrastive learning module is employed to enable the nodes' representation to fit the graph-level representation, followed by a generative learning module which further maximizes the node-level agreement across the global and local views by learning the probabilistic connectivity distribution of the original heterogeneous network. Experimental results show that our model can achieve better prediction performance than state-of-the-art methods.",
      "doi": "https://doi.org/10.1109/tcbb.2022.3153963",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "SSL-VQ: vector-quantized variational autoencoders for semi-supervised prediction of therapeutic targets across diverse diseases.",
      "authors": [
        "Namba S",
        "Li C",
        "Yuyama Otani N",
        "Yamanishi Y"
      ],
      "abstract": "Identifying effective therapeutic targets poses a challenge in drug discovery, especially for uncharacterized diseases without known therapeutic targets (e.g. rare diseases, intractable diseases). This study presents a novel machine learning approach using multimodal vector-quantized variational autoencoders (VQ-VAEs) for predicting therapeutic target molecules across diseases. To address the lack of known therapeutic target-disease associations, we incorporate the information on uncharacterized diseases without known targets or uncharacterized proteins without known indications (applicable diseases) in the semi-supervised learning (SSL) framework. The method integrates disease-specific and protein perturbation profiles with genetic perturbations (e.g. gene knockdowns and gene overexpressions) at the transcriptome level. Cross-cell representation learning, facilitated by VQ-VAEs, was performed to extract informative features from protein perturbation profiles across diverse human cell types. Concurrently, cross-disease representation learning was performed, leveraging VQ-VAE, to extract informative features reflecting disease states from disease-specific profiles. The model's applicability to uncharacterized diseases or proteins is enhanced by considering the consistency between disease-specific and patient-specific signatures. The efficacy of the method is demonstrated across three practical scenarios for 79 diseases: target repositioning for target-disease pairs, new target prediction for uncharacterized diseases, and new indication prediction for uncharacterized proteins. This method is expected to be valuable for identifying therapeutic targets across various diseases. Code: github.com/YamanishiLab/SSL-VQ and Data: 10.5281/zenodo.14644837.",
      "doi": "https://doi.org/10.1093/bioinformatics/btaf039",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "Learning Deep Representations of Cardiac Structures for 4D Cine MRI Image Segmentation through Semi-Supervised Learning.",
      "authors": [
        "Hasan SMK",
        "Linte CA"
      ],
      "abstract": "Learning good data representations for medical imaging tasks ensures the preservation of relevant information and the removal of irrelevant information from the data to improve the interpretability of the learned features. In this paper, we propose a semi-supervised model-namely, combine-all in semi-supervised learning (C",
      "doi": "https://pubmed.ncbi.nlm.nih.gov/37125242/",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Generative Adversarial Networks and Adversarial Autoencoders: Tutorial\n  and Survey",
      "authors": [
        "Benyamin Ghojogh",
        "Ali Ghodsi",
        "Fakhri Karray",
        "Mark Crowley"
      ],
      "abstract": "This is a tutorial and survey paper on Generative Adversarial Network (GAN),\nadversarial autoencoders, and their variants. We start with explaining\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\nand DCGAN. The mode collapse problem is introduced and various methods,\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\nintroduce some applications of GAN such as image-to-image translation\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\nGAN), text-to-image translation (including StackGAN), and mixing image\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\nautoencoders based on adversarial learning including adversarial autoencoder,\nPixelGAN, and implicit autoencoder.",
      "doi": "arXiv:2111.13282v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection",
      "authors": [
        "Yannick Burkhardt",
        "Simon Schaefer",
        "Stefan Leutenegger"
      ],
      "abstract": "Event-based keypoint detection and matching holds significant potential,\nenabling the integration of event sensors into highly optimized Visual SLAM\nsystems developed for frame cameras over decades of research. Unfortunately,\nexisting approaches struggle with the motion-dependent appearance of keypoints\nand the complex noise prevalent in event streams, resulting in severely limited\nfeature matching capabilities and poor performance on downstream tasks. To\nmitigate this problem, we propose SuperEvent, a data-driven approach to predict\nstable keypoints with expressive descriptors. Due to the absence of event\ndatasets with ground truth keypoint labels, we leverage existing frame-based\nkeypoint detectors on readily available event-aligned and synchronized\ngray-scale frames for self-supervision: we generate temporally sparse keypoint\npseudo-labels considering that events are a product of both scene appearance\nand camera motion. Combined with our novel, information-rich event\nrepresentation, we enable SuperEvent to effectively learn robust keypoint\ndetection and description in event streams. Finally, we demonstrate the\nusefulness of SuperEvent by its integration into a modern sparse keypoint and\ndescriptor-based SLAM framework originally developed for traditional cameras,\nsurpassing the state-of-the-art in event-based SLAM by a wide margin. Source\ncode and multimedia material are available at\nsmartroboticslab.github.io/SuperEvent.",
      "doi": "arXiv:2504.00139v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "Understanding Latent Correlation-Based Multiview Learning and\n  Self-Supervision: An Identifiability Perspective",
      "authors": [
        "Qi Lyu",
        "Xiao Fu",
        "Weiran Wang",
        "Songtao Lu"
      ],
      "abstract": "Multiple views of data, both naturally acquired (e.g., image and audio) and\nartificially produced (e.g., via adding different noise to data samples), have\nproven useful in enhancing representation learning. Natural views are often\nhandled by multiview analysis tools, e.g., (deep) canonical correlation\nanalysis [(D)CCA], while the artificial ones are frequently used in\nself-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins. Both\ntypes of approaches often involve learning neural feature extractors such that\nthe embeddings of data exhibit high cross-view correlations. Although\nintuitive, the effectiveness of correlation-based neural embedding is mostly\nempirically validated.\n  This work aims to understand latent correlation maximization-based deep\nmultiview learning from a latent component identification viewpoint. An\nintuitive generative model of multiview data is adopted, where the views are\ndifferent nonlinear mixtures of shared and private components. Since the shared\ncomponents are view/distortion-invariant, representing the data using such\ncomponents is believed to reveal the identity of the samples effectively and\nrobustly. Under this model, latent correlation maximization is shown to\nguarantee the extraction of the shared components across views (up to certain\nambiguities). In addition, it is further shown that the private information in\neach view can be provably disentangled from the shared using proper\nregularization design. A finite sample analysis, which has been rare in\nnonlinear mixture identifiability study, is also presented. The theoretical\nresults and newly designed regularization are tested on a series of tasks.",
      "doi": "arXiv:2106.07115v3",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action\n  Representations",
      "authors": [
        "Luca Franco",
        "Paolo Mandica",
        "Bharti Munjal",
        "Fabio Galasso"
      ],
      "abstract": "Self-paced learning has been beneficial for tasks where some initial\nknowledge is available, such as weakly supervised learning and domain\nadaptation, to select and order the training sample sequence, from easy to\ncomplex. However its applicability remains unexplored in unsupervised learning,\nwhereby the knowledge of the task matures during training. We propose a novel\nHYperbolic Self-Paced model (HYSP) for learning skeleton-based action\nrepresentations. HYSP adopts self-supervision: it uses data augmentations to\ngenerate two views of the same sample, and it learns by matching one (named\nonline) to the other (the target). We propose to use hyperbolic uncertainty to\ndetermine the algorithmic learning pace, under the assumption that less\nuncertain samples should be more strongly driving the training, with a larger\nweight and pace. Hyperbolic uncertainty is a by-product of the adopted\nhyperbolic neural networks, it matures during training and it comes with no\nextra cost, compared to the established Euclidean SSL framework counterparts.\nWhen tested on three established skeleton-based action recognition datasets,\nHYSP outperforms the state-of-the-art on PKU-MMD I, as well as on 2 out of 3\ndownstream tasks on NTU-60 and NTU-120. Additionally, HYSP only uses positive\npairs and bypasses therefore the complex and computationally-demanding mining\nprocedures required for the negatives in contrastive techniques. Code is\navailable at https://github.com/paolomandica/HYSP.",
      "doi": "arXiv:2303.06242v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Unveiling the Power of Self-Supervision for Multi-View Multi-Human Association and Tracking.",
      "authors": [
        "Feng W",
        "Wang F",
        "Han R",
        "Gan Y",
        "Qian Z",
        "Hou J",
        "Wang S"
      ],
      "abstract": "Multi-view multi-human association and tracking (MvMHAT), is an emerging yet important problem for multi-person scene video surveillance, aiming to track a group of people over time in each view, as well as to identify the same person across different views at the same time, which is different from previous MOT and multi-camera MOT tasks only considering the over-time human tracking. This way, the videos for MvMHAT require more complex annotations while containing more information for self-learning. In this work, we tackle this problem with an end-to-end neural network in a self-supervised learning manner. Specifically, we propose to take advantage of the spatial-temporal self-consistency rationale by considering three properties of reflexivity, symmetry, and transitivity. Besides the reflexivity property that naturally holds, we design the self-supervised learning losses based on the properties of symmetry and transitivity, for both appearance feature learning and assignment matrix optimization, to associate multiple humans over time and across views. Furthermore, to promote the research on MvMHAT, we build two new large-scale benchmarks for the network training and testing of different algorithms. Extensive experiments on the proposed benchmarks verify the effectiveness of our method. We have released the benchmark and code to the public.",
      "doi": "https://doi.org/10.1109/tpami.2024.3463966",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "Something Always Works: A Self-Study of Strengths-based Coaching in Supervision",
      "authors": [
        "Steven Haberlin"
      ],
      "abstract": "<jats:p>Mentoring remains a major component of teacher education programs. Moving away from the traditional apprenticeship model, teacher educators have begun to adopt more affirming coaching practices that nurture the strengths and inner qualities of pre-service teachers. In this self-study, the researcher – an emerging teacher educator hoping to enhance his practice – investigated ways to help pre-service teachers discover and develop their individual strengths and how strength-based coaching might impact his beliefs and assumptions. Data were drawn from interviews, focus groups, lesson plans, and researcher journal reflections as well as participant-created written responses and illustrations. Themes were developed using content analysis. Findings involved the teacher educator realizing the need for a variety for strengths-based exploration tools, the practicality of including strengths discussion in observation conferences and lesson planning, and the gaining of a new, appreciative mindset. Implications suggest a pathway for other teacher educators to consider when implementing strengths-based coaching.</jats:p>",
      "doi": "https://doi.org/10.31045/jes.2.1.3",
      "year": 2019,
      "source": "Crossref"
    },
    {
      "title": "Semi-supervised Models are Strong Unsupervised Domain Adaptation\n  Learners",
      "authors": [
        "Yabin Zhang",
        "Haojian Zhang",
        "Bin Deng",
        "Shuai Li",
        "Kui Jia",
        "Lei Zhang"
      ],
      "abstract": "Unsupervised domain adaptation (UDA) and semi-supervised learning (SSL) are\ntwo typical strategies to reduce expensive manual annotations in machine\nlearning. In order to learn effective models for a target task, UDA utilizes\nthe available labeled source data, which may have different distributions from\nunlabeled samples in the target domain, while SSL employs few manually\nannotated target samples. Although UDA and SSL are seemingly very different\nstrategies, we find that they are closely related in terms of task objectives\nand solutions, and SSL is a special case of UDA problems. Based on this\nfinding, we further investigate whether SSL methods work on UDA tasks. By\nadapting eight representative SSL algorithms on UDA benchmarks, we show that\nSSL methods are strong UDA learners. Especially, state-of-the-art SSL methods\nsignificantly outperform existing UDA methods on the challenging UDA benchmark\nof DomainNet, and state-of-the-art UDA methods could be further enhanced with\nSSL techniques. We thus promote that SSL methods should be employed as\nbaselines in future UDA studies and expect that the revealed relationship\nbetween UDA and SSL could shed light on future UDA development. Codes are\navailable at \\url{https://github.com/YBZh}.",
      "doi": "arXiv:2106.00417v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised\n  Learning",
      "authors": [
        "Aarash Feizi",
        "Randall Balestriero",
        "Adriana Romero-Soriano",
        "Reihaneh Rabbany"
      ],
      "abstract": "We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a\ngeneral method to inject a priori knowledge into Self-Supervised Learning (SSL)\npositive samples selection. Current SSL methods leverage Data-Augmentations\n(DA) for generating positive samples and incorporate prior knowledge - an\nincorrect, or too weak DA will drastically reduce the quality of the learned\nrepresentation. GPS-SSL proposes instead to design a metric space where\nEuclidean distances become a meaningful proxy for semantic relationship. In\nthat space, it is now possible to generate positive samples from nearest\nneighbor sampling. Any prior knowledge can now be embedded into that metric\nspace independently from the employed DA. From its simplicity, GPS-SSL is\napplicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is\nin reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches\n85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We\ntherefore move a step forward towards the goal of making SSL less reliant on\nDA. We also show that even when using strong DAs, GPS-SSL outperforms the\nbaselines on under-studied domains. We evaluate GPS-SSL along with multiple\nbaseline SSL methods on numerous downstream datasets from different domains\nwhen the models use strong or minimal data augmentations. We hope that GPS-SSL\nwill open new avenues in studying how to inject a priori knowledge into SSL in\na principled manner.",
      "doi": "arXiv:2401.01990v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Representation Learning Dynamics of Self-Supervised Models",
      "authors": [
        "Pascal Esser",
        "Satyaki Mukherjee",
        "Debarghya Ghoshdastidar"
      ],
      "abstract": "Self-Supervised Learning (SSL) is an important paradigm for learning\nrepresentations from unlabelled data, and SSL with neural networks has been\nhighly successful in practice. However current theoretical analysis of SSL is\nmostly restricted to generalisation error bounds. In contrast, learning\ndynamics often provide a precise characterisation of the behaviour of neural\nnetworks based models but, so far, are mainly known in supervised settings. In\nthis paper, we study the learning dynamics of SSL models, specifically\nrepresentations obtained by minimising contrastive and non-contrastive losses.\nWe show that a naive extension of the dymanics of multivariate regression to\nSSL leads to learning trivial scalar representations that demonstrates\ndimension collapse in SSL. Consequently, we formulate SSL objectives with\northogonality constraints on the weights, and derive the exact (network width\nindependent) learning dynamics of the SSL models trained using gradient descent\non the Grassmannian manifold. We also argue that the infinite width\napproximation of SSL models significantly deviate from the neural tangent\nkernel approximations of supervised models. We numerically illustrate the\nvalidity of our theoretical findings, and discuss how the presented results\nprovide a framework for further theoretical analysis of contrastive and\nnon-contrastive SSL.",
      "doi": "arXiv:2309.02011v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Training Data Synthesis with Difficulty Controlled Diffusion Model",
      "authors": [
        "Zerun Wang",
        "Jiafeng Mao",
        "Xueting Wang",
        "Toshihiko Yamasaki"
      ],
      "abstract": "Semi-supervised learning (SSL) can improve model performance by leveraging\nunlabeled images, which can be collected from public image sources with low\ncosts. In recent years, synthetic images have become increasingly common in\npublic image sources due to rapid advances in generative models. Therefore, it\nis becoming inevitable to include existing synthetic images in the unlabeled\ndata for SSL. How this kind of contamination will affect SSL remains\nunexplored. In this paper, we introduce a new task, Real-Synthetic Hybrid SSL\n(RS-SSL), to investigate the impact of unlabeled data contaminated by synthetic\nimages for SSL. First, we set up a new RS-SSL benchmark to evaluate current SSL\nmethods and found they struggled to improve by unlabeled synthetic images,\nsometimes even negatively affected. To this end, we propose RSMatch, a novel\nSSL method specifically designed to handle the challenges of RS-SSL. RSMatch\neffectively identifies unlabeled synthetic data and further utilizes them for\nimprovement. Extensive experimental results show that RSMatch can transfer\nsynthetic unlabeled data from `obstacles' to `resources.' The effectiveness is\nfurther verified through ablation studies and visualization.",
      "doi": "arXiv:2411.18109v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Text-Independent Phone-to-Audio Alignment Leveraging SSL (TIPAA-SSL) Pre-Trained Model Latent Representation and Knowledge Transfer",
      "authors": [
        "Noé Tits",
        "Prernna Bhatnagar",
        "Thierry Dutoit"
      ],
      "abstract": "<jats:p>In this paper, we present a novel approach for text-independent phone-to-audio alignment based on phoneme recognition, representation learning and knowledge transfer. Our method leverages a self-supervised model (Wav2Vec2) fine-tuned for phoneme recognition using a Connectionist Temporal Classification (CTC) loss, a dimension reduction model and a frame-level phoneme classifier trained using forced-alignment labels (using Montreal Forced Aligner) to produce multi-lingual phonetic representations, thus requiring minimal additional training. We evaluate our model using synthetic native data from the TIMIT dataset and the SCRIBE dataset for American and British English, respectively. Our proposed model outperforms the state-of-the-art (charsiu) in statistical metrics and has applications in language learning and speech processing systems. We leave experiments on other languages for future work but the design of the system makes it easily adaptable to other languages.</jats:p>",
      "doi": "https://doi.org/10.3390/acoustics6030042",
      "year": 2024,
      "source": "Crossref"
    },
    {
      "title": "VCC-INFUSE: Towards Accurate and Efficient Selection of Unlabeled\n  Examples in Semi-supervised Learning",
      "authors": [
        "Shijie Fang",
        "Qianhan Feng",
        "Tong Lin"
      ],
      "abstract": "Despite the progress of Semi-supervised Learning (SSL), existing methods fail\nto utilize unlabeled data effectively and efficiently. Many pseudo-label-based\nmethods select unlabeled examples based on inaccurate confidence scores from\nthe classifier. Most prior work also uses all available unlabeled data without\npruning, making it difficult to handle large amounts of unlabeled data. To\naddress these issues, we propose two methods: Variational Confidence\nCalibration (VCC) and Influence-Function-based Unlabeled Sample Elimination\n(INFUSE). VCC is an universal plugin for SSL confidence calibration, using a\nvariational autoencoder to select more accurate pseudo labels based on three\ntypes of consistency scores. INFUSE is a data pruning method that constructs a\ncore dataset of unlabeled examples under SSL. Our methods are effective in\nmultiple datasets and settings, reducing classification errors rates and saving\ntraining time. Together, VCC-INFUSE reduces the error rate of FlexMatch on the\nCIFAR-100 dataset by 1.08% while saving nearly half of the training time.",
      "doi": "arXiv:2404.11947v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Semi-supervised source localization with deep generative modeling",
      "authors": [
        "Michael J. Bianco",
        "Sharon Gannot",
        "Peter Gerstoft"
      ],
      "abstract": "We propose a semi-supervised localization approach based on deep generative\nmodeling with variational autoencoders (VAEs). Localization in reverberant\nenvironments remains a challenge, which machine learning (ML) has shown promise\nin addressing. Even with large data volumes, the number of labels available for\nsupervised learning in reverberant environments is usually small. We address\nthis issue by performing semi-supervised learning (SSL) with convolutional\nVAEs. The VAE is trained to generate the phase of relative transfer functions\n(RTFs), in parallel with a DOA classifier, on both labeled and unlabeled RTF\nsamples. The VAE-SSL approach is compared with SRP-PHAT and fully-supervised\nCNNs. We find that VAE-SSL can outperform both SRP-PHAT and CNN in\nlabel-limited scenarios.",
      "doi": "https://doi.org/10.1109/MLSP49062.2020.9231825",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "VAE-based Phoneme Alignment Using Gradient Annealing and SSL Acoustic\n  Features",
      "authors": [
        "Tomoki Koriyama"
      ],
      "abstract": "This paper presents an accurate phoneme alignment model that aims for speech\nanalysis and video content creation. We propose a variational autoencoder\n(VAE)-based alignment model in which a probable path is searched using encoded\nacoustic and linguistic embeddings in an unsupervised manner. Our proposed\nmodel is based on one TTS alignment (OTA) and extended to obtain phoneme\nboundaries. Specifically, we incorporate a VAE architecture to maintain\nconsistency between the embedding and input, apply gradient annealing to avoid\nlocal optimum during training, and introduce a self-supervised learning\n(SSL)-based acoustic-feature input and state-level linguistic unit to utilize\nrich and detailed information. Experimental results show that the proposed\nmodel generated phoneme boundaries closer to annotated ones compared with the\nconventional OTA model, the CTC-based segmentation model, and the widely-used\ntool MFA.",
      "doi": "https://doi.org/10.21437/Interspeech.2024-1127",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700\n  Robot Hours",
      "authors": [
        "Lerrel Pinto",
        "Abhinav Gupta"
      ],
      "abstract": "Current learning-based robot grasping approaches exploit human-labeled\ndatasets for training the models. However, there are two problems with such a\nmethodology: (a) since each object can be grasped in multiple ways, manually\nlabeling grasp locations is not a trivial task; (b) human labeling is biased by\nsemantics. While there have been attempts to train robots using trial-and-error\nexperiments, the amount of data used in such experiments remains substantially\nlow and hence makes the learner prone to over-fitting. In this paper, we take\nthe leap of increasing the available training data to 40 times more than prior\nwork, leading to a dataset size of 50K data points collected over 700 hours of\nrobot grasping attempts. This allows us to train a Convolutional Neural Network\n(CNN) for the task of predicting grasp locations without severe overfitting. In\nour formulation, we recast the regression problem to an 18-way binary\nclassification over image patches. We also present a multi-stage learning\napproach where a CNN trained in one stage is used to collect hard negatives in\nsubsequent stages. Our experiments clearly show the benefit of using\nlarge-scale datasets (and multi-stage training) for the task of grasping. We\nalso compare to several baselines and show state-of-the-art performance on\ngeneralization to unseen objects for grasping.",
      "doi": "arXiv:1509.06825v1",
      "year": 2015,
      "source": "arXiv"
    },
    {
      "title": "Social NCE: Contrastive Learning of Socially-aware Motion\n  Representations",
      "authors": [
        "Yuejiang Liu",
        "Qi Yan",
        "Alexandre Alahi"
      ],
      "abstract": "Learning socially-aware motion representations is at the core of recent\nadvances in multi-agent problems, such as human motion forecasting and robot\nnavigation in crowds. Despite promising progress, existing representations\nlearned with neural networks still struggle to generalize in closed-loop\npredictions (e.g., output colliding trajectories). This issue largely arises\nfrom the non-i.i.d. nature of sequential prediction in conjunction with\nill-distributed training data. Intuitively, if the training data only comes\nfrom human behaviors in safe spaces, i.e., from \"positive\" examples, it is\ndifficult for learning algorithms to capture the notion of \"negative\" examples\nlike collisions. In this work, we aim to address this issue by explicitly\nmodeling negative examples through self-supervision: (i) we introduce a social\ncontrastive loss that regularizes the extracted motion representation by\ndiscerning the ground-truth positive events from synthetic negative ones; (ii)\nwe construct informative negative samples based on our prior knowledge of rare\nbut dangerous circumstances. Our method substantially reduces the collision\nrates of recent trajectory forecasting, behavioral cloning and reinforcement\nlearning algorithms, outperforming state-of-the-art methods on several\nbenchmarks. Our code is available at https://github.com/vita-epfl/social-nce.",
      "doi": "arXiv:2012.11717v3",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Learning Topology-Specific Experts for Molecular Property Prediction",
      "authors": [
        "Su Kim",
        "Dongha Lee",
        "SeongKu Kang",
        "Seonghyeon Lee",
        "Hwanjo Yu"
      ],
      "abstract": "Recently, graph neural networks (GNNs) have been successfully applied to\npredicting molecular properties, which is one of the most classical\ncheminformatics tasks with various applications. Despite their effectiveness,\nwe empirically observe that training a single GNN model for diverse molecules\nwith distinct structural patterns limits its prediction performance. In this\npaper, motivated by this observation, we propose TopExpert to leverage\ntopology-specific prediction models (referred to as experts), each of which is\nresponsible for each molecular group sharing similar topological semantics.\nThat is, each expert learns topology-specific discriminative features while\nbeing trained with its corresponding topological group. To tackle the key\nchallenge of grouping molecules by their topological patterns, we introduce a\nclustering-based gating module that assigns an input molecule into one of the\nclusters and further optimizes the gating module with two different types of\nself-supervision: topological semantics induced by GNNs and molecular\nscaffolds, respectively. Extensive experiments demonstrate that TopExpert has\nboosted the performance for molecular property prediction and also achieved\nbetter generalization for new molecules with unseen scaffolds than baselines.\nThe code is available at https://github.com/kimsu55/ToxExpert.",
      "doi": "arXiv:2302.13693v3",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "No More Shortcuts: Realizing the Potential of Temporal Self-Supervision",
      "authors": [
        "Ishan Rajendrakumar Dave",
        "Simon Jenni",
        "Mubarak Shah"
      ],
      "abstract": "Self-supervised approaches for video have shown impressive results in video\nunderstanding tasks. However, unlike early works that leverage temporal\nself-supervision, current state-of-the-art methods primarily rely on tasks from\nthe image domain (e.g., contrastive learning) that do not explicitly promote\nthe learning of temporal features. We identify two factors that limit existing\ntemporal self-supervision: 1) tasks are too simple, resulting in saturated\ntraining performance, and 2) we uncover shortcuts based on local appearance\nstatistics that hinder the learning of high-level features. To address these\nissues, we propose 1) a more challenging reformulation of temporal\nself-supervision as frame-level (rather than clip-level) recognition tasks and\n2) an effective augmentation strategy to mitigate shortcuts. Our model extends\na representation of single video frames, pre-trained through contrastive\nlearning, with a transformer that we train through temporal self-supervision.\nWe demonstrate experimentally that our more challenging frame-level task\nformulations and the removal of shortcuts drastically improve the quality of\nfeatures learned through temporal self-supervision. The generalization\ncapability of our self-supervised video method is evidenced by its\nstate-of-the-art performance in a wide range of high-level semantic tasks,\nincluding video retrieval, action classification, and video attribute\nrecognition (such as object and scene identification), as well as low-level\ntemporal correspondence tasks like video object segmentation and pose tracking.\nAdditionally, we show that the video representations learned through our method\nexhibit increased robustness to the input perturbations.",
      "doi": "arXiv:2312.13008v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "From Obstacles to Resources: Semi-supervised Learning Faces Synthetic\n  Data Contamination",
      "authors": [
        "Zerun Wang",
        "Jiafeng Mao",
        "Liuyu Xiang",
        "Toshihiko Yamasaki"
      ],
      "abstract": "Semi-supervised learning (SSL) can improve model performance by leveraging\nunlabeled images, which can be collected from public image sources with low\ncosts. In recent years, synthetic images have become increasingly common in\npublic image sources due to rapid advances in generative models. Therefore, it\nis becoming inevitable to include existing synthetic images in the unlabeled\ndata for SSL. How this kind of contamination will affect SSL remains\nunexplored. In this paper, we introduce a new task, Real-Synthetic Hybrid SSL\n(RS-SSL), to investigate the impact of unlabeled data contaminated by synthetic\nimages for SSL. First, we set up a new RS-SSL benchmark to evaluate current SSL\nmethods and found they struggled to improve by unlabeled synthetic images,\nsometimes even negatively affected. To this end, we propose RSMatch, a novel\nSSL method specifically designed to handle the challenges of RS-SSL. RSMatch\neffectively identifies unlabeled synthetic data and further utilizes them for\nimprovement. Extensive experimental results show that RSMatch can transfer\nsynthetic unlabeled data from `obstacles' to `resources.' The effectiveness is\nfurther verified through ablation studies and visualization.",
      "doi": "arXiv:2405.16930v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL\n  Models",
      "authors": [
        "Ruchao Fan",
        "Natarajan Balaji Shanka",
        "Abeer Alwan"
      ],
      "abstract": "Non-autoregressive automatic speech recognition (NASR) models have gained\nattention due to their parallelism and fast inference. The encoder-based NASR,\ne.g. connectionist temporal classification (CTC), can be initialized from the\nspeech foundation models (SFM) but does not account for any dependencies among\nintermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based\nsingle-step non-autoregressive transformer (CASS-NAT), can mitigate the\ndependency problem but is not able to efficiently integrate SFM. Inspired by\nthe success of recent work of speech-text joint pre-training with a shared\ntransformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to\ncombine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an\nencoder as the major module, which can be the SFM. The encoder plays the role\nof both the CASS-NAT encoder and decoder by two forward passes. The first pass\nof the encoder accepts the speech signal as input, while the concatenation of\nthe speech signal and the token-level acoustic embedding is used as the input\nfor the second pass. Examined on the Librispeech 100h, MyST, and Aishell1\ndatasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results\nand is better or comparable to CASS-NAT with only an encoder and hence, fewer\nmodel parameters. Our codes are publicly available.",
      "doi": "https://doi.org/10.1109/LSP.2024.3365036",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Boosting Self-Supervised Embeddings for Speech Enhancement",
      "authors": [
        "Kuo-Hsuan Hung",
        "Szu-wei Fu",
        "Huan-Hsin Tseng",
        "Hsin-Tien Chiang",
        "Yu Tsao",
        "Chii-Wann Lin"
      ],
      "abstract": "Self-supervised learning (SSL) representation for speech has achieved\nstate-of-the-art (SOTA) performance on several downstream tasks. However, there\nremains room for improvement in speech enhancement (SE) tasks. In this study,\nwe used a cross-domain feature to solve the problem that SSL embeddings may\nlack fine-grained information to regenerate speech signals. By integrating the\nSSL representation and spectrogram, the result can be significantly boosted. We\nfurther study the relationship between the noise robustness of SSL\nrepresentation via clean-noisy distance (CN distance) and the layer importance\nfor SE. Consequently, we found that SSL representations with lower noise\nrobustness are more important. Furthermore, our experiments on the VCTK-DEMAND\ndataset demonstrated that fine-tuning an SSL representation with an SE model\ncan outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without\ninvoking complicated network architectures. In later experiments, the CN\ndistance in SSL embeddings was observed to increase after fine-tuning. These\nresults verify our expectations and may help design SE-related SSL training in\nthe future.",
      "doi": "arXiv:2204.03339v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?",
      "authors": [
        "Qian Ma",
        "Haitao Mao",
        "Jingzhe Liu",
        "Zhehua Zhang",
        "Chunlin Feng",
        "Yu Song",
        "Yihan Shao",
        "Yao Ma"
      ],
      "abstract": "Self-supervised learning~(SSL) is essential to obtain foundation models in\nNLP and CV domains via effectively leveraging knowledge in large-scale\nunlabeled data. The reason for its success is that a suitable SSL design can\nhelp the model to follow the neural scaling law, i.e., the performance\nconsistently improves with increasing model and dataset sizes. However, it\nremains a mystery whether existing SSL in the graph domain can follow the\nscaling behavior toward building Graph Foundation Models~(GFMs) with\nlarge-scale pre-training. In this study, we examine whether existing graph SSL\ntechniques can follow the neural scaling behavior with the potential to serve\nas the essential component for GFMs. Our benchmark includes comprehensive SSL\ntechnique implementations with analysis conducted on both the conventional SSL\nsetting and many new settings adopted in other domains. Surprisingly, despite\nthe SSL loss continuously decreasing, no existing graph SSL techniques follow\nthe neural scaling behavior on the downstream performance. The model\nperformance only merely fluctuates on different data scales and model scales.\nInstead of the scales, the key factors influencing the performance are the\nchoices of model architecture and pretext task design. This paper examines\nexisting SSL techniques for the feasibility of Graph SSL techniques in\ndeveloping GFMs and opens a new direction for graph SSL design with the new\nevaluation prototype. Our code implementation is available online to ease\nreproducibility on https://github.com/GraphSSLScaling/GraphSSLScaling.",
      "doi": "arXiv:2408.11243v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Determinants of Inflation in Ethiopia: Autoregressive Distributed Lag Model",
      "authors": [],
      "abstract": "<jats:p>Price stability is one of the major goals of monetary policy and the key indicators of macroeconomic stability. Pursuing of price stability is primary to long-run growth and development; it should be the concern of every economy. This study examine the factors in determining inflation in Ethiopia, using the autoregressive distributed lag (ARDL) model by employing the data series for the period ranging from 1974 to 2017. The ADF Unit root test confirmed that the variables in the model are integrated of order 0 I (0) and order one I (1)). The Bound F-test of co integration has confirmed the existence of long run relationships among the variables entered in the inflation model. ARDL regression result suggest that real gross domestic product, real effective exchange rate and budget deficit variables are important in determining inflation in the short run. In the long run inflation is determined by real effective exchange rate, budget deficit, gross national saving and lending interest rate. The error correction term coefficient is negative and significant at 1 percent level of significance suggesting that inflation adjusts to deviations from its long -term equilibrium. The estimated model passes diagnostic tests and the graphical evidence (CUSUM and CUSUMQ graphs) indicate that the model is stable during the sample period. Finally, investments in food and agricultural sectors could considerably support the process of ensuring price stability. Moreover, a credible and sustained fiscal adjustment, aiming to boost revenue generation can reduce fiscal deficit.</jats:p>",
      "doi": "https://doi.org/10.33140/jerr.03.02.07",
      "year": 2023,
      "source": "Crossref"
    },
    {
      "title": "Self-supervised TransUNet for Ultrasound regional segmentation of the\n  distal radius in children",
      "authors": [
        "Yuyue Zhou",
        "Jessica Knight",
        "Banafshe Felfeliyan",
        "Christopher Keen",
        "Abhilash Rakkunedeth Hareendranathan",
        "Jacob L. Jaremko"
      ],
      "abstract": "Supervised deep learning offers great promise to automate analysis of medical\nimages from segmentation to diagnosis. However, their performance highly relies\non the quality and quantity of the data annotation. Meanwhile, curating large\nannotated datasets for medical images requires a high level of expertise, which\nis time-consuming and expensive. Recently, to quench the thirst for large data\nsets with high-quality annotation, self-supervised learning (SSL) methods using\nunlabeled domain-specific data, have attracted attention. Therefore, designing\nan SSL method that relies on minimal quantities of labeled data has\nfar-reaching significance in medical images. This paper investigates the\nfeasibility of deploying the Masked Autoencoder for SSL (SSL-MAE) of TransUNet,\nfor segmenting bony regions from children's wrist ultrasound scans. We found\nthat changing the embedding and loss function in SSL-MAE can produce better\ndownstream results compared to the original SSL-MAE. In addition, we determined\nthat only pretraining TransUNet embedding and encoder with SSL-MAE does not\nwork as well as TransUNet without SSL-MAE pretraining on downstream\nsegmentation tasks.",
      "doi": "arXiv:2309.09490v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Deep Closest Point: Learning Representations for Point Cloud\n  Registration",
      "authors": [
        "Yue Wang",
        "Justin M. Solomon"
      ],
      "abstract": "Point cloud registration is a key problem for computer vision applied to\nrobotics, medical imaging, and other applications. This problem involves\nfinding a rigid transformation from one point cloud into another so that they\nalign. Iterative Closest Point (ICP) and its variants provide simple and\neasily-implemented iterative methods for this task, but these algorithms can\nconverge to spurious local optima. To address local optima and other\ndifficulties in the ICP pipeline, we propose a learning-based method, titled\nDeep Closest Point (DCP), inspired by recent techniques in computer vision and\nnatural language processing. Our model consists of three parts: a point cloud\nembedding network, an attention-based module combined with a pointer generation\nlayer, to approximate combinatorial matching, and a differentiable singular\nvalue decomposition (SVD) layer to extract the final rigid transformation. We\ntrain our model end-to-end on the ModelNet40 dataset and show in several\nsettings that it performs better than ICP, its variants (e.g., Go-ICP, FGR),\nand the recently-proposed learning-based method PointNetLK. Beyond providing a\nstate-of-the-art registration technique, we evaluate the suitability of our\nlearned features transferred to unseen objects. We also provide preliminary\nanalysis of our learned model to help understand whether domain-specific and/or\nglobal features facilitate rigid registration.",
      "doi": "arXiv:1905.03304v1",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Visual-Inertial Odometry-enhanced Geometrically Stable ICP for Mapping\n  Applications using Aerial Robots",
      "authors": [
        "Tung Dang",
        "Shehryar Khattak",
        "Christos Papachristos",
        "Kostas Alexis"
      ],
      "abstract": "This paper presents a visual-inertial odometry-enhanced geometrically stable\nIterative Closest Point (ICP) algorithm for accurate mapping using aerial\nrobots. The proposed method employs a visual-inertial odometry framework in\norder to provide robust priors to the ICP step and calculate the overlap among\npoint clouds derived from an onboard time-of-flight depth sensor. Within the\noverlapping parts of the point clouds, the method samples points such that the\ndistribution of normals among them is as large as possible. As different\ngeometries and sensor trajectories will influence the performance of the\nalignment process, evaluation of the expected geometric stability of the ICP\nstep is conducted. It is only when this test is successful that the matching,\noutlier rejection, and minimization of the error metric ICP steps are conducted\nand the new relative translation and rotational components are estimated,\notherwise the system relies on the visual-inertial odometry transformation\nestimates. The proposed strategy was evaluated within handheld, automated and\nfully autonomous exploration and mapping missions using a small aerial robot\nand was shown to provide robust results of superior quality at an affordable\nincrease of the computational load.",
      "doi": "arXiv:1801.08228v2",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "DICP: Doppler Iterative Closest Point Algorithm",
      "authors": [
        "Bruno Hexsel",
        "Heethesh Vhavle",
        "Yi Chen"
      ],
      "abstract": "In this paper, we present a novel algorithm for point cloud registration for\nrange sensors capable of measuring per-return instantaneous radial velocity:\nDoppler ICP. Existing variants of ICP that solely rely on geometry or other\nfeatures generally fail to estimate the motion of the sensor correctly in\nscenarios that have non-distinctive features and/or repetitive geometric\nstructures such as hallways, tunnels, highways, and bridges. We propose a new\nDoppler velocity objective function that exploits the compatibility of each\npoint's Doppler measurement and the sensor's current motion estimate. We\njointly optimize the Doppler velocity objective function and the geometric\nobjective function which sufficiently constrains the point cloud alignment\nproblem even in feature-denied environments. Furthermore, the correspondence\nmatches used for the alignment are improved by pruning away the points from\ndynamic targets which generally degrade the ICP solution. We evaluate our\nmethod on data collected from real sensors and from simulation. Our results\nshow that with the added Doppler velocity residual terms, our method achieves a\nsignificant improvement in registration accuracy along with faster convergence,\non average, when compared to classical point-to-plane ICP that solely relies on\ngeometric residuals.",
      "doi": "arXiv:2201.11944v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Stein ICP for Uncertainty Estimation in Point Cloud Matching",
      "authors": [
        "Fahira Afzal Maken",
        "Fabio Ramos",
        "Lionel Ott"
      ],
      "abstract": "Quantification of uncertainty in point cloud matching is critical in many\ntasks such as pose estimation, sensor fusion, and grasping. Iterative closest\npoint (ICP) is a commonly used pose estimation algorithm which provides a point\nestimate of the transformation between two point clouds. There are many sources\nof uncertainty in this process that may arise due to sensor noise, ambiguous\nenvironment, and occlusion. However, for safety critical problems such as\nautonomous driving, a point estimate of the pose transformation is not\nsufficient as it does not provide information about the multiple solutions.\nCurrent probabilistic ICP methods usually do not capture all sources of\nuncertainty and may provide unreliable transformation estimates which can have\na detrimental effect in state estimation or decision making tasks that use this\ninformation. In this work we propose a new algorithm to align two point clouds\nthat can precisely estimate the uncertainty of ICP's transformation parameters.\nWe develop a Stein variational inference framework with gradient based\noptimization of ICP's cost function. The method provides a non-parametric\nestimate of the transformation, can model complex multi-modal distributions,\nand can be effectively parallelized on a GPU. Experiments using 3D kinect data\nas well as sparse indoor/outdoor LiDAR data show that our method is capable of\nefficiently producing accurate pose uncertainty estimates.",
      "doi": "https://doi.org/10.1109/LRA.2021.3137503n",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Deep Global Features for Point Cloud Alignment.",
      "authors": [
        "Khazari AE",
        "Que Y",
        "Sung TL",
        "Lee HJ"
      ],
      "abstract": "Point cloud registration is a key problem in computer vision applications and involves finding a rigid transform from a point cloud into another such that they align together. The iterative closest point (ICP) method is a simple and effective solution that converges to a local optimum. However, despite the fact that point cloud registration or alignment is addressed in learning-based methods, such as PointNetLK, they do not offer good generalizability for point clouds. In this stud, we proposed a learning-based approach that addressed existing problems, such as finding local optima for ICP and achieving minimum generalizability. The proposed model consisted of three main parts: an encoding network, an auxiliary module that weighed the contribution of each input point cloud, and feature alignment to achieve the final transform. The proposed architecture offered greater generalization among the categories. Experiments were performed on ModelNet40 with different configurations and the results indicated that the proposed approach significantly outperformed the state-of-the-art point cloud alignment methods.",
      "doi": "https://doi.org/10.3390/s20144032",
      "year": 2020,
      "source": "PubMed"
    },
    {
      "title": "Point cloud completion in challenging indoor scenarios with human motion.",
      "authors": [
        "Zhang C",
        "Czarnuch S"
      ],
      "abstract": "Combining and completing point cloud data from two or more sensors with arbitrarily relative perspectives in a dynamic, cluttered, and complex environment is challenging, especially when the two sensors have significant perspective differences while the large overlap ratio and feature-rich scene cannot be guaranteed. We create a novel approach targeting this challenging scenario by registering two camera captures in a time series with unknown perspectives and human movements to easily use our system in a real-life scene. In our approach, we first reduce the six unknowns of 3D point cloud completion to three by aligning the ground planes found by our previous perspective-independent 3D ground plane estimation algorithm. Subsequently, we use a histogram-based approach to identify and extract all the humans from each frame generating a three-dimensional (3D) human walking sequence in a time series. To enhance accuracy and performance, we convert 3D human walking sequences to lines by calculating the center of mass (CoM) point of each human body and connecting them. Finally, we match the walking paths in different data trials by minimizing the Fréchet distance between two walking paths and using 2D iterative closest point (ICP) to find the remaining three unknowns in the overall transformation matrix for the final alignment. Using this approach, we can successfully register the corresponding walking path of the human between the two cameras' captures and estimate the transformation matrix between the two sensors.",
      "doi": "https://doi.org/10.3389/frobt.2023.1184614",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Research on 3D point cloud alignment algorithm based on SHOT features.",
      "authors": [
        "Fu Z",
        "Zhang E",
        "Sun R",
        "Zang J",
        "Zhang W"
      ],
      "abstract": "To overcome the problem of the high initial position of the point cloud required by the traditional Iterative Closest Point (ICP) algorithm, in this paper, we propose a point cloud registration method based on normal vector and directional histogram features (SHOT). Firstly, a hybrid filtering method based on the voxel idea is proposed and verified using the measured point cloud data, and the noise removal rates of 97.5%, 97.8%, and 93.8% are obtained. Secondly, in terms of feature point extraction, the original algorithm is optimized, and the optimized algorithm can better extract the missing part of the point cloud. Finally, a fine alignment method based on normal vector and directional histogram features (SHOT) is proposed, and the improved algorithm is compared with the existing algorithm. Taking the Stanford University point cloud data and the self-measured point cloud data as examples, the plotted iteration-error plots can be concluded that the improved method can reduce the number of iterations by 40.23% and 37.62%, respectively.",
      "doi": "https://doi.org/10.1371/journal.pone.0296704",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Automated Point Cloud Registration Approach Optimized for a Stop-and-Go Scanning System.",
      "authors": [
        "Park S",
        "Ju S",
        "Nguyen MH",
        "Yoon S",
        "Heo J"
      ],
      "abstract": "The latest advances in mobile platforms, such as robots, have enabled the automatic acquisition of full coverage point cloud data from large areas with terrestrial laser scanning. Despite this progress, the crucial post-processing step of registration, which aligns raw point cloud data from separate local coordinate systems into a unified coordinate system, still relies on manual intervention. To address this practical issue, this study presents an automated point cloud registration approach optimized for a stop-and-go scanning system based on a quadruped walking robot. The proposed approach comprises three main phases: perpendicular constrained wall-plane extraction; coarse registration with plane matching using point-to-point displacement calculation; and fine registration with horizontality constrained iterative closest point (ICP). Experimental results indicate that the proposed method successfully achieved automated registration with an accuracy of 0.044 m and a successful scan rate (SSR) of 100% within a time frame of 424.2 s with 18 sets of scan data acquired from the stop-and-go scanning system in a real-world indoor environment. Furthermore, it surpasses conventional approaches, ensuring reliable registration for point cloud pairs with low overlap in specific indoor environmental conditions.",
      "doi": "https://doi.org/10.3390/s24010138",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "GenReg: Deep Generative Method for Fast Point Cloud Registration",
      "authors": [
        "Xiaoshui Huang",
        "Zongyi Xu",
        "Guofeng Mei",
        "Sheng Li",
        "Jian Zhang",
        "Yifan Zuo",
        "Yucheng Wang"
      ],
      "abstract": "Accurate and efficient point cloud registration is a challenge because the\nnoise and a large number of points impact the correspondence search. This\nchallenge is still a remaining research problem since most of the existing\nmethods rely on correspondence search. To solve this challenge, we propose a\nnew data-driven registration algorithm by investigating deep generative neural\nnetworks to point cloud registration. Given two point clouds, the motivation is\nto generate the aligned point clouds directly, which is very useful in many\napplications like 3D matching and search. We design an end-to-end generative\nneural network for aligned point clouds generation to achieve this motivation,\ncontaining three novel components. Firstly, a point multi-perception layer\n(MLP) mixer (PointMixer) network is proposed to efficiently maintain both the\nglobal and local structure information at multiple levels from the self point\nclouds. Secondly, a feature interaction module is proposed to fuse information\nfrom cross point clouds. Thirdly, a parallel and differential sample consensus\nmethod is proposed to calculate the transformation matrix of the input point\nclouds based on the generated registration results. The proposed generative\nneural network is trained in a GAN framework by maintaining the data\ndistribution and structure similarity. The experiments on both ModelNet40 and\n7Scene datasets demonstrate that the proposed algorithm achieves\nstate-of-the-art accuracy and efficiency. Notably, our method reduces $2\\times$\nin registration error (CD) and $12\\times$ running time compared to the\nstate-of-the-art correspondence-based algorithm.",
      "doi": "arXiv:2111.11783v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Category-Level Global Camera Pose Estimation with Multi-Hypothesis Point\n  Cloud Correspondences",
      "authors": [
        "Jun-Jee Chao",
        "Selim Engin",
        "Nicolai Häni",
        "Volkan Isler"
      ],
      "abstract": "Correspondence search is an essential step in rigid point cloud registration\nalgorithms. Most methods maintain a single correspondence at each step and\ngradually remove wrong correspondances. However, building one-to-one\ncorrespondence with hard assignments is extremely difficult, especially when\nmatching two point clouds with many locally similar features. This paper\nproposes an optimization method that retains all possible correspondences for\neach keypoint when matching a partial point cloud to a complete point cloud.\nThese uncertain correspondences are then gradually updated with the estimated\nrigid transformation by considering the matching cost. Moreover, we propose a\nnew point feature descriptor that measures the similarity between local point\ncloud regions. Extensive experiments show that our method outperforms the\nstate-of-the-art (SoTA) methods even when matching different objects within the\nsame category. Notably, our method outperforms the SoTA methods when\nregistering real-world noisy depth images to a template shape by up to 20%\nperformance.",
      "doi": "arXiv:2209.14419v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Dynamic Point Cloud Denoising via Gradient Fields",
      "authors": [
        "Qianjiang Hu",
        "Wei Hu"
      ],
      "abstract": "3D dynamic point clouds provide a discrete representation of real-world\nobjects or scenes in motion, which have been widely applied in immersive\ntelepresence, autonomous driving, surveillance, etc. However, point clouds\nacquired from sensors are usually perturbed by noise, which affects downstream\ntasks such as surface reconstruction and analysis. Although many efforts have\nbeen made for static point cloud denoising, dynamic point cloud denoising\nremains under-explored. In this paper, we propose a novel gradient-field-based\ndynamic point cloud denoising method, exploiting the temporal correspondence\nvia the estimation of gradient fields -- a fundamental problem in dynamic point\ncloud processing and analysis. The gradient field is the gradient of the\nlog-probability function of the noisy point cloud, based on which we perform\ngradient ascent so as to converge each point to the underlying clean surface.\nWe estimate the gradient of each surface patch and exploit the temporal\ncorrespondence, where the temporally corresponding patches are searched\nleveraging on rigid motion in classical mechanics. In particular, we treat each\npatch as a rigid object, which moves in the gradient field of an adjacent frame\nvia force until reaching a balanced state, i.e., when the sum of gradients over\nthe patch reaches 0. Since the gradient would be smaller when the point is\ncloser to the underlying surface, the balanced patch would fit the underlying\nsurface well, thus leading to the temporal correspondence. Finally, the\nposition of each point in the patch is updated along the direction of the\ngradient averaged from corresponding patches in adjacent frames. Experimental\nresults demonstrate that the proposed model outperforms state-of-the-art\nmethods under both synthetic noise and simulated real-world noise.",
      "doi": "arXiv:2204.08755v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "RBFIM: Perceptual Quality Assessment for Compressed Point Clouds Using\n  Radial Basis Function Interpolation",
      "authors": [
        "Zhang Chen",
        "Shuai Wan",
        "Siyu Ren",
        "Fuzheng Yang",
        "Mengting Yu",
        "Junhui Hou"
      ],
      "abstract": "One of the main challenges in point cloud compression (PCC) is how to\nevaluate the perceived distortion so that the codec can be optimized for\nperceptual quality. Current standard practices in PCC highlight a primary\nissue: while single-feature metrics are widely used to assess compression\ndistortion, the classic method of searching point-to-point nearest neighbors\nfrequently fails to adequately build precise correspondences between point\nclouds, resulting in an ineffective capture of human perceptual features. To\novercome the related limitations, we propose a novel assessment method called\nRBFIM, utilizing radial basis function (RBF) interpolation to convert discrete\npoint features into a continuous feature function for the distorted point\ncloud. By substituting the geometry coordinates of the original point cloud\ninto the feature function, we obtain the bijective sets of point features. This\nenables an establishment of precise corresponding features between distorted\nand original point clouds and significantly improves the accuracy of quality\nassessments. Moreover, this method avoids the complexity caused by\nbidirectional searches. Extensive experiments on multiple subjective quality\ndatasets of compressed point clouds demonstrate that our RBFIM excels in\naddressing human perception tasks, thereby providing robust support for PCC\noptimization efforts.",
      "doi": "arXiv:2503.14154v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "HTMC: hierarchical tolerance mask correspondence for human body point cloud registration.",
      "authors": [
        "Yu F",
        "Chen Z",
        "Liu L",
        "Ren L",
        "Jiang M"
      ],
      "abstract": "Point cloud registration can be solved by searching for correspondence pairs. Searching for correspondence pairs in human body point clouds poses some challenges, including: (1) the similar geometrical shapes of the human body are difficult to distinguish. (2) The symmetry of the human body confuses the correspondence pairs searching. To resolve the above issues, this article proposes a Hierarchical Tolerance Mask Correspondence (HTMC) method to achieve better alignment by tolerating obfuscation. First, we define various levels of correspondence pairs and assign different similarity scores for each level. Second, HTMC designs a tolerance loss function to tolerate the obfuscation of correspondence pairs. Third, HTMC uses a differentiable mask to diminish the influence of non-overlapping regions and enhance the influence of overlapping regions. In conclusion, HTMC acknowledges the presence of similar local geometry in human body point clouds. On one hand, it avoids overfitting caused by forcibly distinguishing similar geometries, and on the other hand, it prevents genuine correspondence relationships from being masked by similar geometries. The codes are available at https://github.com/ChenPointCloud/HTMC.",
      "doi": "https://doi.org/10.7717/peerj-cs.1724",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Efficient and Robust Point Cloud Registration via Heuristics-Guided Parameter Search.",
      "authors": [
        "Huang T",
        "Li H",
        "Peng L",
        "Liu Y",
        "Liu YH"
      ],
      "abstract": "Estimating the rigid transformation with 6 degrees of freedom based on a putative 3D correspondence set is a crucial procedure in point cloud registration. Existing correspondence identification methods usually lead to large outlier ratios (>95% is common), underscoring the significance of robust registration methods. Many researchers turn to parameter search-based strategies (e.g., Branch-and-Bround) for robust registration. Although related methods show high robustness, their efficiency is limited to the high-dimensional search space. This paper proposes a heuristics-guided parameter search strategy to accelerate the search while maintaining high robustness. We first sample some correspondences (i.e., heuristics) and then just need to sequentially search the feasible regions that make each sample an inlier. Our strategy largely reduces the search space and can guarantee accuracy with only a few inlier samples, therefore enjoying an excellent trade-off between efficiency and robustness. Since directly parameterizing the 6-dimensional nonlinear feasible region for efficient search is intractable, we construct a three-stage decomposition pipeline to reparameterize the feasible region, resulting in three lower-dimensional sub-problems that are easily solvable via our strategy. Besides reducing the searching dimension, our decomposition enables the leverage of 1-dimensional interval stabbing at all three stages for searching acceleration. Moreover, we propose a valid sampling strategy to guarantee our sampling effectiveness, and a compatibility verification setup to further accelerate our search. Extensive experiments on both simulated and real-world datasets demonstrate that our approach exhibits comparable robustness with state-of-the-art methods while achieving a significant efficiency boost.",
      "doi": "https://doi.org/10.1109/tpami.2024.3387553",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "GraphReg: Dynamical Point Cloud Registration With Geometry-Aware Graph Signal Processing.",
      "authors": [
        "Zhao M",
        "Ma L",
        "Jia X",
        "Yan DM",
        "Huang T"
      ],
      "abstract": "This study presents a high-accuracy, efficient, and physically induced method for 3D point cloud registration, which is the core of many important 3D vision problems. In contrast to existing physics-based methods that merely consider spatial point information and ignore surface geometry, we explore geometry aware rigid-body dynamics to regulate the particle (point) motion, which results in more precise and robust registration. Our proposed method consists of four major modules. First, we leverage the graph signal processing (GSP) framework to define a new signature, i.e., point response intensity for each point, by which we succeed in describing the local surface variation, resampling keypoints, and distinguishing different particles. Then, to address the shortcomings of current physics-based approaches that are sensitive to outliers, we accommodate the defined point response intensity to median absolute deviation (MAD) in robust statistics and adopt the X84 principle for adaptive outlier depression, ensuring a robust and stable registration. Subsequently, we propose a novel geometric invariant under rigid transformations to incorporate higher-order features of point clouds, which is further embedded for force modeling to guide the correspondence between pairwise scans credibly. Finally, we introduce an adaptive simulated annealing (ASA) method to search for the global optimum and substantially accelerate the registration process. We perform comprehensive experiments to evaluate the proposed method on various datasets captured from range scanners to LiDAR. Results demonstrate that our proposed method outperforms representative state-of-the-art approaches in terms of accuracy and is more suitable for registering large-scale point clouds. Furthermore, it is considerably faster and more robust than most competitors. Our implementation is publicly available at https://github.com/zikai1/GraphReg.",
      "doi": "https://doi.org/10.1109/tip.2022.3223793",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Constructing Diverse Inlier Consistency for Partial Point Cloud Registration.",
      "authors": [
        "Zhang YX",
        "Gui J",
        "Kwok JT"
      ],
      "abstract": "Partial point cloud registration aims to align partial scans into a shared coordinate system. While learning-based partial point cloud registration methods have achieved remarkable progress, they often fail to take full advantage of the relative positional relationships both within (intra-) and between (inter-) point clouds. This oversight hampers their ability to accurately identify overlapping regions and search for reliable correspondences. To address these limitations, a diverse inlier consistency (DIC) method has been proposed that adaptively embeds the positional information of a reliable correspondence in the intra- and inter-point cloud. Firstly, a diverse inlier consistency-driven region perception (DICdRP) module is devised, which encodes the positional information of the selected correspondence within the intra-point cloud. This module enhances the sensitivity of all points to overlapping regions by recognizing the position of the selected correspondence. Secondly, a diverse inlier consistency-aware correspondence search (DICaCS) module is developed, which leverages relative positions in the inter-point cloud. This module studies an inter-point cloud DIC weight to supervise correspondence compatibility, allowing for precise identification of correspondences and effective outlier filtration. Thirdly, diverse information is integrated throughout our framework to achieve a more holistic and detailed registration process. Extensive experiments on object-level and scene-level datasets demonstrate the superior performance of the proposed algorithm. The code is available at https://github.com/yxzhang15/DIC.",
      "doi": "https://doi.org/10.1109/tip.2024.3492700",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die\n  Castings",
      "authors": [
        "Yu Du",
        "Yu Song",
        "Ce Guo",
        "Xiaojing Tian",
        "Dong Liu",
        "Ming Cong"
      ],
      "abstract": "Due to their complex spatial structure and diverse geometric features,\nachieving high-precision and robust point cloud registration for complex Die\nCastings has been a significant challenge in the die-casting industry. Existing\npoint cloud registration methods primarily optimize network models using\nwell-established high-quality datasets, often neglecting practical application\nin real scenarios. To address this gap, this paper proposes a high-precision\nadaptive registration method called Multiscale Efficient Deep Closest Point\n(MEDPNet) and introduces a die-casting point cloud dataset, DieCastCloud,\nspecifically designed to tackle the challenges of point cloud registration in\nthe die-casting industry. The MEDPNet method performs coarse die-casting point\ncloud data registration using the Efficient-DCP method, followed by precision\nregistration using the Multiscale feature fusion dual-channel registration\n(MDR) method. We enhance the modeling capability and computational efficiency\nof the model by replacing the attention mechanism of the Transformer in DCP\nwith Efficient Attention and implementing a collaborative scale mechanism\nthrough the combination of serial and parallel blocks. Additionally, we propose\nthe MDR method, which utilizes multilayer perceptrons (MLP), Normal\nDistributions Transform (NDT), and Iterative Closest Point (ICP) to achieve\nlearnable adaptive fusion, enabling high-precision, scalable, and\nnoise-resistant global point cloud registration. Our proposed method\ndemonstrates excellent performance compared to state-of-the-art geometric and\nlearning-based registration methods when applied to complex die-casting point\ncloud data.",
      "doi": "arXiv:2403.09996v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "DIPR: Efficient Point Cloud Registration via Dynamic Iteration",
      "authors": [
        "Yang Ai",
        "Qiang Bai",
        "Jindong Li",
        "Xi Yang"
      ],
      "abstract": "Point cloud registration (PCR) is an essential task in 3D vision. Existing\nmethods achieve increasingly higher accuracy. However, a large proportion of\nnon-overlapping points in point cloud registration consume a lot of\ncomputational resources while negatively affecting registration accuracy. To\novercome this challenge, we introduce a novel Efficient Point Cloud\nRegistration via Dynamic Iteration framework, DIPR, that makes the neural\nnetwork interactively focus on overlapping points based on sparser input\npoints. We design global and local registration stages to achieve efficient\ncourse-tofine processing. Beyond basic matching modules, we propose the Refined\nNodes to narrow down the scope of overlapping points by using adopted\ndensity-based clustering to significantly reduce the computation amount. And\nour SC Classifier serves as an early-exit mechanism to terminate the\nregistration process in time according to matching accuracy. Extensive\nexperiments on multiple datasets show that our proposed approach achieves\nsuperior registration accuracy while significantly reducing computational time\nand GPU memory consumption compared to state-of-the-art methods.",
      "doi": "arXiv:2312.02877v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "A Dynamical Perspective on Point Cloud Registration",
      "authors": [
        "Heng Yang"
      ],
      "abstract": "We provide a dynamical perspective on the classical problem of 3D point cloud\nregistration with correspondences. A point cloud is considered as a rigid body\nconsisting of particles. The problem of registering two point clouds is\nformulated as a dynamical system, where the dynamic model point cloud\ntranslates and rotates in a viscous environment towards the static scene point\ncloud, under forces and torques induced by virtual springs placed between each\npair of corresponding points. We first show that the potential energy of the\nsystem recovers the objective function of the maximum likelihood estimation. We\nthen adopt Lyapunov analysis, particularly the invariant set theorem, to\nanalyze the rigid body dynamics and show that the system globally\nasymptotically tends towards the set of equilibrium points, where the globally\noptimal registration solution lies in. We conjecture that, besides the globally\noptimal equilibrium point, the system has either three or infinite \"spurious\"\nequilibrium points, and these spurious equilibria are all locally unstable. The\ncase of three spurious equilibria corresponds to generic shape of the point\ncloud, while the case of infinite spurious equilibria happens when the point\ncloud exhibits symmetry. Therefore, simulating the dynamics with random\nperturbations guarantees to obtain the globally optimal registration solution.\nNumerical experiments support our analysis and conjecture.",
      "doi": "arXiv:2005.03190v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Robust-DefReg: A Robust Deformable Point Cloud Registration Method based\n  on Graph Convolutional Neural Networks",
      "authors": [
        "Sara Monji-Azad",
        "Marvin Kinz",
        "Jürgen Hesser"
      ],
      "abstract": "Point cloud registration is a fundamental problem in computer vision that\naims to estimate the transformation between corresponding sets of points.\nNon-rigid registration, in particular, involves addressing challenges including\nvarious levels of deformation, noise, outliers, and data incompleteness. This\npaper introduces Robust-DefReg, a robust non-rigid point cloud registration\nmethod based on graph convolutional networks (GCNNs). Robust-DefReg is a\ncoarse-to-fine registration approach within an end-to-end pipeline, leveraging\nthe advantages of both coarse and fine methods. The method learns global\nfeatures to find correspondences between source and target point clouds, to\nenable appropriate initial alignment, and subsequently fine registration. The\nsimultaneous achievement of high accuracy and robustness across all challenges\nis reported less frequently in existing studies, making it a key objective of\nthe Robust-DefReg method. The proposed method achieves high accuracy in large\ndeformations while maintaining computational efficiency. This method possesses\nthree primary attributes: high accuracy, robustness to different challenges,\nand computational efficiency. The experimental results show that the proposed\nRobust-DefReg holds significant potential as a foundational architecture for\nfuture investigations in non-rigid point cloud registration. The source code of\nRobust-DefReg is available.",
      "doi": "https://doi.org/10.1088/1361-6501/ad916c",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Accurate Global Point Cloud Registration Using GPU-Based Parallel Angular Radon Spectrum.",
      "authors": [
        "Fontana E",
        "Lodi Rizzini D"
      ],
      "abstract": "Accurate robot localization and mapping can be improved through the adoption of globally optimal registration methods, like the Angular Radon Spectrum (ARS). In this paper, we present Cud-ARS, an efficient variant of the ARS algorithm for 2D registration designed for parallel execution of the most computationally expensive steps on Nvidia™ Graphics Processing Units (GPUs). Cud-ARS is able to compute the ARS in parallel blocks, with each associated to a subset of input points. We also propose a global branch-and-bound method for translation estimation. This novel parallel algorithm has been tested on multiple datasets. The proposed method is able to speed up the execution time by two orders of magnitude while obtaining more accurate results in rotation estimation than state-of-the-art correspondence-based algorithms. Our experiments also assess the potential of this novel approach in mapping applications, showing the contribution of GPU programming to efficient solutions of robotic tasks.",
      "doi": "https://doi.org/10.3390/s23208628",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Multi-View Laser Point Cloud Global Registration for a Single Object.",
      "authors": [
        "Wang S",
        "Sun HY",
        "Guo HC",
        "Du L",
        "Liu TJ"
      ],
      "abstract": "Global registration is an important step in the three-dimensional reconstruction of multi-view laser point clouds for moving objects, but the severe noise, density variation, and overlap ratio between multi-view laser point clouds present significant challenges to global registration. In this paper, a multi-view laser point cloud global registration method based on low-rank sparse decomposition is proposed. Firstly, the spatial distribution features of point clouds were extracted by spatial rasterization to realize loop-closure detection, and the corresponding weight matrix was established according to the similarities of spatial distribution features. The accuracy of adjacent registration transformation was evaluated, and the robustness of low-rank sparse matrix decomposition was enhanced. Then, the objective function that satisfies the global optimization condition was constructed, which prevented the solution space compression generated by the column-orthogonal hypothesis of the matrix. The objective function was solved by the Augmented Lagrange method, and the iterative termination condition was designed according to the prior conditions of single-object global registration. The simulation analysis shows that the proposed method was robust with a wide range of parameters, and the accuracy of loop-closure detection was over 90%. When the pairwise registration error was below 0.1 rad, the proposed method performed better than the three compared methods, and the global registration accuracy was better than 0.05 rad. Finally, the global registration results of real point cloud experiments further proved the validity and stability of the proposed method.",
      "doi": "https://doi.org/10.3390/s18113729",
      "year": 2018,
      "source": "PubMed"
    },
    {
      "title": "3D Global Localization in the Underground Mine Environment Using Mobile LiDAR Mapping and Point Cloud Registration.",
      "authors": [
        "Baek J",
        "Park J",
        "Cho S",
        "Lee C"
      ],
      "abstract": "This study proposes a 3D global localization method that implements mobile LiDAR mapping and point cloud registration to recognize the locations of objects in an underground mine. An initial global point cloud map was built for an entire underground mine area using mobile LiDAR; a local LiDAR scan (local point cloud) was generated at the point where underground positioning was required. We calculated fast point feature histogram (FPFH) descriptors for the global and local point clouds to extract point features. The match areas between the global and the local point clouds were searched and aligned using random sample consensus (RANSAC) and iterative closest point (ICP) registration. The object's location on the global coordinate system was measured using the LiDAR sensor trajectory. Field experiments were performed at the Gwan-in underground mine using three mobile LiDAR systems. The local point cloud dataset formed for the six areas of the underground mine precisely matched the global point cloud, with a low average error of approximately 0.13 m, regardless of the type of mobile LiDAR system used. In addition, the LiDAR senor trajectory was aligned on the global coordinate system to confirm the change in the dynamic object's position over time.",
      "doi": "https://doi.org/10.3390/s22082873",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Speeding Up Iterative Closest Point Using Stochastic Gradient Descent",
      "authors": [
        "Fahira Afzal Maken",
        "Fabio Ramos",
        "Lionel Ott"
      ],
      "abstract": "Sensors producing 3D point clouds such as 3D laser scanners and RGB-D cameras\nare widely used in robotics, be it for autonomous driving or manipulation.\nAligning point clouds produced by these sensors is a vital component in such\napplications to perform tasks such as model registration, pose estimation, and\nSLAM. Iterative closest point (ICP) is the most widely used method for this\ntask, due to its simplicity and efficiency. In this paper we propose a novel\nmethod which solves the optimisation problem posed by ICP using stochastic\ngradient descent (SGD). Using SGD allows us to improve the convergence speed of\nICP without sacrificing solution quality. Experiments using Kinect as well as\nVelodyne data show that, our proposed method is faster than existing methods,\nwhile obtaining solutions comparable to standard ICP. An additional benefit is\nrobustness to parameters when processing data from different sensors.",
      "doi": "arXiv:1907.09133v1",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Joint Pose and Principal Curvature Refinement Using Quadrics",
      "authors": [
        "Andrew Spek",
        "Tom Drummond"
      ],
      "abstract": "In this paper we present a novel joint approach for optimising surface\ncurvature and pose alignment. We present two implementations of this joint\noptimisation strategy, including a fast implementation that uses two frames and\nan offline multi-frame approach. We demonstrate an order of magnitude\nimprovement in simulation over state of the art dense relative point-to-plane\nIterative Closest Point (ICP) pose alignment using our dense joint\nframe-to-frame approach and show comparable pose drift to dense point-to-plane\nICP bundle adjustment using low-cost depth sensors. Additionally our improved\njoint quadric based approach can be used to more accurately estimate surface\ncurvature on noisy point clouds than previous approaches.",
      "doi": "arXiv:1707.00381v2",
      "year": 2017,
      "source": "arXiv"
    },
    {
      "title": "Registration of 3D Point Sets Using Exponential-based Similarity Matrix",
      "authors": [
        "Ashutosh Singandhupe",
        "Sanket Lokhande",
        "Hung Manh La"
      ],
      "abstract": "Point cloud registration is a fundamental problem in computer vision and\nrobotics, involving the alignment of 3D point sets captured from varying\nviewpoints using depth sensors such as LiDAR or structured light. In modern\nrobotic systems, especially those focused on mapping, it is essential to merge\nmultiple views of the same environment accurately. However, state-of-the-art\nregistration techniques often struggle when large rotational differences exist\nbetween point sets or when the data is significantly corrupted by sensor noise.\nThese challenges can lead to misalignments and, consequently, to inaccurate or\ndistorted 3D reconstructions. In this work, we address both these limitations\nby proposing a robust modification to the classic Iterative Closest Point (ICP)\nalgorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),\nintegrates a Gaussian-inspired exponential weighting scheme to construct a\nsimilarity matrix that dynamically adapts across iterations. This matrix\nfacilitates improved estimation of both rotational and translational components\nduring alignment. We demonstrate the robustness of ESM-ICP in two challenging\nscenarios: (i) large rotational discrepancies between the source and target\npoint clouds, and (ii) data corrupted by non-Gaussian noise. Our results show\nthat ESM-ICP outperforms traditional geometric registration techniques as well\nas several recent learning-based methods. To encourage reproducibility and\ncommunity engagement, our full implementation is made publicly available on\nGitHub. https://github.com/aralab-unr/ESM_ICP",
      "doi": "arXiv:2505.04540v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "LiPO: LiDAR Inertial Odometry for ICP Comparison",
      "authors": [
        "Darwin Mick",
        "Taylor Pool",
        "Madankumar Sathenahally Nagaraju",
        "Michael Kaess",
        "Howie Choset",
        "Matt Travers"
      ],
      "abstract": "We introduce a LiDAR inertial odometry (LIO) framework, called LiPO, that\nenables direct comparisons of different iterative closest point (ICP) point\ncloud registration methods. The two common ICP methods we compare are\npoint-to-point (P2P) and point-to-feature (P2F). In our experience, within the\ncontext of LIO, P2F-ICP results in less drift and improved mapping accuracy\nwhen robots move aggressively through challenging environments when compared to\nP2P-ICP. However, P2F-ICP methods require more hand-tuned hyper-parameters that\nmake P2F-ICP less general across all environments and motions. In real-world\nfield robotics applications where robots are used across different\nenvironments, more general P2P-ICP methods may be preferred despite increased\ndrift. In this paper, we seek to better quantify the trade-off between P2P-ICP\nand P2F-ICP to help inform when each method should be used. To explore this\ntrade-off, we use LiPO to directly compare ICP methods and test on relevant\nbenchmark datasets as well as on our custom unpiloted ground vehicle (UGV). We\nfind that overall, P2F-ICP has reduced drift and improved mapping accuracy,\nbut, P2P-ICP is more consistent across all environments and motions with\nminimal drift increase.",
      "doi": "arXiv:2410.08097v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "DeepMatch: Toward Lightweight in Point Cloud Registration.",
      "authors": [
        "Qi L",
        "Wu F",
        "Ge Z",
        "Sun Y"
      ],
      "abstract": "From source to target, point cloud registration solves for a rigid body transformation that aligns the two point clouds. IterativeClosest Point (ICP) and other traditional algorithms require a long registration time and are prone to fall into local optima. Learning-based algorithms such as Deep ClosestPoint (DCP) perform better than those traditional algorithms and escape from local optimality. However, they are still not perfectly robust and rely on the complex model design due to the extracted local features are susceptible to noise. In this study, we propose a lightweight point cloud registration algorithm, DeepMatch. DeepMatch extracts a point feature for each point, which is a spatial structure composed of each point itself, the center point of the point cloud, and the farthest point of each point. Because of the superiority of this per-point feature, the computing resources and time required by DeepMatch to complete the training are less than one-tenth of other learning-based algorithms with similar performance. In addition, experiments show that our algorithm achieves state-of-the-art (SOTA) performance on both clean, with Gaussian noise and unseen category datasets. Among them, on the unseen categories, compared to the previous best learning-based point cloud registration algorithms, the registration error of DeepMatch is reduced by two orders of magnitude, achieving the same performance as on the categories seen in training, which proves DeepMatch is generalizable in point cloud registration tasks. Finally, only our DeepMatch completes 100% recall on all three test sets.",
      "doi": "https://doi.org/10.3389/fnbot.2022.891158",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Colored Point Cloud Registration by Depth Filtering.",
      "authors": [
        "Choi O",
        "Hwang W"
      ],
      "abstract": "In the last stage of colored point cloud registration, depth measurement errors hinder the achievement of accurate and visually plausible alignments. Recently, an algorithm has been proposed to extend the Iterative Closest Point (ICP) algorithm to refine the measured depth values instead of the pose between point clouds. However, the algorithm suffers from numerical instability, so a postprocessing step is needed to restrict erroneous output depth values. In this paper, we present a new algorithm with improved numerical stability. Unlike the previous algorithm heavily relying on point-to-plane distances, our algorithm constructs a cost function based on an adaptive combination of two different projected distances to prevent numerical instability. We address the problem of registering a source point cloud to the union of the source and reference point clouds. This extension allows all source points to be processed in a unified filtering framework, irrespective of the existence of their corresponding points in the reference point cloud. The extension also improves the numerical stability of using the point-to-plane distances. The experiments show that the proposed algorithm improves the registration accuracy and provides high-quality alignments of colored point clouds.",
      "doi": "https://doi.org/10.3390/s21217023",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "RGB-D Point Cloud Registration Based on Salient Object Detection.",
      "authors": [
        "Wan T",
        "Du S",
        "Cui W",
        "Yao R",
        "Ge Y",
        "Li C",
        "Gao Y",
        "Zheng N"
      ],
      "abstract": "We propose a robust algorithm for aligning rigid, noisy, and partially overlapping red green blue-depth (RGB-D) point clouds. To address the problems of data degradation and uneven distribution, we offer three strategies to increase the robustness of the iterative closest point (ICP) algorithm. First, we introduce a salient object detection (SOD) method to extract a set of points with significant structural variation in the foreground, which can avoid the unbalanced proportion of foreground and background point sets leading to the local registration. Second, registration algorithms that rely only on structural information for alignment cannot establish the correct correspondences when faced with the point set with no significant change in structure. Therefore, a bidirectional color distance (BCD) is designed to build precise correspondence with bidirectional search and color guidance. Third, the maximum correntropy criterion (MCC) and trimmed strategy are introduced into our algorithm to handle with noise and outliers. We experimentally validate that our algorithm is more robust than previous algorithms on simulated and real-world scene data in most scenarios and achieve a satisfying 3-D reconstruction of indoor scenes.",
      "doi": "https://doi.org/10.1109/tnnls.2021.3053274",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "GCP-Based Automated Fine Alignment Method for Improving the Accuracy of Coordinate Information on UAV Point Cloud Data.",
      "authors": [
        "Choi Y",
        "Park S",
        "Kim S"
      ],
      "abstract": "3D point cloud data (PCD) can accurately and efficiently capture the 3D geometric information of a target and exhibits significant potential for construction applications. Although one of the most common approaches for generating PCD is the use of unmanned aerial vehicles (UAV), UAV photogrammetry-based point clouds are erroneous. This study proposes a novel framework for automatically improving the coordinate accuracy of PCD. Image-based deep learning and PCD analysis methods are integrated into a framework that includes the following four phases: GCP (Ground Control Point) detection, GCP global coordinate extraction, transformation matrix estimation, and fine alignment. Two different experiments, as follows, were performed in the case study to validate the proposed framework: (1) experiments on the fine alignment performance of the developed framework, and (2) performance and run time comparison between the fine alignment framework and common registration algorithms such as ICP (Iterative Closest Points). The framework achieved millimeter-level accuracy for each axis. The run time was less than 30 s, which indicated the feasibility of the proposed framework.",
      "doi": "https://doi.org/10.3390/s22228735",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "3D Dynamic Point Cloud Inpainting via Temporal Consistency on Graphs",
      "authors": [
        "Zeqing Fu",
        "Wei Hu",
        "Zongming Guo"
      ],
      "abstract": "With the development of 3D laser scanning techniques and depth sensors, 3D\ndynamic point clouds have attracted increasing attention as a representation of\n3D objects in motion, enabling various applications such as 3D immersive\ntele-presence, gaming and navigation. However, dynamic point clouds usually\nexhibit holes of missing data, mainly due to the fast motion, the limitation of\nacquisition and complicated structure. Leveraging on graph signal processing\ntools, we represent irregular point clouds on graphs and propose a novel\ninpainting method exploiting both intra-frame self-similarity and inter-frame\nconsistency in 3D dynamic point clouds. Specifically, for each missing region\nin every frame of the point cloud sequence, we search for its self-similar\nregions in the current frame and corresponding ones in adjacent frames as\nreferences. Then we formulate dynamic point cloud inpainting as an optimization\nproblem based on the two types of references, which is regularized by a\ngraph-signal smoothness prior. Experimental results show the proposed approach\noutperforms three competing methods significantly, both in objective and\nsubjective quality.",
      "doi": "arXiv:1904.10795v2",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "A Direct Semi-Exhaustive Search Method for Robust, Partial-to-Full Point\n  Cloud Registration",
      "authors": [
        "Richard Cheng",
        "Chavdar Papozov",
        "Dan Helmick",
        "Mark Tjersland"
      ],
      "abstract": "Point cloud registration refers to the problem of finding the rigid\ntransformation that aligns two given point clouds, and is crucial for many\napplications in robotics and computer vision. The main insight of this paper is\nthat we can directly optimize the point cloud registration problem without\ncorrespondences by utilizing an algorithmically simple, yet computationally\ncomplex, semi-exhaustive search approach that is very well-suited for\nparallelization on modern GPUs. Our proposed algorithm, Direct Semi-Exhaustive\nSearch (DSES), iterates over potential rotation matrices and efficiently\ncomputes the inlier-maximizing translation associated with each rotation. It\nthen computes the optimal rigid transformation based on any desired distance\nmetric by directly computing the error associated with each transformation\ncandidate $\\{R, t\\}$. By leveraging the parallelism of modern GPUs, DSES\noutperforms state-of-the-art methods for partial-to-full point cloud\nregistration on the simulated ModelNet40 benchmark and demonstrates high\nperformance and robustness for pose estimation on a real-world robotics problem\n(https://youtu.be/q0q2-s2KSuA).",
      "doi": "https://doi.org/10.1109/IROS58592.2024.10801518",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "ICOS: Efficient and Highly Robust Rotation Search and Point Cloud\n  Registration with Correspondences",
      "authors": [
        "Lei Sun"
      ],
      "abstract": "Rotation search and point cloud registration are two fundamental problems in\nrobotics and computer vision, which aim to estimate the rotation and the\ntransformation between the 3D vector sets and point clouds, respectively. Due\nto the presence of outliers, probably in very large numbers, among the putative\nvector or point correspondences in real-world applications, robust estimation\nis of great importance. In this paper, we present ICOS (Inlier searching using\nCOmpatible Structures), a novel, efficient and highly robust solver for both\nthe correspondence-based rotation search and point cloud registration problems.\nSpecifically, we (i) propose and construct a series of compatible structures\nfor the two problems where various invariants can be established, and (ii)\ndesign three time-efficient frameworks, the first for rotation search, the\nsecond for known-scale registration and the third for unknown-scale\nregistration, to filter out outliers and seek inliers from the\ninvariant-constrained random sampling based on the compatible structures\nproposed. In this manner, even with extreme outlier ratios, inliers can be\nsifted out and collected for solving the optimal rotation and transformation\neffectively, leading to our robust solver ICOS. Through plentiful experiments\nover standard datasets, we demonstrate that: (i) our solver ICOS is fast,\naccurate, robust against over 95% outliers with nearly 100% recall ratio of\ninliers for rotation search and both known-scale and unknown-scale\nregistration, outperforming other state-of-the-art methods, and (ii) ICOS is\npractical for use in multiple real-world applications.",
      "doi": "arXiv:2104.14763v2",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "CoFiNet: Reliable Coarse-to-fine Correspondences for Robust Point Cloud\n  Registration",
      "authors": [
        "Hao Yu",
        "Fu Li",
        "Mahdi Saleh",
        "Benjamin Busam",
        "Slobodan Ilic"
      ],
      "abstract": "We study the problem of extracting correspondences between a pair of point\nclouds for registration. For correspondence retrieval, existing works benefit\nfrom matching sparse keypoints detected from dense points but usually struggle\nto guarantee their repeatability. To address this issue, we present CoFiNet -\nCoarse-to-Fine Network which extracts hierarchical correspondences from coarse\nto fine without keypoint detection. On a coarse scale and guided by a weighting\nscheme, our model firstly learns to match down-sampled nodes whose vicinity\npoints share more overlap, which significantly shrinks the search space of a\nconsecutive stage. On a finer scale, node proposals are consecutively expanded\nto patches that consist of groups of points together with associated\ndescriptors. Point correspondences are then refined from the overlap areas of\ncorresponding patches, by a density-adaptive matching module capable to deal\nwith varying point density. Extensive evaluation of CoFiNet on both indoor and\noutdoor standard benchmarks shows our superiority over existing methods.\nEspecially on 3DLoMatch where point clouds share less overlap, CoFiNet\nsignificantly outperforms state-of-the-art approaches by at least 5% on\nRegistration Recall, with at most two-third of their parameters.",
      "doi": "arXiv:2110.14076v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Fast Descriptors and Correspondence Propagation for Robust Global Point Cloud Registration.",
      "authors": [
        "Lei H",
        "Jiang G",
        "Quan L"
      ],
      "abstract": "In this paper, we present a robust global approach for point cloud registration from uniformly sampled points. Based on eigenvalues and normals computed from multiple scales, we design fast descriptors to extract local structures of these points. The eigenvalue-based descriptor is effective at finding seed matches with low precision using nearest neighbor search. Generally, recovering the transformation from matches with low precision is rather challenging. Therefore, we introduce a mechanism named correspondence propagation to aggregate each seed match into a set of numerous matches. With these sets of matches, multiple transformations between point clouds are computed. A quality function formulated from distance errors is used to identify the best transformation and fulfill a coarse alignment of the point clouds. Finally, we refine the alignment result with the trimmed iterative closest point algorithm. The proposed approach can be applied to register point clouds with significant or limited overlaps and small or large transformations. More encouragingly, it is rather efficient and very robust to noise. A comparison to traditional descriptor-based methods and other global algorithms demonstrates the fine performance of the proposed approach. We also show its promising application in large-scale reconstruction with the scans of two real scenes. In addition, the proposed approach can be used to register low-resolution point clouds captured by Kinect as well.",
      "doi": "https://doi.org/10.1109/tip.2017.2700727",
      "year": 2017,
      "source": "PubMed"
    },
    {
      "title": "Efficient 3D Object Recognition from Cluttered Point Cloud.",
      "authors": [
        "Li W",
        "Cheng H",
        "Zhang X"
      ],
      "abstract": "Recognizing 3D objects and estimating their postures in a complex scene is a challenging task. Sample Consensus Initial Alignment (SAC-IA) is a commonly used point cloud-based method to achieve such a goal. However, its efficiency is low, and it cannot be applied in real-time applications. This paper analyzes the most time-consuming part of the SAC-IA algorithm: sample generation and evaluation. We propose two improvements to increase efficiency. In the initial aligning stage, instead of sampling the key points, the correspondence pairs between model and scene key points are generated in advance and chosen in each iteration, which reduces the redundant correspondence search operations; a geometric filter is proposed to prevent the invalid samples to the evaluation process, which is the most time-consuming operation because it requires transforming and calculating the distance between two point clouds. The introduction of the geometric filter can significantly increase the sample quality and reduce the required sample numbers. Experiments are performed on our own datasets captured by Kinect v2 Camera and on Bologna 1 dataset. The results show that the proposed method can significantly increase (10-30×) the efficiency of the original SAC-IA method without sacrificing accuracy.",
      "doi": "https://doi.org/10.3390/s21175850",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Mitigating Ambiguities in 3D Classification with Gaussian Splatting",
      "authors": [
        "Ruiqi Zhang",
        "Hao Zhu",
        "Jingyi Zhao",
        "Qi Zhang",
        "Xun Cao",
        "Zhan Ma"
      ],
      "abstract": "3D classification with point cloud input is a fundamental problem in 3D\nvision. However, due to the discrete nature and the insufficient material\ndescription of point cloud representations, there are ambiguities in\ndistinguishing wire-like and flat surfaces, as well as transparent or\nreflective objects. To address these issues, we propose Gaussian Splatting (GS)\npoint cloud-based 3D classification. We find that the scale and rotation\ncoefficients in the GS point cloud help characterize surface types.\nSpecifically, wire-like surfaces consist of multiple slender Gaussian\nellipsoids, while flat surfaces are composed of a few flat Gaussian ellipsoids.\nAdditionally, the opacity in the GS point cloud represents the transparency\ncharacteristics of objects. As a result, ambiguities in point cloud-based 3D\nclassification can be mitigated utilizing GS point cloud as input. To verify\nthe effectiveness of GS point cloud input, we construct the first real-world GS\npoint cloud dataset in the community, which includes 20 categories with 200\nobjects in each category. Experiments not only validate the superiority of GS\npoint cloud input, especially in distinguishing ambiguous objects, but also\ndemonstrate the generalization ability across different classification methods.",
      "doi": "arXiv:2503.08352v2",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "Edge Aware Learning for 3D Point Cloud",
      "authors": [
        "Lei Li"
      ],
      "abstract": "This paper proposes an innovative approach to Hierarchical Edge Aware 3D\nPoint Cloud Learning (HEA-Net) that seeks to address the challenges of noise in\npoint cloud data, and improve object recognition and segmentation by focusing\non edge features. In this study, we present an innovative edge-aware learning\nmethodology, specifically designed to enhance point cloud classification and\nsegmentation. Drawing inspiration from the human visual system, the concept of\nedge-awareness has been incorporated into this methodology, contributing to\nimproved object recognition while simultaneously reducing computational time.\nOur research has led to the development of an advanced 3D point cloud learning\nframework that effectively manages object classification and segmentation\ntasks. A unique fusion of local and global network learning paradigms has been\nemployed, enriched by edge-focused local and global embeddings, thereby\nsignificantly augmenting the model's interpretative prowess. Further, we have\napplied a hierarchical transformer architecture to boost point cloud processing\nefficiency, thus providing nuanced insights into structural understanding. Our\napproach demonstrates significant promise in managing noisy point cloud data\nand highlights the potential of edge-aware strategies in 3D point cloud\nlearning. The proposed approach is shown to outperform existing techniques in\nobject classification and segmentation tasks, as demonstrated by experiments on\nModelNet40 and ShapeNet datasets.",
      "doi": "arXiv:2309.13472v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Revisiting Point Cloud Classification: A New Benchmark Dataset and\n  Classification Model on Real-World Data",
      "authors": [
        "Mikaela Angelina Uy",
        "Quang-Hieu Pham",
        "Binh-Son Hua",
        "Duc Thanh Nguyen",
        "Sai-Kit Yeung"
      ],
      "abstract": "Deep learning techniques for point cloud data have demonstrated great\npotentials in solving classical problems in 3D computer vision such as 3D\nobject classification and segmentation. Several recent 3D object classification\nmethods have reported state-of-the-art performance on CAD model datasets such\nas ModelNet40 with high accuracy (~92%). Despite such impressive results, in\nthis paper, we argue that object classification is still a challenging task\nwhen objects are framed with real-world settings. To prove this, we introduce\nScanObjectNN, a new real-world point cloud object dataset based on scanned\nindoor scene data. From our comprehensive benchmark, we show that our dataset\nposes great challenges to existing point cloud classification techniques as\nobjects from real-world scans are often cluttered with background and/or are\npartial due to occlusions. We identify three key open problems for point cloud\nobject classification, and propose new point cloud classification neural\nnetworks that achieve state-of-the-art performance on classifying objects with\ncluttered background. Our dataset and code are publicly available in our\nproject page https://hkust-vgd.github.io/scanobjectnn/.",
      "doi": "arXiv:1908.04616v2",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Point-Syn2Real: Semi-Supervised Synthetic-to-Real Cross-Domain Learning\n  for Object Classification in 3D Point Clouds",
      "authors": [
        "Ziwei Wang",
        "Reza Arablouei",
        "Jiajun Liu",
        "Paulo Borges",
        "Greg Bishop-Hurley",
        "Nicholas Heaney"
      ],
      "abstract": "Object classification using LiDAR 3D point cloud data is critical for modern\napplications such as autonomous driving. However, labeling point cloud data is\nlabor-intensive as it requires human annotators to visualize and inspect the 3D\ndata from different perspectives. In this paper, we propose a semi-supervised\ncross-domain learning approach that does not rely on manual annotations of\npoint clouds and performs similar to fully-supervised approaches. We utilize\navailable 3D object models to train classifiers that can generalize to\nreal-world point clouds. We simulate the acquisition of point clouds by\nsampling 3D object models from multiple viewpoints and with arbitrary partial\nocclusions. We then augment the resulting set of point clouds through random\nrotations and adding Gaussian noise to better emulate the real-world scenarios.\nWe then train point cloud encoding models, e.g., DGCNN, PointNet++, on the\nsynthesized and augmented datasets and evaluate their cross-domain\nclassification performance on corresponding real-world datasets. We also\nintroduce Point-Syn2Real, a new benchmark dataset for cross-domain learning on\npoint clouds. The results of our extensive experiments with this dataset\ndemonstrate that the proposed cross-domain learning approach for point clouds\noutperforms the related baseline and state-of-the-art approaches in both indoor\nand outdoor settings in terms of cross-domain generalizability. The code and\ndata will be available upon publishing.",
      "doi": "arXiv:2210.17009v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Airborne LiDAR point cloud classification using PointNet++ network with full neighborhood features.",
      "authors": [
        "Nong X",
        "Bai W",
        "Liu G"
      ],
      "abstract": "Compared with other point clouds, the airborne LiDAR point cloud has its own characteristics. The deep learning network PointNet++ ignores the inherent properties of airborne LiDAR point, and the classification precision is low. Therefore, we propose a framework based on the PointNet++ network. In this work, we proposed an interpolation method that uses adaptive elevation weight to make full use of the objects in the airborne LiDAR point, which exhibits discrepancies in elevation distributions. The class-balanced loss function is used for the uneven density distribution of point cloud data. Moreover, the relationship between a point and its neighbours is captured, densely connecting point pairs in multiscale regions and adding centroid features to learn contextual information. Experiments are conducted on the Vaihingen 3D semantic labelling benchmark dataset and GML(B) benchmark dataset. The experiments show that the proposed method, which has additional contextual information and makes full use of the airborne LiDAR point cloud properties to support classification, achieves high accuracy and can be widely used in airborne LiDAR point classification.",
      "doi": "https://doi.org/10.1371/journal.pone.0280346",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Efficient three-dimensional point cloud object detection based on improved Complex-YOLO.",
      "authors": [
        "Shao Y",
        "Sun Z",
        "Tan A",
        "Yan T"
      ],
      "abstract": "Lidar-based 3D object detection and classification is a critical task for autonomous driving. However, inferencing from exceedingly sparse 3D data in real-time is a formidable challenge. Complex-YOLO solves the problem of point cloud disorder and sparsity by projecting it onto the bird's-eye view and realizes real-time 3D object detection based on LiDAR. However, Complex-YOLO has no object height detection, a shallow network depth, and poor small-size object detection accuracy. To address these issues, this paper has made the following improvements: (1) adds a multi-scale feature fusion network to improve the algorithm's capability to detect small-size objects; (2) uses a more advanced RepVGG as the backbone network to improve network depth and overall detection performance; and (3) adds an effective height detector to the network to improve the height detection. Through experiments, we found that our algorithm's accuracy achieved good performance on the KITTI dataset, while the detection speed and memory usage were very superior, 48FPS on RTX3070Ti and 20FPS on GTX1060, with a memory usage of 841Mib.",
      "doi": "https://doi.org/10.3389/fnbot.2023.1092564",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Privacy protection for 3D point cloud classification based on an optical chaotic encryption scheme.",
      "authors": [
        "Liu B",
        "Liu Y",
        "Xie Y",
        "Jiang X",
        "Ye Y",
        "Song T",
        "Chai J",
        "Liu M",
        "Feng M",
        "Yuan H"
      ],
      "abstract": "In allusion to the privacy and security problems in 3D point cloud classification, a novel privacy protection method for 3D point cloud classification based on optical chaotic encryption scheme is proposed and implemented in this paper for the first time. The mutually coupled spin-polarized vertical-cavity surface-emitting lasers (MC-SPVCSELs) subject to double optical feedback (DOF) are studied to generate optical chaos for permutation and diffusion encryption process of 3D point cloud. The nonlinear dynamics and complexity results demonstrate that the MC-SPVCSELs with DOF have high chaotic complexity and can provide tremendously large key space. All the test-sets of ModelNet40 dataset containing 40 object categories are encrypted and decrypted by the proposed scheme, and then the classification results of 40 object categories for original, encrypted, and decrypted 3D point cloud are entirely enumerated through the PointNet++. Intriguingly, the class accuracies of the encrypted point cloud are nearly all equal to 0.0000% except for the plant class with 100.0000%, indicating the encrypted point cloud cannot be classified and identified. The decryption class accuracies are very close to the original class accuracies. Therefore, the classification results verify that the proposed privacy protection scheme is practically feasible and remarkably effective. Additionally, the encryption and decryption results show that the encrypted point cloud images are ambiguous and unrecognizable, while the decrypted point cloud images are identical to original images. Moreover, this paper improves the security analysis via analyzing 3D point cloud geometric features. Eventually, various security analysis results validate that the proposed privacy protection scheme has high security level and good privacy protection effect for 3D point cloud classification.",
      "doi": "https://doi.org/10.1364/oe.483522",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Object Recognition, Segmentation, and Classification of Mobile Laser Scanning Point Clouds: A State of the Art Review.",
      "authors": [
        "Che E",
        "Jung J",
        "Olsen MJ"
      ],
      "abstract": "Mobile Laser Scanning (MLS) is a versatile remote sensing technology based on Light Detection and Ranging (lidar) technology that has been utilized for a wide range of applications. Several previous reviews focused on applications or characteristics of these systems exist in the literature, however, reviews of the many innovative data processing strategies described in the literature have not been conducted in sufficient depth. To this end, we review and summarize the state of the art for MLS data processing approaches, including feature extraction, segmentation, object recognition, and classification. In this review, we first discuss the impact of the scene type to the development of an MLS data processing method. Then, where appropriate, we describe relevant generalized algorithms for feature extraction and segmentation that are applicable to and implemented in many processing approaches. The methods for object recognition and point cloud classification are further reviewed including both the general concepts as well as technical details. In addition, available benchmark datasets for object recognition and classification are summarized. Further, the current limitations and challenges that a significant portion of point cloud processing techniques face are discussed. This review concludes with our future outlook of the trends and opportunities of MLS data processing algorithms and applications.",
      "doi": "https://doi.org/10.3390/s19040810",
      "year": 2019,
      "source": "PubMed"
    },
    {
      "title": "JSNet: Joint Instance and Semantic Segmentation of 3D Point Clouds",
      "authors": [
        "Lin Zhao",
        "Wenbing Tao"
      ],
      "abstract": "In this paper, we propose a novel joint instance and semantic segmentation\napproach, which is called JSNet, in order to address the instance and semantic\nsegmentation of 3D point clouds simultaneously. Firstly, we build an effective\nbackbone network to extract robust features from the raw point clouds.\nSecondly, to obtain more discriminative features, a point cloud feature fusion\nmodule is proposed to fuse the different layer features of the backbone\nnetwork. Furthermore, a joint instance semantic segmentation module is\ndeveloped to transform semantic features into instance embedding space, and\nthen the transformed features are further fused with instance features to\nfacilitate instance segmentation. Meanwhile, this module also aggregates\ninstance features into semantic feature space to promote semantic segmentation.\nFinally, the instance predictions are generated by applying a simple mean-shift\nclustering on instance embeddings. As a result, we evaluate the proposed JSNet\non a large-scale 3D indoor point cloud dataset S3DIS and a part dataset\nShapeNet, and compare it with existing approaches. Experimental results\ndemonstrate our approach outperforms the state-of-the-art method in 3D instance\nsegmentation with a significant improvement in 3D semantic prediction and our\nmethod is also beneficial for part segmentation. The source code for this work\nis available at https://github.com/dlinzhao/JSNet.",
      "doi": "arXiv:1912.09654v1",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Few-Shot 3D Point Cloud Semantic Segmentation via Stratified\n  Class-Specific Attention Based Transformer Network",
      "authors": [
        "Canyu Zhang",
        "Zhenyao Wu",
        "Xinyi Wu",
        "Ziyu Zhao",
        "Song Wang"
      ],
      "abstract": "3D point cloud semantic segmentation aims to group all points into different\nsemantic categories, which benefits important applications such as point cloud\nscene reconstruction and understanding. Existing supervised point cloud\nsemantic segmentation methods usually require large-scale annotated point\nclouds for training and cannot handle new categories. While a few-shot learning\nmethod was proposed recently to address these two problems, it suffers from\nhigh computational complexity caused by graph construction and inability to\nlearn fine-grained relationships among points due to the use of pooling\noperations. In this paper, we further address these problems by developing a\nnew multi-layer transformer network for few-shot point cloud semantic\nsegmentation. In the proposed network, the query point cloud features are\naggregated based on the class-specific support features in different scales.\nWithout using pooling operations, our method makes full use of all pixel-level\nfeatures from the support samples. By better leveraging the support features\nfor few-shot learning, the proposed method achieves the new state-of-the-art\nperformance, with 15\\% less inference time, over existing few-shot 3D point\ncloud segmentation models on the S3DIS dataset and the ScanNet dataset.",
      "doi": "arXiv:2303.15654v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation\n  through Self-Attention and Multiscale",
      "authors": [
        "Shuochen Xu",
        "Zhenxin Zhang"
      ],
      "abstract": "The semantic understanding of indoor 3D point cloud data is crucial for a\nrange of subsequent applications, including indoor service robots, navigation\nsystems, and digital twin engineering. Global features are crucial for\nachieving high-quality semantic and instance segmentation of indoor point\nclouds, as they provide essential long-range context information. To this end,\nwe propose JSMNet, which combines a multi-layer network with a global feature\nself-attention module to jointly segment three-dimensional point cloud\nsemantics and instances. To better express the characteristics of indoor\ntargets, we have designed a multi-resolution feature adaptive fusion module\nthat takes into account the differences in point cloud density caused by\nvarying scanner distances from the target. Additionally, we propose a framework\nfor joint semantic and instance segmentation by integrating semantic and\ninstance features to achieve superior results. We conduct experiments on S3DIS,\nwhich is a large three-dimensional indoor point cloud dataset. Our proposed\nmethod is compared against other methods, and the results show that it\noutperforms existing methods in semantic and instance segmentation and provides\nbetter results in target local area segmentation. Specifically, our proposed\nmethod outperforms PointNet (Qi et al., 2017a) by 16.0% and 26.3% in terms of\nsemantic segmentation mIoU in S3DIS (Area 5) and instance segmentation mPre,\nrespectively. Additionally, it surpasses ASIS (Wang et al., 2019) by 6.0% and\n4.6%, respectively, as well as JSPNet (Chen et al., 2022) by a margin of 3.3%\nfor semantic segmentation mIoU and a slight improvement of 0.3% for instance\nsegmentation mPre.",
      "doi": "arXiv:2309.07425v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "PCSCNet: Fast 3D Semantic Segmentation of LiDAR Point Cloud for\n  Autonomous Car using Point Convolution and Sparse Convolution Network",
      "authors": [
        "Jaehyun Park",
        "Chansoo Kim",
        "Kichun Jo"
      ],
      "abstract": "The autonomous car must recognize the driving environment quickly for safe\ndriving. As the Light Detection And Range (LiDAR) sensor is widely used in the\nautonomous car, fast semantic segmentation of LiDAR point cloud, which is the\npoint-wise classification of the point cloud within the sensor framerate, has\nattracted attention in recognition of the driving environment. Although the\nvoxel and fusion-based semantic segmentation models are the state-of-the-art\nmodel in point cloud semantic segmentation recently, their real-time\nperformance suffer from high computational load due to high voxel resolution.\nIn this paper, we propose the fast voxel-based semantic segmentation model\nusing Point Convolution and 3D Sparse Convolution (PCSCNet). The proposed model\nis designed to outperform at both high and low voxel resolution using point\nconvolution-based feature extraction. Moreover, the proposed model accelerates\nthe feature propagation using 3D sparse convolution after the feature\nextraction. The experimental results demonstrate that the proposed model\noutperforms the state-of-the-art real-time models in semantic segmentation of\nSemanticKITTI and nuScenes, and achieves the real-time performance in LiDAR\npoint cloud inference.",
      "doi": "arXiv:2202.10047v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Point Cloud Semantic Segmentation Network Based on Multi-Scale Feature Fusion.",
      "authors": [
        "Du J",
        "Jiang Z",
        "Huang S",
        "Wang Z",
        "Su J",
        "Su S",
        "Wu Y",
        "Cai G"
      ],
      "abstract": "The semantic segmentation of small objects in point clouds is currently one of the most demanding tasks in photogrammetry and remote sensing applications. Multi-resolution feature extraction and fusion can significantly enhance the ability of object classification and segmentation, so it is widely used in the image field. For this motivation, we propose a point cloud semantic segmentation network based on multi-scale feature fusion (MSSCN) to aggregate the feature of a point cloud with different densities and improve the performance of semantic segmentation. In our method, random downsampling is first applied to obtain point clouds of different densities. A Spatial Aggregation Net (SAN) is then employed as the backbone network to extract local features from these point clouds, followed by concatenation of the extracted feature descriptors at different scales. Finally, a loss function is used to combine the different semantic information from point clouds of different densities for network optimization. Experiments were conducted on the S3DIS and ScanNet datasets, and our MSSCN achieved accuracies of 89.80% and 86.3%, respectively, on these datasets. Our method showed better performance than the recent methods PointNet, PointNet++, PointCNN, PointSIFT, and SAN.",
      "doi": "https://doi.org/10.3390/s21051625",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Compositional Semantic Mix for Domain Adaptation in Point Cloud Segmentation.",
      "authors": [
        "Saltori C",
        "Galasso F",
        "Fiameni G",
        "Sebe N",
        "Poiesi F",
        "Ricci E"
      ],
      "abstract": "Deep-learning models for 3D point cloud semantic segmentation exhibit limited generalization capabilities when trained and tested on data captured with different sensors or in varying environments due to domain shift. Domain adaptation methods can be employed to mitigate this domain shift, for instance, by simulating sensor noise, developing domain-agnostic generators, or training point cloud completion networks. Often, these methods are tailored for range view maps or necessitate multi-modal input. In contrast, domain adaptation in the image domain can be executed through sample mixing, which emphasizes input data manipulation rather than employing distinct adaptation modules. In this study, we introduce compositional semantic mixing for point cloud domain adaptation, representing the first unsupervised domain adaptation technique for point cloud segmentation based on semantic and geometric sample mixing. We present a two-branch symmetric network architecture capable of concurrently processing point clouds from a source domain (e.g. synthetic) and point clouds from a target domain (e.g. real-world). Each branch operates within one domain by integrating selected data fragments from the other domain and utilizing semantic information derived from source labels and target (pseudo) labels. Additionally, our method can leverage a limited number of human point-level annotations (semi-supervised) to further enhance performance. We assess our approach in both synthetic-to-real and real-to-real scenarios using LiDAR datasets and demonstrate that it significantly outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.",
      "doi": "https://doi.org/10.1109/tpami.2023.3310261",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Filtering-Assisted Airborne Point Cloud Semantic Segmentation for Transmission Lines.",
      "authors": [
        "Yan W",
        "Ma W",
        "Wu X",
        "Wang C",
        "Zhang J",
        "Deng Y"
      ],
      "abstract": "Point cloud semantic segmentation is crucial for identifying and analyzing transmission lines. Due to the number of point clouds being huge, complex scenes, and unbalanced sample proportion, the mainstream machine learning methods of point cloud segmentation cannot provide high efficiency and accuracy when extending to transmission line scenes. This paper proposes a filter-assisted airborne point cloud semantic segmentation for transmission lines. First, a large number of ground point clouds is identified by introducing the well-developed cloth simulation filter to alleviate the impact of the imbalance of the target object proportion on the classifier's performance. The multi-dimensional features are then defined, and the classification model is trained to achieve the multi-element semantic segmentation of the transmission line scene. The experimental results and analysis indicate that the proposed filter-assisted algorithm can significantly improve the semantic segmentation performance of the transmission line point cloud, enhancing both the point cloud segmentation efficiency and accuracy by more than 25.46% and 3.15%, respectively. The filter-assisted point cloud semantic segmentation method reduces the volume of sample data, the number of sample classes, and the sample imbalance index in power line scenarios to a certain extent, thereby improving the classification accuracy of classifiers and reducing time consumption. This research holds significant theoretical reference value and engineering application potential for scene reconstruction and intelligent understanding of airborne laser point cloud transmission lines.",
      "doi": "https://doi.org/10.3390/s24217028",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Semantic Point Cloud Segmentation Using Fast Deep Neural Network and DCRF.",
      "authors": [
        "Rao Y",
        "Zhang M",
        "Cheng Z",
        "Xue J",
        "Pu J",
        "Wang Z"
      ],
      "abstract": "Accurate segmentation of entity categories is the critical step for 3D scene understanding. This paper presents a fast deep neural network model with Dense Conditional Random Field (DCRF) as a post-processing method, which can perform accurate semantic segmentation for 3D point cloud scene. On this basis, a compact but flexible framework is introduced for performing segmentation to the semantics of point clouds concurrently, contribute to more precise segmentation. Moreover, based on semantics labels, a novel DCRF model is elaborated to refine the result of segmentation. Besides, without any sacrifice to accuracy, we apply optimization to the original data of the point cloud, allowing the network to handle fewer data. In the experiment, our proposed method is conducted comprehensively through four evaluation indicators, proving the superiority of our method.",
      "doi": "https://doi.org/10.3390/s21082731",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "3D Object Detection Method Based on YOLO and K-Means for Image and Point\n  Clouds",
      "authors": [
        "Xuanyu Yin",
        "Yoko Sasaki",
        "Weimin Wang",
        "Kentaro Shimizu"
      ],
      "abstract": "Lidar based 3D object detection and classification tasks are essential for\nautonomous driving(AD). A lidar sensor can provide the 3D point cloud data\nreconstruction of the surrounding environment. However, real time detection in\n3D point clouds still needs a strong algorithmic. This paper proposes a 3D\nobject detection method based on point cloud and image which consists of there\nparts.(1)Lidar-camera calibration and undistorted image transformation.\n(2)YOLO-based detection and PointCloud extraction, (3)K-means based point cloud\nsegmentation and detection experiment test and evaluation in depth image. In\nour research, camera can capture the image to make the Real-time 2D object\ndetection by using YOLO, we transfer the bounding box to node whose function is\nmaking 3d object detection on point cloud data from Lidar. By comparing whether\n2D coordinate transferred from the 3D point is in the object bounding box or\nnot can achieve High-speed 3D object recognition function in GPU. The accuracy\nand precision get imporved after k-means clustering in point cloud. The speed\nof our detection method is a advantage faster than PointNet.",
      "doi": "arXiv:2005.02132v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Deep learning for 3D Object Detection and Tracking in Autonomous\n  Driving: A Brief Survey",
      "authors": [
        "Yang Peng"
      ],
      "abstract": "Object detection and tracking are vital and fundamental tasks for autonomous\ndriving, aiming at identifying and locating objects from those predefined\ncategories in a scene. 3D point cloud learning has been attracting more and\nmore attention among all other forms of self-driving data. Currently, there are\nmany deep learning methods for 3D object detection. However, the tasks of\nobject detection and tracking for point clouds still need intensive study due\nto the unique characteristics of point cloud data. To help get a good grasp of\nthe present situation of this research, this paper shows recent advances in\ndeep learning methods for 3D object detection and tracking.",
      "doi": "arXiv:2311.06043v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "No More Potentially Dynamic Objects: Static Point Cloud Map Generation\n  based on 3D Object Detection and Ground Projection",
      "authors": [
        "Soojin Woo",
        "Donghwi Jung",
        "Seong-Woo Kim"
      ],
      "abstract": "In this paper, we propose an algorithm to generate a static point cloud map\nbased on LiDAR point cloud data. Our proposed pipeline detects dynamic objects\nusing 3D object detectors and projects points of dynamic objects onto the\nground. Typically, point cloud data acquired in real-time serves as a snapshot\nof the surrounding areas containing both static objects and dynamic objects.\nThe static objects include buildings and trees, otherwise, the dynamic objects\ncontain objects such as parked cars that change their position over time.\nRemoving dynamic objects from the point cloud map is crucial as they can\ndegrade the quality and localization accuracy of the map. To address this\nissue, in this paper, we propose an algorithm that creates a map only\nconsisting of static objects. We apply a 3D object detection algorithm to the\npoint cloud data which are obtained from LiDAR to implement our pipeline. We\nthen stack the points to create the map after performing ground segmentation\nand projection. As a result, not only we can eliminate currently dynamic\nobjects at the time of map generation but also potentially dynamic objects such\nas parked vehicles. We validate the performance of our method using two kinds\nof datasets collected on real roads: KITTI and our dataset. The result\ndemonstrates the capability of our proposal to create an accurate static map\nexcluding dynamic objects from input point clouds. Also, we verified the\nimproved performance of localization using a generated map based on our method.",
      "doi": "arXiv:2407.01073v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Toward Unsupervised 3D Point Cloud Anomaly Detection using Variational\n  Autoencoder",
      "authors": [
        "Mana Masuda",
        "Ryo Hachiuma",
        "Ryo Fujii",
        "Hideo Saito",
        "Yusuke Sekikawa"
      ],
      "abstract": "In this paper, we present an end-to-end unsupervised anomaly detection\nframework for 3D point clouds. To the best of our knowledge, this is the first\nwork to tackle the anomaly detection task on a general object represented by a\n3D point cloud. We propose a deep variational autoencoder-based unsupervised\nanomaly detection network adapted to the 3D point cloud and an anomaly score\nspecifically for 3D point clouds. To verify the effectiveness of the model, we\nconducted extensive experiments on the ShapeNet dataset. Through quantitative\nand qualitative evaluation, we demonstrate that the proposed method outperforms\nthe baseline method. Our code is available at\nhttps://github.com/llien30/point_cloud_anomaly_detection.",
      "doi": "arXiv:2304.03420v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "3D Cascade RCNN: High Quality Object Detection in Point Clouds.",
      "authors": [
        "Cai Q",
        "Pan Y",
        "Yao T",
        "Mei T"
      ],
      "abstract": "Recent progress on 2D object detection has featured Cascade RCNN, which capitalizes on a sequence of cascade detectors to progressively improve proposal quality, towards high-quality object detection. However, there has not been evidence in support of building such cascade structures for 3D object detection, a challenging detection scenario with highly sparse LiDAR point clouds. In this work, we present a simple yet effective cascade architecture, named 3D Cascade RCNN, that allocates multiple detectors based on the voxelized point clouds in a cascade paradigm, pursuing higher quality 3D object detector progressively. Furthermore, we quantitatively define the sparsity level of the points within 3D bounding box of each object as the point completeness score, which is exploited as the task weight for each proposal to guide the learning of each stage detector. The spirit behind is to assign higher weights for high-quality proposals with relatively complete point distribution, while down-weight the proposals with extremely sparse points that often incur noise during training. This design of completeness-aware re-weighting elegantly upgrades the cascade paradigm to be better applicable for the sparse input data, without increasing any FLOP budgets. Through extensive experiments on both the KITTI dataset and Waymo Open Dataset, we validate the superiority of our proposed 3D Cascade RCNN, when comparing to state-of-the-art 3D object detection techniques. The source code is publicly available at https://github.com/caiqi/Cascasde-3D.",
      "doi": "https://doi.org/10.1109/tip.2022.3201469",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Relation Graph Network for 3D Object Detection in Point Clouds.",
      "authors": [
        "Feng M",
        "Gilani SZ",
        "Wang Y",
        "Zhang L",
        "Mian A"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) have emerged as a powerful tool for object detection in 2D images. However, their power has not been fully realised for detecting 3D objects directly in point clouds without conversion to regular grids. Moreover, existing state-of-the-art 3D object detection methods aim to recognize objects individually without exploiting their relationships during learning or inference. In this article, we first propose a strategy that associates the predictions of direction vectors with pseudo geometric centers, leading to a win-win solution for 3D bounding box candidates regression. Secondly, we propose point attention pooling to extract uniform appearance features for each 3D object proposal, benefiting from the learned direction features, semantic features and spatial coordinates of the object points. Finally, the appearance features are used together with the position features to build 3D object-object relationship graphs for all proposals to model their co-existence. We explore the effect of relation graphs on proposals' appearance feature enhancement under supervised and unsupervised settings. The proposed relation graph network comprises a 3D object proposal generation module and a 3D relation module, making it an end-to-end trainable network for detecting 3D objects in point clouds. Experiments on challenging benchmark point cloud datasets (SunRGB-D, ScanNet and KITTI) show that our algorithm performs better than existing state-of-the-art.",
      "doi": "https://doi.org/10.1109/tip.2020.3031371",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Point Cloud Compression: Impact on Object Detection in Outdoor Contexts.",
      "authors": [
        "Garrote L",
        "Perdiz J",
        "da Silva Cruz LA",
        "Nunes UJ"
      ],
      "abstract": "Increasing demand for more reliable and safe autonomous driving means that data involved in the various aspects of perception, such as object detection, will become more granular as the number and resolution of sensors progress. Using these data for on-the-fly object detection causes problems related to the computational complexity of onboard processing in autonomous vehicles, leading to a desire to offload computation to roadside infrastructure using vehicle-to-infrastructure communication links. The need to transmit sensor data also arises in the context of vehicle fleets exchanging sensor data, over vehicle-to-vehicle communication links. Some types of sensor data modalities, such as Light Detection and Ranging (LiDAR) point clouds, are so voluminous that their transmission is impractical without data compression. With most emerging autonomous driving implementations being anchored on point cloud data, we propose to evaluate the impact of point cloud compression on object detection. To that end, two different object detection architectures are evaluated using point clouds from the KITTI object dataset: raw point clouds and point clouds compressed with a state-of-the-art encoder and three different compression levels. The analysis is extended to the impact of compression on depth maps generated from images projected from the point clouds, with two conversion methods tested. Results show that low-to-medium levels of compression do not have a major impact on object detection performance, especially for larger objects. Results also show that the impact of point cloud compression is lower when detecting objects using depth maps, placing this particular method of point cloud data representation on a competitive footing compared to raw point cloud data.",
      "doi": "https://doi.org/10.3390/s22155767",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Refined Voting and Scene Feature Fusion for 3D Object Detection in Point Clouds.",
      "authors": [
        "Yu H",
        "Su J",
        "Piao Y",
        "Cai G",
        "Lin Y",
        "Liu N",
        "Liu W"
      ],
      "abstract": "An essential task for 3D visual world understanding is 3D object detection in lidar point clouds. To predict directly bounding box parameters from point clouds, existing voting-based methods use Hough voting to obtain the centroid of each object. However, it may be difficult for the inaccurately voted centers to regress boxes accurately, leading to the generation of redundant bounding boxes. For objects in indoor scenes, there are several co-occurrence patterns for objects in indoor scenes. Concurrently, semantic relations between object layouts and scenes can be used as prior context to guide object detection. We propose a simple, yet effective network, RSFF-Net, which adds refined voting and scene feature fusion for indoor 3D object detection. The RSFF-Net consists of three modules: geometric function, refined voting, and scene constraint. First, a geometric function module is used to capture the geometric features of the nearest object of the voted points. Then, the coarse votes are revoted by a refined voting module, which is based on the fused feature between the coarse votes and geometric features. Finally, a scene constraint module is used to add the association information between candidate objects and scenes. RSFF-Net achieves competitive results on indoor 3D object detection benchmarks: ScanNet V2 and SUN RGB-D.",
      "doi": "https://doi.org/10.1155/2022/3023934",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Geodesic-Former: a Geodesic-Guided Few-shot 3D Point Cloud Instance\n  Segmenter",
      "authors": [
        "Tuan Ngo",
        "Khoi Nguyen"
      ],
      "abstract": "This paper introduces a new problem in 3D point cloud: few-shot instance\nsegmentation. Given a few annotated point clouds exemplified a target class,\nour goal is to segment all instances of this target class in a query point\ncloud. This problem has a wide range of practical applications where point-wise\ninstance segmentation annotation is prohibitively expensive to collect. To\naddress this problem, we present Geodesic-Former -- the first geodesic-guided\ntransformer for 3D point cloud instance segmentation. The key idea is to\nleverage the geodesic distance to tackle the density imbalance of LiDAR 3D\npoint clouds. The LiDAR 3D point clouds are dense near the object surface and\nsparse or empty elsewhere making the Euclidean distance less effective to\ndistinguish different objects. The geodesic distance, on the other hand, is\nmore suitable since it encodes the scene's geometry which can be used as a\nguiding signal for the attention mechanism in a transformer decoder to generate\nkernels representing distinct features of instances. These kernels are then\nused in a dynamic convolution to obtain the final instance masks. To evaluate\nGeodesic-Former on the new task, we propose new splits of the two common 3D\npoint cloud instance segmentation datasets: ScannetV2 and S3DIS.\nGeodesic-Former consistently outperforms strong baselines adapted from\nstate-of-the-art 3D point cloud instance segmentation approaches with a\nsignificant margin. Code is available at\nhttps://github.com/VinAIResearch/GeoFormer.",
      "doi": "arXiv:2207.10859v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance\n  Segmentation",
      "authors": [
        "Linghua Tang",
        "Le Hui",
        "Jin Xie"
      ],
      "abstract": "Due to the few annotated labels of 3D point clouds, how to learn\ndiscriminative features of point clouds to segment object instances is a\nchallenging problem. In this paper, we propose a simple yet effective 3D\ninstance segmentation framework that can achieve good performance by annotating\nonly one point for each instance. Specifically, to tackle extremely few labels\nfor instance segmentation, we first oversegment the point cloud into\nsuperpoints in an unsupervised manner and extend the point-level annotations to\nthe superpoint level. Then, based on the superpoint graph, we propose an\ninter-superpoint affinity mining module that considers the semantic and spatial\nrelations to adaptively learn inter-superpoint affinity to generate\nhigh-quality pseudo labels via semantic-aware random walk. Finally, we propose\na volume-aware instance refinement module to segment high-quality instances by\napplying volume constraints of objects in clustering on the superpoint graph.\nExtensive experiments on the ScanNet-v2 and S3DIS datasets demonstrate that our\nmethod achieves state-of-the-art performance in the weakly supervised point\ncloud instance segmentation task, and even outperforms some fully supervised\nmethods.",
      "doi": "arXiv:2210.05534v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Point Cloud Instance Segmentation with Inaccurate Bounding-Box Annotations.",
      "authors": [
        "Peng Y",
        "Feng H",
        "Chen T",
        "Hu B"
      ],
      "abstract": "Most existing point cloud instance segmentation methods require accurate and dense point-level annotations, which are extremely laborious to collect. While incomplete and inexact supervision has been exploited to reduce labeling efforts, inaccurate supervision remains under-explored. This kind of supervision is almost inevitable in practice, especially in complex 3D point clouds, and it severely degrades the generalization performance of deep networks. To this end, we propose the first weakly supervised point cloud instance segmentation framework with inaccurate box-level labels. A novel self-distillation architecture is presented to boost the generalization ability while leveraging the cheap but noisy bounding-box annotations. Specifically, we employ consistency regularization to distill self-knowledge from data perturbation and historical predictions, which prevents the deep network from overfitting the noisy labels. Moreover, we progressively select reliable samples and correct their labels based on the historical consistency. Extensive experiments on the ScanNet-v2 dataset were used to validate the effectiveness and robustness of our method in dealing with inexact and inaccurate annotations.",
      "doi": "https://doi.org/10.3390/s23042343",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Dynamic Convolution for 3D Point Cloud Instance Segmentation.",
      "authors": [
        "He T",
        "Shen C",
        "van den Hengel A"
      ],
      "abstract": "In this paper, we come up with a simple yet effective approach for instance segmentation on 3D point cloud with strong robustness. Previous top-performing methods for this task adopt a bottom-up strategy, which often involves various inefficient operations or complex pipelines, such as grouping over-segmented components, introducing heuristic post-processing steps, and designing complex loss functions. As a result, the inevitable variations of the instances sizes make it vulnerable and sensitive to the values of pre-defined hyper-parameters. To this end, we instead propose a novel pipeline that applies dynamic convolution to generate instance-aware parameters in response to the characteristics of the instances. The representation capability of the parameters is greatly improved by gathering homogeneous points that have identical semantic categories and close votes for the geometric centroids. Instances are then decoded via several simple convolution layers, where the parameters are generated depending on the input. In addition, to introduce a large context and maintain limited computational overheads, a light-weight transformer is built upon the bottleneck layer to capture the long-range dependencies. With the only post-processing step, non-maximum suppression (NMS), we demonstrate a simpler and more robust approach that achieves promising performance on various datasets: ScanNetV2, S3DIS, and PartNet. The consistent improvements on both voxel- and point-based architectures imply the effectiveness of the proposed method. Code is available at: https://git.io/DyCo3D.",
      "doi": "https://doi.org/10.1109/tpami.2022.3216926",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Point Cloud Instance Segmentation With Semi-Supervised Bounding-Box Mining.",
      "authors": [
        "Liao Y",
        "Zhu H",
        "Zhang Y",
        "Ye C",
        "Chen T",
        "Fan J"
      ],
      "abstract": "Point cloud instance segmentation has achieved huge progress with the emergence of deep learning. However, these methods are usually data-hungry with expensive and time-consuming dense point cloud annotations. To alleviate the annotation cost, unlabeled or weakly labeled data is still less explored in the task. In this paper, we introduce the first semi-supervised point cloud instance segmentation framework (SPIB) using both labeled and unlabelled bounding boxes as supervision. To be specific, our SPIB architecture involves a two-stage learning procedure. For stage one, a bounding box proposal generation network is trained under a semi-supervised setting with perturbation consistency regularization (SPCR). The regularization works by enforcing an invariance of the bounding box predictions over different perturbations applied to the input point clouds, to provide self-supervision for network learning. For stage two, the bounding box proposals with SPCR are grouped into some subsets, and the instance masks are mined inside each subset with a novel semantic propagation module and a property consistency graph module. Moreover, we introduce a novel occupancy ratio guided refinement module to refine the instance masks. Extensive experiments on the challenging ScanNet v2 dataset demonstrate our method can achieve competitive performance compared with the recent fully-supervised methods.",
      "doi": "https://doi.org/10.1109/tpami.2021.3131120",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Point-Cloud Instance Segmentation for Spinning Laser Sensors.",
      "authors": [
        "Casado-Coscolla A",
        "Sanchez-Belenguer C",
        "Wolfart E",
        "Sequeira V"
      ],
      "abstract": "In this paper, we face the point-cloud segmentation problem for spinning laser sensors from a deep-learning (DL) perspective. Since the sensors natively provide their measurements in a 2D grid, we directly use state-of-the-art models designed for visual information for the segmentation task and then exploit the range information to ensure 3D accuracy. This allows us to effectively address the main challenges of applying DL techniques to point clouds, i.e., lack of structure and increased dimensionality. To the best of our knowledge, this is the first work that faces the 3D segmentation problem from a 2D perspective without explicitly re-projecting 3D point clouds. Moreover, our approach exploits multiple channels available in modern sensors, i.e., range, reflectivity, and ambient illumination. We also introduce a novel data-mining pipeline that enables the annotation of 3D scans without human intervention. Together with this paper, we present a new public dataset with all the data collected for training and evaluating our approach, where point clouds preserve their native sensor structure and where every single measurement contains range, reflectivity, and ambient information, together with its associated 3D point. As experimental results show, our approach achieves state-of-the-art results both in terms of performance and inference time. Additionally, we provide a novel ablation test that analyses the individual and combined contributions of the different channels provided by modern laser sensors.",
      "doi": "https://doi.org/10.3390/jimaging10120325",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Self-Supervised Deep Learning on Point Clouds by Reconstructing Space",
      "authors": [
        "Jonathan Sauder",
        "Bjarne Sievers"
      ],
      "abstract": "Point clouds provide a flexible and natural representation usable in\ncountless applications such as robotics or self-driving cars. Recently, deep\nneural networks operating on raw point cloud data have shown promising results\non supervised learning tasks such as object classification and semantic\nsegmentation. While massive point cloud datasets can be captured using modern\nscanning technology, manually labelling such large 3D point clouds for\nsupervised learning tasks is a cumbersome process. This necessitates methods\nthat can learn from unlabelled data to significantly reduce the number of\nannotated samples needed in supervised learning. We propose a self-supervised\nlearning task for deep learning on raw point cloud data in which a neural\nnetwork is trained to reconstruct point clouds whose parts have been randomly\nrearranged. While solving this task, representations that capture semantic\nproperties of the point cloud are learned. Our method is agnostic of network\narchitecture and outperforms current unsupervised learning approaches in\ndownstream object classification tasks. We show experimentally, that\npre-training with our method before supervised training improves the\nperformance of state-of-the-art models and significantly improves sample\nefficiency.",
      "doi": "arXiv:1901.08396v2",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "ABD-Net: Attention Based Decomposition Network for 3D Point Cloud\n  Decomposition",
      "authors": [
        "Siddharth Katageri",
        "Shashidhar V Kudari",
        "Akshaykumar Gunari",
        "Ramesh Ashok Tabib",
        "Uma Mudenagudi"
      ],
      "abstract": "In this paper, we propose Attention Based Decomposition Network (ABD-Net),\nfor point cloud decomposition into basic geometric shapes namely, plane,\nsphere, cone and cylinder. We show improved performance of 3D object\nclassification using attention features based on primitive shapes in point\nclouds. Point clouds, being the simple and compact representation of 3D objects\nhave gained increasing popularity. They demand robust methods for feature\nextraction due to unorderness in point sets. In ABD-Net the proposed Local\nProximity Encapsulator captures the local geometric variations along with\nspatial encoding around each point from the input point sets. The encapsulated\nlocal features are further passed to proposed Attention Feature Encoder to\nlearn basic shapes in point cloud. Attention Feature Encoder models geometric\nrelationship between the neighborhoods of all the points resulting in capturing\nglobal point cloud information. We demonstrate the results of our proposed\nABD-Net on ANSI mechanical component and ModelNet40 datasets. We also\ndemonstrate the effectiveness of ABD-Net over the acquired attention features\nby improving the performance of 3D object classification on ModelNet40\nbenchmark dataset and compare them with state-of-the-art techniques.",
      "doi": "arXiv:2108.04221v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Classification of Single-View Object Point Clouds",
      "authors": [
        "Zelin Xu",
        "Ke Chen",
        "Kangjun Liu",
        "Changxing Ding",
        "Yaowei Wang",
        "Kui Jia"
      ],
      "abstract": "Object point cloud classification has drawn great research attention since\nthe release of benchmarking datasets, such as the ModelNet and the ShapeNet.\nThese benchmarks assume point clouds covering complete surfaces of object\ninstances, for which plenty of high-performing methods have been developed.\nHowever, their settings deviate from those often met in practice, where, due to\n(self-)occlusion, a point cloud covering partial surface of an object is\ncaptured from an arbitrary view. We show in this paper that performance of\nexisting point cloud classifiers drops drastically under the considered\nsingle-view, partial setting; the phenomenon is consistent with the observation\nthat semantic category of a partial object surface is less ambiguous only when\nits distribution on the whole surface is clearly specified. To this end, we\nargue for a single-view, partial setting where supervised learning of object\npose estimation should be accompanied with classification. Technically, we\npropose a baseline method of Pose-Accompanied Point cloud classification\nNetwork (PAPNet); built upon SE(3)-equivariant convolutions, the PAPNet learns\nintermediate pose transformations for equivariant features defined on vector\nfields, which makes the subsequent classification easier (ideally) in the\ncategory-level, canonical pose. By adapting existing ModelNet40 and ScanNet\ndatasets to the single-view, partial setting, experiment results can verify the\nnecessity of object pose estimation and superiority of our PAPNet to existing\nclassifiers.",
      "doi": "arXiv:2012.10042v4",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Advancing Cycling Safety: On-Bike Alert System Utilizing Multi-Layer Radar Point Cloud Clustering for Coarse Object Classification.",
      "authors": [
        "Omri A",
        "Benothman N",
        "Sayahi S",
        "Tlili F",
        "Chaabane F",
        "Besbes H"
      ],
      "abstract": "Cyclists are considered to be vulnerable road users (VRUs) and need protection from potential collisions with cars and other vehicles induced by unsafe driving, dangerous road conditions, or weak cycling infrastructure. Integrating mmWave radars into cycling safety measures presents an efficient solution to this problem given their compact size, low power consumption, and low cost compared to other sensors. This paper introduces an mmWave radar-based bike safety system designed to offer real-time alerts to cyclists. The system consists of a low-power radar sensor affixed to the bicycle, connected to a micro-controller, and delivering a preliminary classification of detected obstacles. An efficient two-level clustering based on the accumulation of radar point clouds from multiple frames with a temporal projection from previous frames into the current frame is proposed. The clustering is followed by a coarse classification algorithm in which we use relevant features extracted from the resulting clusters. An annotated RadBike dataset composed of radar point cloud data synchronized with RGB camera images is developed to evaluate our system. The two-level clustering outperforms the DBSCAN algorithm, achieving a v-measure score of 0.91, compared to 0.88 with classical DBSCAN. Different classifiers, including decision trees, random forests, support vector machines (SVMs), and AdaBoost, have been assessed, with an overall accuracy of 87% for the three main object classes: four-wheeled, two-wheeled, and others. The system has the ability to improve rider safety on the road and substantially reduce the frequency of incidents involving cyclists.",
      "doi": "https://doi.org/10.3390/s24103094",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Point Cloud Classification by Domain Adaptation Using Recycling Max Pooling and Cutting Plane Identification.",
      "authors": [
        "Yoo H",
        "Jun K"
      ],
      "abstract": "Deep models have been studied in point cloud classification for the applications of autonomous driving and robotics. One challenging issue is that the point cloud of the same object could be discrepantly captured depending on sensors. Such a difference is the main cause of the domain gap. The deep models trained with one domain of point clouds may not work well with other domains because of such a domain gap. A technique to reduce domain inconsistency is domain adaptation. In this paper, we propose an unsupervised domain adaptation with two novel schemes. First, to improve unreliable pseudo-label assignment, we introduce a voting-based procedure based on the recycling max pooling module, which involves self-paced learning. It helps to increase the training stability of the models. Second, to learn the geometrical characteristics of point clouds in unfamiliar settings, we propose a training method of cutting plane identification, which works in an unsupervised way. Testing with the popular point cloud dataset of PointDA-10 and Sim-to-Real, experiments show that our method increase classification accuracy by 6.5%-points on average, ModelNet and ShapeNet as the source domain and ScanNet, and ScanObjectNN as the target domain. From an ablation study, it was observed that each method contributes to improving the robustness of domain adaptation.",
      "doi": "https://doi.org/10.3390/s23031177",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Point-to-Pixel Prompting for Point Cloud Analysis With Pre-Trained Image Models.",
      "authors": [
        "Wang Z",
        "Rao Y",
        "Yu X",
        "Zhou J",
        "Lu J"
      ],
      "abstract": "Nowadays, pre-training big models on large-scale datasets has achieved great success and dominated many downstream tasks in natural language processing and 2D vision, while pre-training in 3D vision is still under development. In this paper, we provide a new perspective of transferring the pre-trained knowledge from 2D domain to 3D domain with Point-to-Pixel Prompting in data space and Pixel-to-Point distillation in feature space, exploiting shared knowledge in images and point clouds that display the same visual world. Following the principle of prompting engineering, Point-to-Pixel Prompting transforms point clouds into colorful images with geometry-preserved projection and geometry-aware coloring. Then the pre-trained image models can be directly implemented for point cloud tasks without structural changes or weight modifications. With projection correspondence in feature space, Pixel-to-Point distillation further regards pre-trained image models as the teacher model and distills pre-trained 2D knowledge to student point cloud models, remarkably enhancing inference efficiency and model capacity for point cloud analysis. We conduct extensive experiments in both object classification and scene segmentation under various settings to demonstrate the superiority of our method. In object classification, we reveal the important scale-up trend of Point-to-Pixel Prompting and attain 90.3% accuracy on ScanObjectNN dataset, surpassing previous literature by a large margin. In scene-level semantic segmentation, our method outperforms traditional 3D analysis approaches and shows competitive capacity in dense prediction tasks.",
      "doi": "https://doi.org/10.1109/tpami.2024.3354961",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Variational Relational Point Completion Network for Robust 3D Classification.",
      "authors": [
        "Pan L",
        "Chen X",
        "Cai Z",
        "Zhang J",
        "Zhao H",
        "Yi S",
        "Liu Z"
      ],
      "abstract": "Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise, which hampers 3D geometric modeling and perception. Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details. Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects. To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds. One path consumes complete point clouds for reconstruction by learning a point VAE. The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training. 2) Relational Enhancement. Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion. In addition, we contribute multi-view partial point cloud datasets (MVP and MVP-40 dataset) containing over 200,000 high-quality scans, which render partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model. Extensive experiments demonstrate that VRCNet outperforms state-of-the-art methods on all standard point cloud completion benchmarks. Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans. Moreover, we can achieve robust 3D classification for partial point clouds with the help of VRCNet, which can highly increase classification accuracy.",
      "doi": "https://doi.org/10.1109/tpami.2023.3268305",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "High-Fidelity Point Cloud Completion with Low-Resolution Recovery and\n  Noise-Aware Upsampling",
      "authors": [
        "Ren-Wu Li",
        "Bo Wang",
        "Chun-Peng Li",
        "Ling-Xiao Zhang",
        "Lin Gao"
      ],
      "abstract": "Completing an unordered partial point cloud is a challenging task. Existing\napproaches that rely on decoding a latent feature to recover the complete\nshape, often lead to the completed point cloud being over-smoothing, losing\ndetails, and noisy. Instead of decoding a whole shape, we propose to decode and\nrefine a low-resolution (low-res) point cloud first, and then performs a\npatch-wise noise-aware upsampling rather than interpolating the whole sparse\npoint cloud at once, which tends to lose details. Regarding the possibility of\nlacking details of the initially decoded low-res point cloud, we propose an\niterative refinement to recover the geometric details and a symmetrization\nprocess to preserve the trustworthy information from the input partial point\ncloud. After obtaining a sparse and complete point cloud, we propose a\npatch-wise upsampling strategy. Patch-based upsampling allows to better recover\nfine details unlike decoding a whole shape, however, the existing upsampling\nmethods are not applicable to completion task due to the data discrepancy\n(i.e., input sparse data here is not from ground-truth). Therefore, we propose\na patch extraction approach to generate training patch pairs between the sparse\nand ground-truth point clouds, and an outlier removal step to suppress the\nnoisy points from the sparse point cloud. Together with the low-res recovery,\nour whole method is able to achieve high-fidelity point cloud completion.\nComprehensive evaluations are provided to demonstrate the effectiveness of the\nproposed method and its individual components.",
      "doi": "arXiv:2112.11271v2",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Sparse SPN: Depth Completion from Sparse Keypoints",
      "authors": [
        "Yuqun Wu",
        "Jae Yong Lee",
        "Derek Hoiem"
      ],
      "abstract": "Our long term goal is to use image-based depth completion to quickly create\n3D models from sparse point clouds, e.g. from SfM or SLAM. Much progress has\nbeen made in depth completion. However, most current works assume well\ndistributed samples of known depth, e.g. Lidar or random uniform sampling, and\nperform poorly on uneven samples, such as from keypoints, due to the large\nunsampled regions. To address this problem, we extend CSPN with multiscale\nprediction and a dilated kernel, leading to much better completion of\nkeypoint-sampled depth. We also show that a model trained on NYUv2 creates\nsurprisingly good point clouds on ETH3D by completing sparse SfM points.",
      "doi": "arXiv:2212.00987v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "SPU-IMR: Self-supervised Arbitrary-scale Point Cloud Upsampling via\n  Iterative Mask-recovery Network",
      "authors": [
        "Ziming Nie",
        "Qiao Wu",
        "Chenlei Lv",
        "Siwen Quan",
        "Zhaoshuai Qi",
        "Muze Wang",
        "Jiaqi Yang"
      ],
      "abstract": "Point cloud upsampling aims to generate dense and uniformly distributed point\nsets from sparse point clouds. Existing point cloud upsampling methods\ntypically approach the task as an interpolation problem. They achieve\nupsampling by performing local interpolation between point clouds or in the\nfeature space, then regressing the interpolated points to appropriate\npositions. By contrast, our proposed method treats point cloud upsampling as a\nglobal shape completion problem. Specifically, our method first divides the\npoint cloud into multiple patches. Then, a masking operation is applied to\nremove some patches, leaving visible point cloud patches. Finally, our\ncustom-designed neural network iterative completes the missing sections of the\npoint cloud through the visible parts. During testing, by selecting different\nmask sequences, we can restore various complete patches. A sufficiently dense\nupsampled point cloud can be obtained by merging all the completed patches. We\ndemonstrate the superior performance of our method through both quantitative\nand qualitative experiments, showing overall superiority against both existing\nself-supervised and supervised methods.",
      "doi": "arXiv:2502.19452v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "Temporal Point Cloud Completion with Pose Disturbance",
      "authors": [
        "Jieqi Shi",
        "Lingyun Xu",
        "Peiliang Li",
        "Xiaozhi Chen",
        "Shaojie Shen"
      ],
      "abstract": "Point clouds collected by real-world sensors are always unaligned and sparse,\nwhich makes it hard to reconstruct the complete shape of object from a single\nframe of data. In this work, we manage to provide complete point clouds from\nsparse input with pose disturbance by limited translation and rotation. We also\nuse temporal information to enhance the completion model, refining the output\nwith a sequence of inputs. With the help of gated recovery units(GRU) and\nattention mechanisms as temporal units, we propose a point cloud completion\nframework that accepts a sequence of unaligned and sparse inputs, and outputs\nconsistent and aligned point clouds. Our network performs in an online manner\nand presents a refined point cloud for each frame, which enables it to be\nintegrated into any SLAM or reconstruction pipeline. As far as we know, our\nframework is the first to utilize temporal information and ensure temporal\nconsistency with limited transformation. Through experiments in ShapeNet and\nKITTI, we prove that our framework is effective in both synthetic and\nreal-world datasets.",
      "doi": "arXiv:2202.03084v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Cascaded Refinement Network for Point Cloud Completion With Self-Supervision.",
      "authors": [
        "Wang X",
        "Ang MH",
        "Lee GH"
      ],
      "abstract": "Point clouds are often sparse and incomplete, which imposes difficulties for real-world applications. Existing shape completion methods tend to generate rough shapes without fine-grained details. Considering this, we introduce a two-branch network for shape completion. The first branch is a cascaded shape completion sub-network to synthesize complete objects, where we propose to use the partial input together with the coarse output to preserve the object details during the dense point reconstruction. The second branch is an auto-encoder to reconstruct the original partial input. The two branches share a same feature extractor to learn an accurate global feature for shape completion. Furthermore, we propose two strategies to enable the training of our network when ground truth data are not available. This is to mitigate the dependence of existing approaches on large amounts of ground truth training data that are often difficult to obtain in real-world applications. Additionally, our proposed strategies are also able to improve the reconstruction quality for fully supervised learning. We verify our approach in self-supervised, semi-supervised and fully supervised settings with superior performances. Quantitative and qualitative results on different datasets demonstrate that our method achieves more realistic outputs than state-of-the-art approaches on the point cloud completion task.",
      "doi": "https://doi.org/10.1109/tpami.2021.3108410",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "3D LiDAR Point Cloud Registration Based on IMU Preintegration in Coal Mine Roadways.",
      "authors": [
        "Yang L",
        "Ma H",
        "Nie Z",
        "Zhang H",
        "Wang Z",
        "Wang C"
      ],
      "abstract": "Point cloud registration is the basis of real-time environment perception for robots using 3D LiDAR and is also the key to robust simultaneous localization and mapping (SLAM) for robots. Because LiDAR point clouds are characterized by local sparseness and motion distortion, the point cloud features of coal mine roadway environments show a weak texture and degradation. Therefore, for these environments, the traditional point cloud registration method to register directly will lead to problems, such as a decline in registration accuracy, z-axis drift, and map ghosting. To solve the above problems, we propose a point cloud registration method based on IMU preintegration with the sensor characteristics of LiDAR and IMU. The system framework of this method mainly consists of four modules: IMU preintegration, point cloud preprocessing, point cloud frame matching and point cloud registration. First, IMU sensor data are introduced, and IMU linear interpolation is used to correct the motion distortion in LiDAR scanning, and the IMU preintegration error function is constructed. Second, the point cloud segmentation is performed using the ground segmentation method of RANSAC to provide additional ground constraints for the z-axis displacement and to remove the unstable flawed points from the point cloud. On this basis, the LiDAR point cloud registration error function is constructed by extracting the feature corner points and feature plane points. Finally, the Gaussian Newton solution is used to optimize the constraint relationship between the LiDAR odometry frames to minimize the error function, complete the LiDAR point cloud registration and better estimate the position and pose of the mobile robot. The experimental results show that compared with the traditional point cloud registration method, the proposed method has a higher point cloud registration accuracy, success rate and computational efficiency. The LiDAR odometry constructed using this method can better reflect the authenticity of the robot trajectory and has higher trajectory accuracy and smaller absolute position and pose error.",
      "doi": "https://doi.org/10.3390/s23073473",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Parallel Structure from Motion for Sparse Point Cloud Generation in Large-Scale Scenes.",
      "authors": [
        "Bao Y",
        "Lin P",
        "Li Y",
        "Qi Y",
        "Wang Z",
        "Du W",
        "Fan Q"
      ],
      "abstract": "Scene reconstruction uses images or videos as input to reconstruct a 3D model of a real scene and has important applications in smart cities, surveying and mapping, military, and other fields. Structure from motion (SFM) is a key step in scene reconstruction, which recovers sparse point clouds from image sequences. However, large-scale scenes cannot be reconstructed using a single compute node. Image matching and geometric filtering take up a lot of time in the traditional SFM problem. In this paper, we propose a novel divide-and-conquer framework to solve the distributed SFM problem. First, we use the global navigation satellite system (GNSS) information from images to calculate the GNSS neighborhood. The number of images matched is greatly reduced by matching each image to only valid GNSS neighbors. This way, a robust matching relationship can be obtained. Second, the calculated matching relationship is used as the initial camera graph, which is divided into multiple subgraphs by the clustering algorithm. The local SFM is executed on several computing nodes to register the local cameras. Finally, all of the local camera poses are integrated and optimized to complete the global camera registration. Experiments show that our system can accurately and efficiently solve the structure from motion problem in large-scale scenes.",
      "doi": "https://doi.org/10.3390/s21113939",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Visual and tactile 3D point cloud data from real robots for shape modeling and completion.",
      "authors": [
        "Bekiroglu Y",
        "Björkman M",
        "Zarzar Gandler G",
        "Exner J",
        "Ek CH",
        "Kragic D"
      ],
      "abstract": "Representing 3D geometry for different tasks, e.g. rendering and reconstruction, is an important goal in different fields, such as computer graphics, computer vision and robotics. Robotic applications often require perception of object shape information extracted from sensory data that can be noisy and incomplete. This is a challenging task and in order to facilitate analysis of new methods and comparison of different approaches for shape modeling (e.g. surface estimation), completion and exploration, we provide real sensory data acquired from exploring various objects of different complexities. The dataset includes visual and tactile readings in the form of 3D point clouds obtained using two different robot setups that are equipped with visual and tactile sensors. During data collection, the robots touch the experiment objects in a predefined manner at various exploration configurations and gather visual and tactile points in the same coordinate frame based on calibration between the robots and the used cameras. The goal of this exhaustive exploration procedure is to sense unseen parts of the objects which are not visible to the cameras, but can be sensed via tactile sensors activated at touched areas. The data was used for shape completion and modeling via Implicit Surface representation and Gaussian-Process-based regression, in the work \"Object shape estimation and modeling, based on sparse Gaussian process implicit surfaces, combining visual data and tactile exploration\" [3], and also used partially in \"Enhancing visual perception of shape through tactile glances\" [4], both studying efficient exploration of objects to reduce number of touches.",
      "doi": "https://doi.org/10.1016/j.dib.2020.105335",
      "year": 2020,
      "source": "PubMed"
    },
    {
      "title": "mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body\n  Reconstruction",
      "authors": [
        "Jiarui Yang",
        "Songpengcheng Xia",
        "Zengyuan Lai",
        "Lan Sun",
        "Qi Wu",
        "Wenxian Yu",
        "Ling Pei"
      ],
      "abstract": "Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverse\nenvironments, making it a highly promising solution for human body\nreconstruction due to its privacy-friendly and non-intrusive nature. However,\nthe significant sparsity of mmWave point clouds limits the estimation accuracy.\nTo overcome this challenge, we propose a two-stage deep learning framework that\nenhances mmWave point clouds and improves human body reconstruction accuracy.\nOur method includes a mmWave point cloud enhancement module that densifies the\nraw data by leveraging temporal features and a multi-stage completion network,\nfollowed by a 2D-3D fusion module that extracts both 2D and 3D motion features\nto refine SMPL parameters. The mmWave point cloud enhancement module learns the\ndetailed shape and posture information from 2D human masks in single-view\nimages. However, image-based supervision is involved only during the training\nphase, and the inference relies solely on sparse point clouds to maintain\nprivacy. Experiments on multiple datasets demonstrate that our approach\noutperforms state-of-the-art methods, with the enhanced point clouds further\nimproving performance when integrated into existing models.",
      "doi": "arXiv:2503.02375v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "Multimodal Point Cloud Semantic Segmentation With Virtual Point\n  Enhancement",
      "authors": [
        "Zaipeng Duan",
        "Xuzhong Hu",
        "Pei An",
        "Jie Ma"
      ],
      "abstract": "LiDAR-based 3D point cloud recognition has been proven beneficial in various\napplications. However, the sparsity and varying density pose a significant\nchallenge in capturing intricate details of objects, particularly for\nmedium-range and small targets. Therefore, we propose a multi-modal point cloud\nsemantic segmentation method based on Virtual Point Enhancement (VPE), which\nintegrates virtual points generated from images to address these issues. These\nvirtual points are dense but noisy, and directly incorporating them can\nincrease computational burden and degrade performance. Therefore, we introduce\na spatial difference-driven adaptive filtering module that selectively extracts\nvaluable pseudo points from these virtual points based on density and distance,\nenhancing the density of medium-range targets. Subsequently, we propose a\nnoise-robust sparse feature encoder that incorporates noise-robust feature\nextraction and fine-grained feature enhancement. Noise-robust feature\nextraction exploits the 2D image space to reduce the impact of noisy points,\nwhile fine-grained feature enhancement boosts sparse geometric features through\ninner-voxel neighborhood point aggregation and downsampled voxel aggregation.\nThe results on the SemanticKITTI and nuScenes, two large-scale benchmark data\nsets, have validated effectiveness, significantly improving 2.89\\% mIoU with\nthe introduction of 7.7\\% virtual points on nuScenes.",
      "doi": "arXiv:2504.01449v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "Test-Time Augmentation for 3D Point Cloud Classification and\n  Segmentation",
      "authors": [
        "Tuan-Anh Vu",
        "Srinjay Sarkar",
        "Zhiyuan Zhang",
        "Binh-Son Hua",
        "Sai-Kit Yeung"
      ],
      "abstract": "Data augmentation is a powerful technique to enhance the performance of a\ndeep learning task but has received less attention in 3D deep learning. It is\nwell known that when 3D shapes are sparsely represented with low point density,\nthe performance of the downstream tasks drops significantly. This work explores\ntest-time augmentation (TTA) for 3D point clouds. We are inspired by the recent\nrevolution of learning implicit representation and point cloud upsampling,\nwhich can produce high-quality 3D surface reconstruction and\nproximity-to-surface, respectively. Our idea is to leverage the implicit field\nreconstruction or point cloud upsampling techniques as a systematic way to\naugment point cloud data. Mainly, we test both strategies by sampling points\nfrom the reconstructed results and using the sampled point cloud as test-time\naugmented data. We show that both strategies are effective in improving\naccuracy. We observed that point cloud upsampling for test-time augmentation\ncan lead to more significant performance improvement on downstream tasks such\nas object classification and segmentation on the ModelNet40, ShapeNet,\nScanObjectNN, and SemanticKITTI datasets, especially for sparse point clouds.",
      "doi": "arXiv:2311.13152v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "DenserRadar: A 4D millimeter-wave radar point cloud detector based on\n  dense LiDAR point clouds",
      "authors": [
        "Zeyu Han",
        "Junkai Jiang",
        "Xiaokang Ding",
        "Qingwen Meng",
        "Shaobing Xu",
        "Lei He",
        "Jianqiang Wang"
      ],
      "abstract": "The 4D millimeter-wave (mmWave) radar, with its robustness in extreme\nenvironments, extensive detection range, and capabilities for measuring\nvelocity and elevation, has demonstrated significant potential for enhancing\nthe perception abilities of autonomous driving systems in corner-case\nscenarios. Nevertheless, the inherent sparsity and noise of 4D mmWave radar\npoint clouds restrict its further development and practical application. In\nthis paper, we introduce a novel 4D mmWave radar point cloud detector, which\nleverages high-resolution dense LiDAR point clouds. Our approach constructs\ndense 3D occupancy ground truth from stitched LiDAR point clouds, and employs a\nspecially designed network named DenserRadar. The proposed method surpasses\nexisting probability-based and learning-based radar point cloud detectors in\nterms of both point cloud density and accuracy on the K-Radar dataset.",
      "doi": "arXiv:2405.05131v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Virtualized Point Cloud Rendering.",
      "authors": [
        "Collado JA",
        "Lopez A",
        "Jurado JM",
        "Jimenez JR"
      ],
      "abstract": "Remote sensing technologies, such as LiDAR, produce billions of points that commonly exceed the storage capacity of the GPU, restricting their processing and rendering. Level of detail (LoD) techniques have been widely investigated, but building the LoD structures is also time-consuming. This study proposes a GPU-driven culling system focused on determining the number of points visible in every frame. It can manipulate point clouds of any arbitrary size while maintaining a low memory footprint in both the CPU and GPU. Instead of organizing point clouds into hierarchical data structures, these are split into groups of points sorted using the Hilbert encoding. This alternative alleviates the occurrence of anomalous groups found in Morton curves. Instead of keeping the entire point cloud in the GPU, points are transferred on demand to ensure real-time capability. Accordingly, our solution can manipulate huge point clouds even in commodity hardware with low memory capacities. Moreover, hole filling is implemented to cover the gaps derived from insufficient density and our LoD system. Our proposal was evaluated with point clouds of up to 18 billion points, achieving an average of 80 frames per second (FPS) without perceptible quality loss. Relaxing memory constraints further enhances visual quality while maintaining an interactive frame rate. We assessed our method on real-world data, comparing it against three state-ofthe- art methods, demonstrating its ability to handle significantly larger point clouds. The code is available on https://github.com/Krixtalx/Nimbus.",
      "doi": "https://doi.org/10.1109/tvcg.2025.3562696",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "Research on a 3D Point Cloud Map Learning Algorithm Based on Point Normal Constraints.",
      "authors": [
        "Fang Z",
        "Liu Y",
        "Xu L",
        "Shahed MH",
        "Shi L"
      ],
      "abstract": "Laser point clouds are commonly affected by Gaussian and Laplace noise, resulting in decreased accuracy in subsequent surface reconstruction and visualization processes. However, existing point cloud denoising algorithms often overlook the local consistency and density of the point cloud normal vector. A feature map learning algorithm which integrates point normal constraints, Dirichlet energy, and coupled orthogonality bias terms is proposed. Specifically, the Dirichlet energy is employed to penalize the difference between neighboring normal vectors and combined with a coupled orthogonality bias term to enhance the orthogonality between the normal vectors and the subsurface, thereby enhancing the accuracy and robustness of the learned denoising of the feature maps. Additionally, to mitigate the effect of mixing noise, a point cloud density function is introduced to rapidly capture local feature correlations. In experimental findings on the anchor public dataset, the proposed method reduces the average mean square error (MSE) by 0.005 and 0.054 compared to the MRPCA and NLD algorithms, respectively. Moreover, it improves the average signal-to-noise ratio (SNR) by 0.13 DB and 2.14 DB compared to MRPCA and AWLOP, respectively. The proposed algorithm enhances computational efficiency by 27% compared to the RSLDM method. It not only removes mixed noise but also preserves the local geometric features of the point cloud, further improving computational efficiency.",
      "doi": "https://doi.org/10.3390/s24196185",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "AGO-Net: Association-Guided 3D Point Cloud Object Detection Network.",
      "authors": [
        "Du L",
        "Ye X",
        "Tan X",
        "Johns E",
        "Chen B",
        "Ding E",
        "Xue X",
        "Feng J"
      ],
      "abstract": "The human brain can effortlessly recognize and localize objects, whereas current 3D object detection methods based on LiDAR point clouds still report inferior performance for detecting occluded and distant objects: The point cloud appearance varies greatly due to occlusion, and has inherent variance in point densities along the distance to sensors. Therefore, designing feature representations robust to such point clouds is critical. Inspired by human associative recognition, we propose a novel 3D detection framework that associates intact features for objects via domain adaptation. We bridge the gap between the perceptual domain, where features are derived from real scenes with sub-optimal representations, and the conceptual domain, where features are extracted from augmented scenes that consist of non-occlusion objects with rich detailed information. A feasible method is investigated to construct conceptual scenes without external datasets. We further introduce an attention-based re-weighting module that adaptively strengthens the feature adaptation of more informative regions. The network's feature enhancement ability is exploited without introducing extra cost during inference, which is plug-and-play in various 3D detection frameworks. We achieve new state-of-the-art performance on the KITTI 3D detection benchmark in both accuracy and speed. Experiments on nuScenes and Waymo datasets also validate the versatility of our method.",
      "doi": "https://doi.org/10.1109/tpami.2021.3104172",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "UltraLiDAR: Learning Compact Representations for LiDAR Completion and\n  Generation",
      "authors": [
        "Yuwen Xiong",
        "Wei-Chiu Ma",
        "Jingkang Wang",
        "Raquel Urtasun"
      ],
      "abstract": "LiDAR provides accurate geometric measurements of the 3D world.\nUnfortunately, dense LiDARs are very expensive and the point clouds captured by\nlow-beam LiDAR are often sparse. To address these issues, we present\nUltraLiDAR, a data-driven framework for scene-level LiDAR completion, LiDAR\ngeneration, and LiDAR manipulation. The crux of UltraLiDAR is a compact,\ndiscrete representation that encodes the point cloud's geometric structure, is\nrobust to noise, and is easy to manipulate. We show that by aligning the\nrepresentation of a sparse point cloud to that of a dense point cloud, we can\ndensify the sparse point clouds as if they were captured by a real high-density\nLiDAR, drastically reducing the cost. Furthermore, by learning a prior over the\ndiscrete codebook, we can generate diverse, realistic LiDAR point clouds for\nself-driving. We evaluate the effectiveness of UltraLiDAR on sparse-to-dense\nLiDAR completion and LiDAR generation. Experiments show that densifying\nreal-world point clouds with our approach can significantly improve the\nperformance of downstream perception systems. Compared to prior art on LiDAR\ngeneration, our approach generates much more realistic point clouds. According\nto A/B test, over 98.5\\% of the time human participants prefer our results over\nthose of previous methods.",
      "doi": "arXiv:2311.01448v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "DualGenerator: Information Interaction-based Generative Network for\n  Point Cloud Completion",
      "authors": [
        "Pengcheng Shi",
        "Haozhe Cheng",
        "Xu Han",
        "Yiyang Zhou",
        "Jihua Zhu"
      ],
      "abstract": "Point cloud completion estimates complete shapes from incomplete point clouds\nto obtain higher-quality point cloud data. Most existing methods only consider\nglobal object features, ignoring spatial and semantic information of adjacent\npoints. They cannot distinguish structural information well between different\nobject parts, and the robustness of models is poor. To tackle these challenges,\nwe propose an information interaction-based generative network for point cloud\ncompletion ($\\mathbf{DualGenerator}$). It contains an adversarial generation\npath and a variational generation path, which interact with each other and\nshare weights. DualGenerator introduces a local refinement module in generation\npaths, which captures general structures from partial inputs, and then refines\nshape details of the point cloud. It promotes completion in the unknown region\nand makes a distinction between different parts more obvious. Moreover, we\ndesign DGStyleGAN to improve the generation quality further. It promotes the\nrobustness of this network combined with fusion analysis of dual-path\ncompletion results. Qualitative and quantitative evaluations demonstrate that\nour method is superior on MVP and Completion3D datasets. The performance will\nnot degrade significantly after adding noise interference or sparse sampling.",
      "doi": "arXiv:2305.09132v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning\n  Contextual Shape Priors from Scene Completion",
      "authors": [
        "Xu Yan",
        "Jiantao Gao",
        "Jie Li",
        "Ruimao Zhang",
        "Zhen Li",
        "Rui Huang",
        "Shuguang Cui"
      ],
      "abstract": "LiDAR point cloud analysis is a core task for 3D computer vision, especially\nfor autonomous driving. However, due to the severe sparsity and noise\ninterference in the single sweep LiDAR point cloud, the accurate semantic\nsegmentation is non-trivial to achieve. In this paper, we propose a novel\nsparse LiDAR point cloud semantic segmentation framework assisted by learned\ncontextual shape priors. In practice, an initial semantic segmentation (SS) of\na single sweep point cloud can be achieved by any appealing network and then\nflows into the semantic scene completion (SSC) module as the input. By merging\nmultiple frames in the LiDAR sequence as supervision, the optimized SSC module\nhas learned the contextual shape priors from sequential LiDAR data, completing\nthe sparse single sweep point cloud to the dense one. Thus, it inherently\nimproves SS optimization through fully end-to-end training. Besides, a\nPoint-Voxel Interaction (PVI) module is proposed to further enhance the\nknowledge fusion between SS and SSC tasks, i.e., promoting the interaction of\nincomplete local geometry of point cloud and complete voxel-wise global\nstructure. Furthermore, the auxiliary SSC and PVI modules can be discarded\nduring inference without extra burden for SS. Extensive experiments confirm\nthat our JS3C-Net achieves superior performance on both SemanticKITTI and\nSemanticPOSS benchmarks, i.e., 4% and 3% improvement correspondingly.",
      "doi": "arXiv:2012.03762v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "3D Point Cloud Generation via Autoregressive Up-sampling",
      "authors": [
        "Ziqiao Meng",
        "Qichao Wang",
        "Zhipeng Zhou",
        "Irwin King",
        "Peilin Zhao"
      ],
      "abstract": "We introduce a pioneering autoregressive generative model for 3D point cloud\ngeneration. Inspired by visual autoregressive modeling (VAR), we conceptualize\npoint cloud generation as an autoregressive up-sampling process. This leads to\nour novel model, PointARU, which progressively refines 3D point clouds from\ncoarse to fine scales. PointARU follows a two-stage training paradigm: first,\nit learns multi-scale discrete representations of point clouds, and then it\ntrains an autoregressive transformer for next-scale prediction. To address the\ninherent unordered and irregular structure of point clouds, we incorporate\nspecialized point-based up-sampling network modules in both stages and\nintegrate 3D absolute positional encoding based on the decoded point cloud at\neach scale during the second stage. Our model surpasses state-of-the-art (SoTA)\ndiffusion-based approaches in both generation quality and parameter efficiency\nacross diverse experimental settings, marking a new milestone for\nautoregressive methods in 3D point cloud generation. Furthermore, PointARU\ndemonstrates exceptional performance in completing partial 3D shapes and\nup-sampling sparse point clouds, outperforming existing generative models in\nthese tasks.",
      "doi": "arXiv:2503.08594v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "MPED: Quantifying Point Cloud Distortion Based on Multiscale Potential Energy Discrepancy.",
      "authors": [
        "Yang Q",
        "Zhang Y",
        "Chen S",
        "Xu Y",
        "Sun J",
        "Ma Z"
      ],
      "abstract": "In this article, we propose a new distortion quantification method for point clouds, the multiscale potential energy discrepancy (MPED). Currently, there is a lack of effective distortion quantification for a variety of point cloud perception tasks. Specifically, in human vision tasks, a distortion quantification method is used to predict human subjective scores and optimize the selection of human perception task parameters, such as dense point cloud compression and enhancement. In machine vision tasks, a distortion quantification method usually serves as loss function to guide the training of deep neural networks for unsupervised learning tasks (e.g., sparse point cloud reconstruction, completion, and upsampling). Therefore, an effective distortion quantification should be differentiable, distortion discriminable, and have low computational complexity. However, current distortion quantification cannot satisfy all three conditions. To fill this gap, we propose a new point cloud feature description method, the point potential energy (PPE), inspired by classical physics. We regard the point clouds are systems that have potential energy and the distortion can change the total potential energy. By evaluating various neighborhood sizes, the proposed MPED achieves global-local tradeoffs, capturing distortion in a multiscale fashion. We further theoretically show that classical Chamfer distance is a special case of our MPED. Extensive experiments show that the proposed MPED is superior to current methods on both human and machine perception tasks. Our code is available at https://github.com/Qi-Yangsjtu/MPED.",
      "doi": "https://doi.org/10.1109/tpami.2022.3213831",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation.",
      "authors": [
        "Liu H",
        "Liao K",
        "Lin C",
        "Zhao Y",
        "Liu M"
      ],
      "abstract": "LiDAR sensors can provide dependable 3D spatial information at a low frequency (around 10 Hz) and have been widely applied in the field of autonomous driving and unmanned aerial vehicle (UAV). However, the camera with a higher frequency (around 20 Hz) has to be decreased so as to match with LiDAR in a multi-sensor system. In this paper, we propose a novel Pseudo-LiDAR interpolation network (PLIN) to increase the frequency of LiDAR sensor data. PLIN can generate temporally and spatially high-quality point cloud sequences to match the high frequency of cameras. To achieve this goal, we design a coarse interpolation stage guided by consecutive sparse depth maps and motion relationship. We also propose a refined interpolation stage guided by the realistic scene. Using this coarse-to-fine cascade structure, our method can progressively perceive multi-modal information and generate accurate intermediate point clouds. To the best of our knowledge, this is the first deep framework for Pseudo-LiDAR point cloud interpolation, which shows appealing applications in navigation systems equipped with LiDAR and cameras. Experimental results demonstrate that PLIN achieves promising performance on the KITTI dataset, significantly outperforming the traditional interpolation method and the state-of-the-art video interpolation technique.",
      "doi": "https://doi.org/10.3390/s20061573",
      "year": 2020,
      "source": "PubMed"
    },
    {
      "title": "TUCNet: A channel and spatial attention-based graph convolutional network for teeth upsampling and completion.",
      "authors": [
        "Liu M",
        "Li X",
        "Liu J",
        "Liu W",
        "Yu Z"
      ],
      "abstract": "With the increasing popularity of the use of 3D scanning equipment in capturing oral cavity in dental health applications, the quality of 3D dental models has become vital in oral prosthodontics and orthodontics. However, the point cloud data obtained can often be sparse and thus missing information. To address this issue, we construct a high-resolution teeth point cloud completion method named TUCNet to fill up the sparse and incomplete oral point cloud collected and output a dense and complete teeth point cloud. First, we propose a Channel and Spatial Attentive EdgeConv (CSAE) module to fuse local and global contexts in the point feature extraction. Second, we propose a CSAE-based point cloud upsample (CPCU) module to gradually increase the number of points in the point clouds. TUCNet employs a tree-based approach to generate complete point clouds, where child points are derived through a splitting process from parent points following each CPCU. The CPCU learns the up-sampling pattern of each parent point by combining the attention mechanism and the point deconvolution operation. Skip connections are introduced between CPCUs to summarize the split mode of the previous layer of CPCUs, which is used to generate the split mode of the current CPCUs. We conduct numerous experiments on the teeth point cloud completion dataset and the PCN dataset. The experimental results show that our TUCNet not only achieves the state-of-the-art performance on the teeth dataset, but also achieves excellent performance on the PCN dataset.",
      "doi": "https://doi.org/10.1016/j.compbiomed.2023.107519",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "The Circle Pure Rolling Method for Point Cloud Boundary Extraction.",
      "authors": [
        "Yang Q",
        "Li Z",
        "Liu Z",
        "Jiang X",
        "Gao X"
      ],
      "abstract": "We introduce a circle rolling method (CRM) for boundary extraction from 2D point clouds. The core idea is to create a circle that performs pure rolling on the perimeter of the point cloud to obtain the boundary. For a 3D point cloud, a plane adsorbs points on both sides to create a 2D point cloud, and the CRM is used to extract the boundary points and map them back into space to obtain 3D boundary points. Continuously moving this plane can obtain a complete boundary, which is called the moving adsorption rolling method (MARM). In this paper, we solve the interference problems in our method caused by unidirectional overlapping points and porous structures and successfully validate the solutions in practical examples. Our point cloud boundary extraction method is faster in 2D and better for surface concavities extracted in 3D compared to existing methods, and it is unaffected by sparse points within the point cloud.",
      "doi": "https://doi.org/10.3390/s25010045",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse\n  Tensor-based Transformer",
      "authors": [
        "Xiao Huo",
        "Junhui Hou",
        "Shuai Wan",
        "Fuzheng Yang"
      ],
      "abstract": "The evolution of 3D visualization techniques has fundamentally transformed\nhow we interact with digital content. At the forefront of this change is point\ncloud technology, offering an immersive experience that surpasses traditional\n2D representations. However, the massive data size of point clouds presents\nsignificant challenges in data compression. Current methods for lossy point\ncloud attribute compression (PCAC) generally focus on reconstructing the\noriginal point clouds with minimal error. However, for point cloud\nvisualization scenarios, the reconstructed point clouds with distortion still\nneed to undergo a complex rendering process, which affects the final\nuser-perceived quality. In this paper, we propose an end-to-end deep learning\nframework that seamlessly integrates PCAC with differentiable rendering,\ndenoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of\nrendered multiview images for viewing. In a differentiable manner, the impact\nof the rendering process on the reconstructed point clouds is taken into\naccount. Moreover, we characterize point clouds as sparse tensors and propose a\nsparse tensor-based transformer, called SP-Trans. By aligning with the local\ndensity of the point cloud and utilizing an enhanced local attention mechanism,\nSP-Trans captures the intricate relationships within the point cloud, further\nimproving feature analysis and synthesis within the framework. Extensive\nexperiments demonstrate that the proposed RO-PCAC achieves state-of-the-art\ncompression performance, compared to existing reconstruction-oriented methods,\nincluding traditional, learning-based, and hybrid methods.",
      "doi": "arXiv:2411.07899v3",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "PillarGen: Enhancing Radar Point Cloud Density and Quality via\n  Pillar-based Point Generation Network",
      "authors": [
        "Jisong Kim",
        "Geonho Bang",
        "Kwangjin Choi",
        "Minjae Seong",
        "Jaechang Yoo",
        "Eunjong Pyo",
        "Jun Won Choi"
      ],
      "abstract": "In this paper, we present a novel point generation model, referred to as\nPillar-based Point Generation Network (PillarGen), which facilitates the\ntransformation of point clouds from one domain into another. PillarGen can\nproduce synthetic point clouds with enhanced density and quality based on the\nprovided input point clouds. The PillarGen model performs the following three\nsteps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar\nto Point Generation (PPG). The input point clouds are encoded using a pillar\ngrid structure to generate pillar features. Then, OPP determines the active\npillars used for point generation and predicts the center of points and the\nnumber of points to be generated for each active pillar. PPG generates the\nsynthetic points for each active pillar based on the information provided by\nOPP. We evaluate the performance of PillarGen using our proprietary radar\ndataset, focusing on enhancing the density and quality of short-range radar\ndata using the long-range radar data as supervision. Our experiments\ndemonstrate that PillarGen outperforms traditional point upsampling methods in\nquantitative and qualitative measures. We also confirm that when PillarGen is\nincorporated into bird's eye view object detection, a significant improvement\nin detection accuracy is achieved.",
      "doi": "arXiv:2403.01663v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "LPRnet: A self-supervised registration network for LiDAR and\n  photogrammetric point clouds",
      "authors": [
        "Chen Wang",
        "Yanfeng Gu",
        "Xian Li"
      ],
      "abstract": "LiDAR and photogrammetry are active and passive remote sensing techniques for\npoint cloud acquisition, respectively, offering complementary advantages and\nheterogeneous. Due to the fundamental differences in sensing mechanisms,\nspatial distributions and coordinate systems, their point clouds exhibit\nsignificant discrepancies in density, precision, noise, and overlap. Coupled\nwith the lack of ground truth for large-scale scenes, integrating the\nheterogeneous point clouds is a highly challenging task. This paper proposes a\nself-supervised registration network based on a masked autoencoder, focusing on\nheterogeneous LiDAR and photogrammetric point clouds. At its core, the method\nintroduces a multi-scale masked training strategy to extract robust features\nfrom heterogeneous point clouds under self-supervision. To further enhance\nregistration performance, a rotation-translation embedding module is designed\nto effectively capture the key features essential for accurate rigid\ntransformations. Building upon the robust representations, a transformer-based\narchitecture seamlessly integrates local and global features, fostering precise\nalignment across diverse point cloud datasets. The proposed method demonstrates\nstrong feature extraction capabilities for both LiDAR and photogrammetric point\nclouds, addressing the challenges of acquiring ground truth at the scene level.\nExperiments conducted on two real-world datasets validate the effectiveness of\nthe proposed method in solving heterogeneous point cloud registration problems.",
      "doi": "arXiv:2501.05669v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "Enhancing Sampling Protocol for Point Cloud Classification Against\n  Corruptions",
      "authors": [
        "Chongshou Li",
        "Pin Tang",
        "Xinke Li",
        "Yuheng Liu",
        "Tianrui Li"
      ],
      "abstract": "Established sampling protocols for 3D point cloud learning, such as Farthest\nPoint Sampling (FPS) and Fixed Sample Size (FSS), have long been relied upon.\nHowever, real-world data often suffer from corruptions, such as sensor noise,\nwhich violates the benign data assumption in current protocols. As a result,\nthese protocols are highly vulnerable to noise, posing significant safety risks\nin critical applications like autonomous driving. To address these issues, we\npropose an enhanced point cloud sampling protocol, PointSP, designed to improve\nrobustness against point cloud corruptions. PointSP incorporates key point\nreweighting to mitigate outlier sensitivity and ensure the selection of\nrepresentative points. It also introduces a local-global balanced downsampling\nstrategy, which allows for scalable and adaptive sampling while maintaining\ngeometric consistency. Additionally, a lightweight tangent plane interpolation\nmethod is used to preserve local geometry while enhancing the density of the\npoint cloud. Unlike learning-based approaches that require additional model\ntraining, PointSP is architecture-agnostic, requiring no extra learning or\nmodification to the network. This enables seamless integration into existing\npipelines. Extensive experiments on synthetic and real-world corrupted datasets\nshow that PointSP significantly improves the robustness and accuracy of point\ncloud classification, outperforming state-of-the-art methods across multiple\nbenchmarks.",
      "doi": "arXiv:2408.12062v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "LWR-Net: Robust and Lightweight Place Recognition Network for Noisy and Low-Density Point Clouds.",
      "authors": [
        "Zhang Z",
        "Chen G",
        "Shu M",
        "Wang X"
      ],
      "abstract": "Point cloud-based retrieval for place recognition is essential in robotic applications like autonomous driving or simultaneous localization and mapping. However, this remains challenging in complex real-world scenes. Existing methods are sensitive to noisy, low-density point clouds and require extensive storage and computation, posing limitations for hardware-limited scenarios. To overcome these challenges, we propose LWR-Net, a lightweight place recognition network for efficient and robust point cloud retrieval in noisy, low-density conditions. Our approach incorporates a fast dilated sampling and grouping module with a residual MLP structure to learn geometric features from local neighborhoods. We also introduce a lightweight attentional weighting module to enhance global feature representation. By utilizing the Generalized Mean pooling structure, we aggregated the global descriptor for point cloud retrieval. We validated LWR-Net's efficiency and robustness on the Oxford robotcar dataset and three in-house datasets. The results demonstrate that our method efficiently and accurately retrieves matching scenes while being more robust to variations in point density and noise intensity. LWR-Net achieves state-of-the-art accuracy and robustness with a lightweight model size of 0.4M parameters. These efficiency, robustness, and lightweight advantages make our network highly suitable for robotic applications relying on point cloud-based place recognition.",
      "doi": "https://doi.org/10.3390/s23218664",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "PointRas: Uncertainty-Aware Multi-Resolution Learning for Point Cloud Segmentation.",
      "authors": [
        "Zheng Y",
        "Xu X",
        "Zhou J",
        "Lu J"
      ],
      "abstract": "In this paper, we propose an uncertainty-aware multi-resolution learning for point cloud segmentation, named PointRas. Most existing works for point cloud segmentation design encoder networks to obtain better representation of local space in point cloud. However, few of them investigate the utilization of features in the lower resolutions produced by encoders and consider the contextual learning between various resolutions in decoder network. To address this, we propose to utilize the descriptive characteristic of point clouds in the lower resolutions. Taking reference to core steps of rasterization in 2D graphics where the properties of pixels in high density are interpolated from a few primitive shapes in rasterization rendering, we use the similar strategy where prediction maps in lower resolution are iteratively regressed and upsampled into higher resolutions. Moreover, to remedy the potential information deficiency of lower-resolution point cloud, we refine the predictions in each resolution under the criterion of uncertainty selection, which notably enhances the representation ability of the point cloud in lower resolutions. Our proposed PointRas module can be incorporated into the backbones of various point cloud segmentation frameworks, and brings only marginal computational cost. We evaluate the proposed method on challenging datasets including ScanNet, S3DIS, NPM3D, STPLS3D and ScanObjectNN, and consistently improve the performance in comparison with the state-of-the-art methods.",
      "doi": "https://doi.org/10.1109/tip.2022.3205208",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Deformation depth decoupling network for point cloud domain adaptation.",
      "authors": [
        "Zhang H",
        "Ning X",
        "Wang C",
        "Ning E",
        "Li L"
      ],
      "abstract": "Recently, point cloud domain adaptation (DA) practices have been implemented to improve the generalization ability of deep learning models on point cloud data. However, variations across domains often result in decreased performance of models trained on different distributed data sources. Previous studies have focused on output-level domain alignment to address this challenge. But this approach may increase the amount of errors experienced when aligning different domains, particularly for targets that would otherwise be predicted incorrectly. Therefore, in this study, we propose an input-level discretization-based matching to enhance the generalization ability of DA. Specifically, an efficient geometric deformation depth decoupling network (3DeNet) is implemented to learn the knowledge from the source domain and embed it into an implicit feature space, which facilitates the effective constraint of unsupervised predictions for downstream tasks. Secondly, we demonstrate that the sparsity within the implicit feature space varies between domains, rendering domain differences difficult to support. Consequently, we match sets of neighboring points with different densities and biases by differentiating the adaptive densities. Finally, inter-domain differences are aligned by constraining the loss originating from and between the target domains. We conduct experiments on point cloud DA datasets PointDA-10 and PointSegDA, achieving advanced results (over 1.2% and 1% on average).",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106626",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Complete & Label: A Domain Adaptation Approach to Semantic Segmentation\n  of LiDAR Point Clouds",
      "authors": [
        "Li Yi",
        "Boqing Gong",
        "Thomas Funkhouser"
      ],
      "abstract": "We study an unsupervised domain adaptation problem for the semantic labeling\nof 3D point clouds, with a particular focus on domain discrepancies induced by\ndifferent LiDAR sensors. Based on the observation that sparse 3D point clouds\nare sampled from 3D surfaces, we take a Complete and Label approach to recover\nthe underlying surfaces before passing them to a segmentation network.\nSpecifically, we design a Sparse Voxel Completion Network (SVCN) to complete\nthe 3D surfaces of a sparse point cloud. Unlike semantic labels, to obtain\ntraining pairs for SVCN requires no manual labeling. We also introduce local\nadversarial learning to model the surface prior. The recovered 3D surfaces\nserve as a canonical domain, from which semantic labels can transfer across\ndifferent LiDAR sensors. Experiments and ablation studies with our new\nbenchmark for cross-domain semantic labeling of LiDAR data show that the\nproposed approach provides 8.2-36.6% better performance than previous domain\nadaptation methods.",
      "doi": "arXiv:2007.08488v2",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable\n  Rendering",
      "authors": [
        "Yifan Zhao",
        "Le Hui",
        "Jin Xie"
      ],
      "abstract": "Point clouds obtained from 3D sensors are usually sparse. Existing methods\nmainly focus on upsampling sparse point clouds in a supervised manner by using\ndense ground truth point clouds. In this paper, we propose a self-supervised\npoint cloud upsampling network (SSPU-Net) to generate dense point clouds\nwithout using ground truth. To achieve this, we exploit the consistency between\nthe input sparse point cloud and generated dense point cloud for the shapes and\nrendered images. Specifically, we first propose a neighbor expansion unit (NEU)\nto upsample the sparse point clouds, where the local geometric structures of\nthe sparse point clouds are exploited to learn weights for point interpolation.\nThen, we develop a differentiable point cloud rendering unit (DRU) as an\nend-to-end module in our network to render the point cloud into multi-view\nimages. Finally, we formulate a shape-consistent loss and an image-consistent\nloss to train the network so that the shapes of the sparse and dense point\nclouds are as consistent as possible. Extensive results on the CAD and scanned\ndatasets demonstrate that our method can achieve impressive results in a\nself-supervised manner. Code is available at\nhttps://github.com/fpthink/SSPU-Net.",
      "doi": "arXiv:2108.00454v2",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Efficient and Scalable Point Cloud Generation with Sparse Point-Voxel\n  Diffusion Models",
      "authors": [
        "Ioannis Romanelis",
        "Vlassios Fotis",
        "Athanasios Kalogeras",
        "Christos Alexakos",
        "Konstantinos Moustakas",
        "Adrian Munteanu"
      ],
      "abstract": "We propose a novel point cloud U-Net diffusion architecture for 3D generative\nmodeling capable of generating high-quality and diverse 3D shapes while\nmaintaining fast generation times. Our network employs a dual-branch\narchitecture, combining the high-resolution representations of points with the\ncomputational efficiency of sparse voxels. Our fastest variant outperforms all\nnon-diffusion generative approaches on unconditional shape generation, the most\npopular benchmark for evaluating point cloud generative models, while our\nlargest model achieves state-of-the-art results among diffusion methods, with a\nruntime approximately 70% of the previously state-of-the-art PVD. Beyond\nunconditional generation, we perform extensive evaluations, including\nconditional generation on all categories of ShapeNet, demonstrating the\nscalability of our model to larger datasets, and implicit generation which\nallows our network to produce high quality point clouds on fewer timesteps,\nfurther decreasing the generation time. Finally, we evaluate the architecture's\nperformance in point cloud completion and super-resolution. Our model excels in\nall tasks, establishing it as a state-of-the-art diffusion U-Net for point\ncloud generative modeling. The code is publicly available at\nhttps://github.com/JohnRomanelis/SPVD.git.",
      "doi": "arXiv:2408.06145v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Pix2Point: Learning Outdoor 3D Using Sparse Point Clouds and Optimal\n  Transport",
      "authors": [
        "Rémy Leroy",
        "Pauline Trouvé-Peloux",
        "Frédéric Champagnat",
        "Bertrand Le Saux",
        "Marcela Carvalho"
      ],
      "abstract": "Good quality reconstruction and comprehension of a scene rely on 3D\nestimation methods. The 3D information was usually obtained from images by\nstereo-photogrammetry, but deep learning has recently provided us with\nexcellent results for monocular depth estimation. Building up a sufficiently\nlarge and rich training dataset to achieve these results requires onerous\nprocessing. In this paper, we address the problem of learning outdoor 3D point\ncloud from monocular data using a sparse ground-truth dataset. We propose\nPix2Point, a deep learning-based approach for monocular 3D point cloud\nprediction, able to deal with complete and challenging outdoor scenes. Our\nmethod relies on a 2D-3D hybrid neural network architecture, and a supervised\nend-to-end minimisation of an optimal transport divergence between point\nclouds. We show that, when trained on sparse point clouds, our simple promising\napproach achieves a better coverage of 3D outdoor scenes than efficient\nmonocular depth methods.",
      "doi": "arXiv:2107.14498v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Toward a more accurate 3D atlas of C. elegans neurons.",
      "authors": [
        "Skuhersky M",
        "Wu T",
        "Yemini E",
        "Nejatbakhsh A",
        "Boyden E",
        "Tegmark M"
      ],
      "abstract": "Determining cell identity in volumetric images of tagged neuronal nuclei is an ongoing challenge in contemporary neuroscience. Frequently, cell identity is determined by aligning and matching tags to an \"atlas\" of labeled neuronal positions and other identifying characteristics. Previous analyses of such C. elegans datasets have been hampered by the limited accuracy of such atlases, especially for neurons present in the ventral nerve cord, and also by time-consuming manual elements of the alignment process. We present a novel automated alignment method for sparse and incomplete point clouds of the sort resulting from typical C. elegans fluorescence microscopy datasets. This method involves a tunable learning parameter and a kernel that enforces biologically realistic deformation. We also present a pipeline for creating alignment atlases from datasets of the recently developed NeuroPAL transgene. In combination, these advances allow us to label neurons in volumetric images with confidence much higher than previous methods. We release, to the best of our knowledge, the most complete full-body C. elegans 3D positional neuron atlas, incorporating positional variability derived from at least 7 animals per neuron, for the purposes of cell-type identity prediction for myriad applications (e.g., imaging neuronal activity, gene expression, and cell-fate).",
      "doi": "https://doi.org/10.1186/s12859-022-04738-3",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Egoshots, an ego-vision life-logging dataset and semantic fidelity\n  metric to evaluate diversity in image captioning models",
      "authors": [
        "Pranav Agarwal",
        "Alejandro Betancourt",
        "Vana Panagiotou",
        "Natalia Díaz-Rodríguez"
      ],
      "abstract": "Image captioning models have been able to generate grammatically correct and\nhuman understandable sentences. However most of the captions convey limited\ninformation as the model used is trained on datasets that do not caption all\npossible objects existing in everyday life. Due to this lack of prior\ninformation most of the captions are biased to only a few objects present in\nthe scene, hence limiting their usage in daily life. In this paper, we attempt\nto show the biased nature of the currently existing image captioning models and\npresent a new image captioning dataset, Egoshots, consisting of 978 real life\nimages with no captions. We further exploit the state of the art pre-trained\nimage captioning and object recognition networks to annotate our images and\nshow the limitations of existing works. Furthermore, in order to evaluate the\nquality of the generated captions, we propose a new image captioning metric,\nobject based Semantic Fidelity (SF). Existing image captioning metrics can\nevaluate a caption only in the presence of their corresponding annotations;\nhowever, SF allows evaluating captions generated for images without\nannotations, making it highly useful for real life generated captions.",
      "doi": "arXiv:2003.11743v2",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Exact Adversarial Attack to Image Captioning via Structured Output\n  Learning with Latent Variables",
      "authors": [
        "Yan Xu",
        "Baoyuan Wu",
        "Fumin Shen",
        "Yanbo Fan",
        "Yong Zhang",
        "Heng Tao Shen",
        "Wei Liu"
      ],
      "abstract": "In this work, we study the robustness of a CNN+RNN based image captioning\nsystem being subjected to adversarial noises. We propose to fool an image\ncaptioning system to generate some targeted partial captions for an image\npolluted by adversarial noises, even the targeted captions are totally\nirrelevant to the image content. A partial caption indicates that the words at\nsome locations in this caption are observed, while words at other locations are\nnot restricted.It is the first work to study exact adversarial attacks of\ntargeted partial captions. Due to the sequential dependencies among words in a\ncaption, we formulate the generation of adversarial noises for targeted partial\ncaptions as a structured output learning problem with latent variables. Both\nthe generalized expectation maximization algorithm and structural SVMs with\nlatent variables are then adopted to optimize the problem. The proposed methods\ngenerate very successful at-tacks to three popular CNN+RNN based image\ncaptioning models. Furthermore, the proposed attack methods are used to\nunderstand the inner mechanism of image captioning systems, providing the\nguidance to further improve automatic image captioning systems towards human\ncaptioning.",
      "doi": "arXiv:1905.04016v1",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Predicting Winning Captions for Weekly New Yorker Comics",
      "authors": [
        "Stanley Cao",
        "Sonny Young"
      ],
      "abstract": "Image captioning using Vision Transformers (ViTs) represents a pivotal\nconvergence of computer vision and natural language processing, offering the\npotential to enhance user experiences, improve accessibility, and provide\ntextual representations of visual data. This paper explores the application of\nimage captioning techniques to New Yorker cartoons, aiming to generate captions\nthat emulate the wit and humor of winning entries in the New Yorker Cartoon\nCaption Contest. This task necessitates sophisticated visual and linguistic\nprocessing, along with an understanding of cultural nuances and humor. We\npropose several new baselines for using vision transformer encoder-decoder\nmodels to generate captions for the New Yorker cartoon caption contest.",
      "doi": "arXiv:2407.18949v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Towards Automatic Satellite Images Captions Generation Using Large\n  Language Models",
      "authors": [
        "Yingxu He",
        "Qiqi Sun"
      ],
      "abstract": "Automatic image captioning is a promising technique for conveying visual\ninformation using natural language. It can benefit various tasks in satellite\nremote sensing, such as environmental monitoring, resource management, disaster\nmanagement, etc. However, one of the main challenges in this domain is the lack\nof large-scale image-caption datasets, as they require a lot of human expertise\nand effort to create. Recent research on large language models (LLMs) has\ndemonstrated their impressive performance in natural language understanding and\ngeneration tasks. Nonetheless, most of them cannot handle images (GPT-3.5,\nFalcon, Claude, etc.), while conventional captioning models pre-trained on\ngeneral ground-view images often fail to produce detailed and accurate captions\nfor aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we\npropose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to\nautomatically collect captions for remote sensing images by guiding LLMs to\ndescribe their object annotations. We also present a benchmark model that\nadapts the pre-trained generative image2text model (GIT) to generate\nhigh-quality captions for remote-sensing images. Our evaluation demonstrates\nthe effectiveness of our approach for collecting captions for remote sensing\nimages.",
      "doi": "arXiv:2310.11392v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Mitigating Harms of Social Media for Adolescent Body Image and Eating Disorders: A Review.",
      "authors": [
        "Mazzeo SE",
        "Weinstock M",
        "Vashro TN",
        "Henning T",
        "Derrigo K"
      ],
      "abstract": "Social media has negative effects on adolescent body image and disordered eating behaviors, yet adolescents are unlikely to discontinue engaging with these platforms. Thus, it is important to identify strategies that can reduce the harms of social media on adolescent mental health. This article reviews research on social media and adolescent body image, and discusses strategies to reduce risks associated with social media use. Topics covered include interventions aimed at mitigating social media's negative impacts, the body-positivity movement, and policies regulating adolescents' social media use. Overall, this review highlights specific factors (such as staffing, duration, modality, facilitator training, and cultural sensitivity) to consider when designing and implementing social media interventions targeting adolescents. This review also discusses psychosocial outcomes associated with body positivity on social media. Finally, policy efforts to reduce the negative impact of social media on adolescents' body image and eating behaviors are described. In sum, there is a strong need to conduct further research identifying optimal approaches to reduce the harms of social media for adolescent body image and eating behavior.",
      "doi": "https://doi.org/10.2147/prbm.s410600",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "An Overview of Image Caption Generation Methods.",
      "authors": [
        "Wang H",
        "Zhang Y",
        "Yu X"
      ],
      "abstract": "In recent years, with the rapid development of artificial intelligence, image caption has gradually attracted the attention of many researchers in the field of artificial intelligence and has become an interesting and arduous task. Image caption, automatically generating natural language descriptions according to the content observed in an image, is an important part of scene understanding, which combines the knowledge of computer vision and natural language processing. The application of image caption is extensive and significant, for example, the realization of human-computer interaction. This paper summarizes the related methods and focuses on the attention mechanism, which plays an important role in computer vision and is recently widely used in image caption generation tasks. Furthermore, the advantages and the shortcomings of these methods are discussed, providing the commonly used datasets and evaluation criteria in this field. Finally, this paper highlights some open challenges in the image caption task.",
      "doi": "https://doi.org/10.1155/2020/3062706",
      "year": 2020,
      "source": "PubMed"
    },
    {
      "title": "Understanding image-text relations and news values for multimodal news analysis.",
      "authors": [
        "Cheema GS",
        "Hakimov S",
        "Müller-Budack E",
        "Otto C",
        "Bateman JA",
        "Ewerth R"
      ],
      "abstract": "The analysis of news dissemination is of utmost importance since the credibility of information and the identification of disinformation and misinformation affect society as a whole. Given the large amounts of news data published daily on the Web, the empirical analysis of news with regard to research questions and the detection of problematic news content on the Web require computational methods that work at scale. Today's online news are typically disseminated in a multimodal form, including various presentation modalities such as text, image, audio, and video. Recent developments in multimodal machine learning now make it possible to capture basic \"descriptive\" relations between modalities-such as correspondences between words and phrases, on the one hand, and corresponding visual depictions of the verbally expressed information on the other. Although such advances have enabled tremendous progress in tasks like image captioning, text-to-image generation and visual question answering, in domains such as news dissemination, there is a need to go further. In this paper, we introduce a novel framework for the computational analysis of multimodal news. We motivate a set of more complex image-text relations as well as multimodal news values based on real examples of news reports and consider their realization by computational approaches. To this end, we provide (a) an overview of existing literature from",
      "doi": "https://doi.org/10.3389/frai.2023.1125533",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Explicit Image Caption Reasoning: Generating Accurate and Informative Captions for Complex Scenes with LMM.",
      "authors": [
        "Cui M",
        "Li C",
        "Yang Y"
      ],
      "abstract": "The rapid advancement of sensor technologies and deep learning has significantly advanced the field of image captioning, especially for complex scenes. Traditional image captioning methods are often unable to handle the intricacies and detailed relationships within complex scenes. To overcome these limitations, this paper introduces Explicit Image Caption Reasoning (ECR), a novel approach that generates accurate and informative captions for complex scenes captured by advanced sensors. ECR employs an enhanced inference chain to analyze sensor-derived images, examining object relationships and interactions to achieve deeper semantic understanding. We implement ECR using the optimized ICICD dataset, a subset of the sensor-oriented Flickr30K-EE dataset containing comprehensive inference chain information. This dataset enhances training efficiency and caption quality by leveraging rich sensor data. We create the Explicit Image Caption Reasoning Multimodal Model (ECRMM) by fine-tuning TinyLLaVA with the ICICD dataset. Experiments demonstrate ECR's effectiveness and robustness in processing sensor data, outperforming traditional methods.",
      "doi": "https://doi.org/10.3390/s24123820",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Understanding Image Caption Algorithms: A Review",
      "authors": [
        "Cao Chenyu"
      ],
      "abstract": "<jats:title>Abstract</jats:title>\n               <jats:p>The technology of Image caption is developing rapidly. In order to review the recent advancement in this field, this article briefly summarize several typical works in image caption researching, in which they figured out new ways to improve the accuracy or efficiency. We describe the methods, organize the results of experiments in one form, and then analyse the data. Besides, a novel quantitative metric which can measure the quality of image caption more objectively is also introduced.</jats:p>",
      "doi": "https://doi.org/10.1088/1742-6596/1438/1/012025",
      "year": 2020,
      "source": "Crossref"
    },
    {
      "title": "Evaluation of Automated Image Descriptions for Visually Impaired\n  Students",
      "authors": [
        "Anett Hoppe",
        "David Morris",
        "Ralph Ewerth"
      ],
      "abstract": "Illustrations are widely used in education, and sometimes, alternatives are\nnot available for visually impaired students. Therefore, those students would\nbenefit greatly from an automatic illustration description system, but only if\nthose descriptions were complete, correct, and easily understandable using a\nscreenreader. In this paper, we report on a study for the assessment of\nautomated image descriptions. We interviewed experts to establish evaluation\ncriteria, which we then used to create an evaluation questionnaire for sighted\nnon-expert raters, and description templates. We used this questionnaire to\nevaluate the quality of descriptions which could be generated with a\ntemplate-based automatic image describer. We present evidence that these\ntemplates have the potential to generate useful descriptions, and that the\nquestionnaire identifies problems with description templates.",
      "doi": "https://doi.org/10.1007/978-3-030-78270-2_35",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Image Specificity",
      "authors": [
        "Mainak Jas",
        "Devi Parikh"
      ],
      "abstract": "For some images, descriptions written by multiple people are consistent with\neach other. But for other images, descriptions across people vary considerably.\nIn other words, some images are specific $-$ they elicit consistent\ndescriptions from different people $-$ while other images are ambiguous.\nApplications involving images and text can benefit from an understanding of\nwhich images are specific and which ones are ambiguous. For instance, consider\ntext-based image retrieval. If a query description is moderately similar to the\ncaption (or reference description) of an ambiguous image, that query may be\nconsidered a decent match to the image. But if the image is very specific, a\nmoderate similarity between the query and the reference description may not be\nsufficient to retrieve the image.\n  In this paper, we introduce the notion of image specificity. We present two\nmechanisms to measure specificity given multiple descriptions of an image: an\nautomated measure and a measure that relies on human judgement. We analyze\nimage specificity with respect to image content and properties to better\nunderstand what makes an image specific. We then train models to automatically\npredict the specificity of an image from image features alone without requiring\ntextual descriptions of the image. Finally, we show that modeling image\nspecificity leads to improvements in a text-based image retrieval application.",
      "doi": "arXiv:1502.04569v2",
      "year": 2015,
      "source": "arXiv"
    },
    {
      "title": "Chart-Text: A Fully Automated Chart Image Descriptor",
      "authors": [
        "Abhijit Balaji",
        "Thuvaarakkesh Ramanathan",
        "Venkateshwarlu Sonathi"
      ],
      "abstract": "Images greatly help in understanding, interpreting and visualizing data.\nAdding textual description to images is the first and foremost principle of web\naccessibility. Visually impaired users using screen readers will use these\ntextual descriptions to get better understanding of images present in digital\ncontents. In this paper, we propose Chart-Text a novel fully automated system\nthat creates textual description of chart images. Given a PNG image of a chart,\nour Chart-Text system creates a complete textual description of it. First, the\nsystem classifies the type of chart and then it detects and classifies the\nlabels and texts in the charts. Finally, it uses specific image processing\nalgorithms to extract relevant information from the chart images. Our proposed\nsystem achieves an accuracy of 99.72% in classifying the charts and an accuracy\nof 78.9% in extracting the data and creating the corresponding textual\ndescription.",
      "doi": "arXiv:1812.10636v1",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "GeoBiked: A Dataset with Geometric Features and Automated Labeling\n  Techniques to Enable Deep Generative Models in Engineering Design",
      "authors": [
        "Phillip Mueller",
        "Sebastian Mueller",
        "Lars Mikelsons"
      ],
      "abstract": "We provide a dataset for enabling Deep Generative Models (DGMs) in\nengineering design and propose methods to automate data labeling by utilizing\nlarge-scale foundation models. GeoBiked is curated to contain 4 355 bicycle\nimages, annotated with structural and technical features and is used to\ninvestigate two automated labeling techniques: The utilization of consolidated\nlatent features (Hyperfeatures) from image-generation models to detect\ngeometric correspondences (e.g. the position of the wheel center) in structural\nimages and the generation of diverse text descriptions for structural images.\nGPT-4o, a vision-language-model (VLM), is instructed to analyze images and\nproduce diverse descriptions aligned with the system-prompt. By representing\ntechnical images as Diffusion-Hyperfeatures, drawing geometric correspondences\nbetween them is possible. The detection accuracy of geometric points in unseen\nsamples is improved by presenting multiple annotated source images. GPT-4o has\nsufficient capabilities to generate accurate descriptions of technical images.\nGrounding the generation only on images leads to diverse descriptions but\ncauses hallucinations, while grounding it on categorical labels restricts the\ndiversity. Using both as input balances creativity and accuracy. Successfully\nusing Hyperfeatures for geometric correspondence suggests that this approach\ncan be used for general point-detection and annotation tasks in technical\nimages. Labeling such images with text descriptions using VLMs is possible, but\ndependent on the models detection capabilities, careful prompt-engineering and\nthe selection of input information. Applying foundation models in engineering\ndesign is largely unexplored. We aim to bridge this gap with a dataset to\nexplore training, finetuning and conditioning DGMs in this field and suggesting\napproaches to bootstrap foundation models to process technical images.",
      "doi": "arXiv:2409.17045v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Hood Technique for Robotic Radical Prostatectomy-Preserving Periurethral Anatomical Structures in the Space of Retzius and Sparing the Pouch of Douglas, Enabling Early Return of Continence Without Compromising Surgical Margin Rates.",
      "authors": [
        "Wagaskar VG",
        "Mittal A",
        "Sobotka S",
        "Ratnani P",
        "Lantz A",
        "Falagario UG",
        "Martini A",
        "Dovey Z",
        "Treacy PJ",
        "Pathak P",
        "Nair S",
        "Roy B",
        "Chakravarty D",
        "Lewis S",
        "Haines K 3rd",
        "Wiklund P",
        "Tewari A"
      ],
      "abstract": "A common side effect following radical prostatectomy is urinary incontinence. Here, we describe a novel surgical technique to reduce postoperative urinary incontinence and facilitate early return of continence. To describe the novel \"hood technique\" for robotic-assisted radical prostatectomy (RARP). This is an institutional review board-approved prospective study of 300 patients (median age 64 yr) with localized prostate cancer treated with the RARP hood technique at a major urban hospital between April 2018 and March 2019. The exclusion criteria were as follows: patients with anterior tumor location based on biopsy or multiparametric magnetic resonance imaging. All but one patient participated in follow-up over 12 mo after the procedure. The RARP \"hood technique\" was performed to preserve the detrusor apron, puboprostatic ligament complex, arcus tendineus, endopelvic fascia, and pouch of Douglas. Clinical data collected included pre- and intraoperative variables, and postoperative functional and oncological outcomes and complications. Descriptive statistical analysis was performed. Continence rates at 1, 2, 4, 6 12, 24, and 48 wk after catheter removal were 21%, 36%, 83%, 88%, 91%, 94%, and 95%, respectively. Positive surgical margin rate was 6%. Thirty patients (9.7%) experienced complications after RARP: 17 (5.7%), 11 (3.6%), and one (0.4%) had Clavien-Dindo grade I, II, and III complications, respectively. This study was conducted within a single health system and may not be generalizable. The study lacked randomization and a comparative arm. Results indicate that the hood technique spares musculofascial structures anterior to the urethral sphincter complex with early return of continence after surgery, without compromising positive surgical margin rates. Exclusion of anterior tumor location contributed to a reduction in positive surgical margins. By better preservation of anatomical structures around the urethra, we were able to achieve early return of urinary continence without a negative impact on complications and cancer outcomes.",
      "doi": "https://doi.org/10.1016/j.eururo.2020.09.044",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Quantitative nailfold capillaroscopy-update and possible next steps.",
      "authors": [
        "Herrick AL",
        "Berks M",
        "Taylor CJ"
      ],
      "abstract": "We review the exciting potential (and challenges) of quantitative nailfold capillaroscopy, focusing on its role in systemic sclerosis. Quantifying abnormality, including automated analysis of nailfold images, overcomes the subjectivity of qualitative/descriptive image interpretation. First we consider the rationale for quantitative analysis, including the potential for precise discrimination between normal and abnormal capillaries and for reliable measurement of disease progression and treatment response. We discuss nailfold image acquisition and interpretation, and describe how early work on semi-quantitative and quantitative analysis paved the way for semi-automated and automated analysis. Measurement of red blood cell velocity is described briefly. Finally we give a personal view on 'next steps'. From a clinical perspective, increased uptake of nailfold capillaroscopy by general rheumatologists could be achieved via low-cost hand-held devices with cloud-based automated analysis. From a research perspective, automated analysis could facilitate large-scale prospective studies using capillaroscopic parameters as possible biomarkers of systemic sclerosis-spectrum disorders.",
      "doi": "https://doi.org/10.1093/rheumatology/keab006",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Controllable 3D Outdoor Scene Generation via Scene Graphs",
      "authors": [
        "Yuheng Liu",
        "Xinke Li",
        "Yuning Zhang",
        "Lu Qi",
        "Xin Li",
        "Wenping Wang",
        "Chongshou Li",
        "Xueting Li",
        "Ming-Hsuan Yang"
      ],
      "abstract": "Three-dimensional scene generation is crucial in computer vision, with\napplications spanning autonomous driving, gaming and the metaverse. Current\nmethods either lack user control or rely on imprecise, non-intuitive\nconditions. In this work, we propose a method that uses, scene graphs, an\naccessible, user friendly control format to generate outdoor 3D scenes. We\ndevelop an interactive system that transforms a sparse scene graph into a dense\nBEV (Bird's Eye View) Embedding Map, which guides a conditional diffusion model\nto generate 3D scenes that match the scene graph description. During inference,\nusers can easily create or modify scene graphs to generate large-scale outdoor\nscenes. We create a large-scale dataset with paired scene graphs and 3D\nsemantic scenes to train the BEV embedding and diffusion models. Experimental\nresults show that our approach consistently produces high-quality 3D urban\nscenes closely aligned with the input scene graphs. To the best of our\nknowledge, this is the first approach to generate 3D outdoor scenes conditioned\non scene graphs.",
      "doi": "arXiv:2503.07152v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "3D Scene Diffusion Guidance using Scene Graphs",
      "authors": [
        "Mohammad Naanaa",
        "Katharina Schmid",
        "Yinyu Nie"
      ],
      "abstract": "Guided synthesis of high-quality 3D scenes is a challenging task. Diffusion\nmodels have shown promise in generating diverse data, including 3D scenes.\nHowever, current methods rely directly on text embeddings for controlling the\ngeneration, limiting the incorporation of complex spatial relationships between\nobjects. We propose a novel approach for 3D scene diffusion guidance using\nscene graphs. To leverage the relative spatial information the scene graphs\nprovide, we make use of relational graph convolutional blocks within our\ndenoising network. We show that our approach significantly improves the\nalignment between scene description and generated scene.",
      "doi": "arXiv:2308.04468v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Hierarchically-Structured Open-Vocabulary Indoor Scene Synthesis with\n  Pre-trained Large Language Model",
      "authors": [
        "Weilin Sun",
        "Xinran Li",
        "Manyi Li",
        "Kai Xu",
        "Xiangxu Meng",
        "Lei Meng"
      ],
      "abstract": "Indoor scene synthesis aims to automatically produce plausible, realistic and\ndiverse 3D indoor scenes, especially given arbitrary user requirements.\nRecently, the promising generalization ability of pre-trained large language\nmodels (LLM) assist in open-vocabulary indoor scene synthesis. However, the\nchallenge lies in converting the LLM-generated outputs into reasonable and\nphysically feasible scene layouts. In this paper, we propose to generate\nhierarchically structured scene descriptions with LLM and then compute the\nscene layouts. Specifically, we train a hierarchy-aware network to infer the\nfine-grained relative positions between objects and design a divide-and-conquer\noptimization to solve for scene layouts. The advantages of using hierarchically\nstructured scene representation are two-fold. First, the hierarchical structure\nprovides a rough grounding for object arrangement, which alleviates\ncontradictory placements with dense relations and enhances the generalization\nability of the network to infer fine-grained placements. Second, it naturally\nsupports the divide-and-conquer optimization, by first arranging the sub-scenes\nand then the entire scene, to more effectively solve for a feasible layout. We\nconduct extensive comparison experiments and ablation studies with both\nqualitative and quantitative evaluations to validate the effectiveness of our\nkey designs with the hierarchically structured scene representation. Our\napproach can generate more reasonable scene layouts while better aligned with\nthe user requirements and LLM descriptions. We also present open-vocabulary\nscene synthesis and interactive scene design results to show the strength of\nour approach in the applications.",
      "doi": "arXiv:2502.10675v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation",
      "authors": [
        "Hritik Bansal",
        "Yonatan Bitton",
        "Michal Yarom",
        "Idan Szpektor",
        "Aditya Grover",
        "Kai-Wei Chang"
      ],
      "abstract": "Most of these text-to-video (T2V) generative models often produce\nsingle-scene video clips that depict an entity performing a particular action\n(e.g., 'a red panda climbing a tree'). However, it is pertinent to generate\nmulti-scene videos since they are ubiquitous in the real-world (e.g., 'a red\npanda climbing a tree' followed by 'the red panda sleeps on the top of the\ntree'). To generate multi-scene videos from the pretrained T2V model, we\nintroduce a simple and effective Time-Aligned Captions (TALC) framework.\nSpecifically, we enhance the text-conditioning mechanism in the T2V\narchitecture to recognize the temporal alignment between the video scenes and\nscene descriptions. For instance, we condition the visual features of the\nearlier and later scenes of the generated video with the representations of the\nfirst scene description (e.g., 'a red panda climbing a tree') and second scene\ndescription (e.g., 'the red panda sleeps on the top of the tree'),\nrespectively. As a result, we show that the T2V model can generate multi-scene\nvideos that adhere to the multi-scene text descriptions and be visually\nconsistent (e.g., entity and background). Further, we finetune the pretrained\nT2V model with multi-scene video-text data using the TALC framework. We show\nthat the TALC-finetuned model outperforms the baseline by achieving a relative\ngain of 29% in the overall score, which averages visual consistency and text\nadherence using human evaluation.",
      "doi": "arXiv:2405.04682v4",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "A Comprehensive Survey of Scene Graphs: Generation and Application.",
      "authors": [
        "Chang X",
        "Ren P",
        "Xu P",
        "Li Z",
        "Chen X",
        "Hauptmann A"
      ],
      "abstract": "Scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisfied with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also understand the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and find similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More specifically, we first summarize the general definition of the scene graph, then conducte a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigate the main applications of scene graphs and summarize the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs.",
      "doi": "https://doi.org/10.1109/tpami.2021.3137605",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Text2NeRF: Text-Driven 3D Scene Generation With Neural Radiance Fields.",
      "authors": [
        "Zhang J",
        "Li X",
        "Wan Z",
        "Wang C",
        "Liao J"
      ],
      "abstract": "Text-driven 3D scene generation is widely applicable to video gaming, film industry, and metaverse applications that have a large demand for 3D scenes. However, existing text-to-3D generation methods are limited to producing 3D objects with simple geometries and dreamlike styles that lack realism. In this work, we present Text2NeRF, which is able to generate a wide range of 3D scenes with complicated geometric structures and high-fidelity textures purely from a text prompt. To this end, we adopt NeRF as the 3D representation and leverage a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF to reflect the scene description. Specifically, we employ the diffusion model to infer the text-related image as the content prior and use a monocular depth estimation method to offer the geometric prior. Both content and geometric priors are utilized to update the NeRF model. To guarantee textured and geometric consistency between different views, we introduce a progressive scene inpainting and updating strategy for novel view synthesis of the scene. Our method requires no additional training data but only a natural language description of the scene as the input. Extensive experiments demonstrate that our Text2NeRF outperforms existing methods in producing photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of natural language prompts. Our code and model are available at https://github.com/eckertzhang/Text2NeRF.",
      "doi": "https://doi.org/10.1109/tvcg.2024.3361502",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Parasitic Language.",
      "authors": [
        "Amir D"
      ],
      "abstract": "This paper presents initial thoughts about the phenomenon of parasitic language. This is a language which clings to the other's linguistic patterns and in doing so produces a double manifestation of omnipotence and impotence; a language that forges a linguistic \"prosthesis,\" and while allowing for a false manifestation of language and thinking, constitutes thoughts as foreign objects that are mechanically and artificially \"stuck\" to the speaking subject. The early roots of this language, as illustrated in a detailed analytical case description, inhere in the infiltration of language by multi-generational traumatic traces. This turns language itself into a scene of simultaneous repetition of the act of salvation and the act of annihilation.",
      "doi": "https://doi.org/10.1080/00332828.2020.1773128",
      "year": 2020,
      "source": "PubMed"
    },
    {
      "title": "The neuroaesthetics of architectural spaces.",
      "authors": [
        "Chatterjee A",
        "Coburn A",
        "Weinberger A"
      ],
      "abstract": "People in developed countries spend over 90% of their time in built environments. Yet, we know little about its pervasive and often hidden effects on our mental state and our brain. Despite growing interest in the neuroscience of architecture, much of this scholarship has been descriptive. The typical approach is to map knowledge of the brain onto constructs important to architecture. For a programmatic line of research, how might descriptive neuroarchitecture be transformed into an experimental science? We review the literature outlining how one might consider experimental architecture first by examining the role of natural features in architectural settings. We then turn to the human experience of occupants, and hypothesized that aesthetic responses to architectural interiors reduce to key psychological dimensions. Conducting Psychometric Network Analysis (PNA) and Principal Components Analysis (PCA) on responses to curated images, we identified three components: coherence (ease of organizing and comprehending a scene), fascination (informational richness and generated interest), and hominess (personal ease and comfort). Coherence and fascination are well-established dimensions for natural scenes. Hominess was a new dimension related to architectural interiors. Central to all three communities in the PNA was emotional valence. We also reanalyzed data from an earlier fMRI study in which participants made beauty and approach-avoidance decisions while viewing the same images. Regardless of task, the degree of fascination covaried with neural activity in the right lingual gyrus. In contrast, coherence covaried with neural activity in the left inferior occipital gyrus only when participants judged beauty, and hominess covaried with neural activity in the left cuneus only when they made approach-avoidance decisions. The visual brain harbours hidden sensitivities to architectural interiors that are captured by the dimensions of coherence, fascination, and hominess. These findings represent first steps towards an experimental neuroarchitecture.",
      "doi": "https://doi.org/10.1007/s10339-021-01043-4",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene\n  Understanding",
      "authors": [
        "Baoxiong Jia",
        "Yixin Chen",
        "Huangyue Yu",
        "Yan Wang",
        "Xuesong Niu",
        "Tengyu Liu",
        "Qing Li",
        "Siyuan Huang"
      ],
      "abstract": "3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io.",
      "doi": "arXiv:2401.09340v3",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast\n  Mapping in Vision-Language Models",
      "authors": [
        "Ziqiao Ma",
        "Jiayi Pan",
        "Joyce Chai"
      ],
      "abstract": "The ability to connect language units to their referents in the physical\nworld, referred to as grounding, is crucial to learning and understanding\ngrounded meanings of words. While humans demonstrate fast mapping in new word\nlearning, it remains unclear whether modern vision-language models can truly\nrepresent language with their grounded meanings and how grounding may further\nbootstrap new word learning. To this end, we introduce Grounded Open Vocabulary\nAcquisition (GOVA) to examine grounding and bootstrapping in open-world\nlanguage learning. As an initial attempt, we propose object-oriented BERT\n(OctoBERT), a novel visually-grounded language model by pre-training on\nimage-text pairs highlighting grounding as an objective. Through extensive\nexperiments and analysis, we demonstrate that OctoBERT is a more coherent and\nfast grounded word learner, and that the grounding ability acquired during\npre-training helps the model to learn unseen words more rapidly and robustly.\nOur code is available at https://github.com/sled-group/world-to-words",
      "doi": "arXiv:2306.08685v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling",
      "authors": [
        "Chengxu Zhuang",
        "Evelina Fedorenko",
        "Jacob Andreas"
      ],
      "abstract": "Today's most accurate language models are trained on orders of magnitude more\nlanguage data than human language learners receive - but with no supervision\nfrom other sensory modalities that play a crucial role in human learning. Can\nwe make LMs' representations and predictions more accurate (and more\nhuman-like) with more ecologically plausible supervision? This paper describes\nLexiContrastive Grounding (LCG), a grounded language learning procedure that\nleverages visual supervision to improve textual representations.\nLexiContrastive Grounding combines a next token prediction strategy with a\ncontrastive visual grounding objective, focusing on early-layer representations\nthat encode lexical information. Across multiple word-learning and\nsentence-understanding benchmarks, LexiContrastive Grounding not only\noutperforms standard language-only models in learning efficiency, but also\nimproves upon vision-and-language learning procedures including CLIP, GIT,\nFlamingo, and Vokenization. Moreover, LexiContrastive Grounding improves\nperplexity by around 5% on multiple language modeling tasks. This work\nunderscores the potential of incorporating visual grounding into language\nmodels, aligning more closely with the multimodal nature of human language\nacquisition.",
      "doi": "arXiv:2403.14551v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Measuring Social Biases in Grounded Vision and Language Embeddings",
      "authors": [
        "Candace Ross",
        "Boris Katz",
        "Andrei Barbu"
      ],
      "abstract": "We generalize the notion of social biases from language embeddings to\ngrounded vision and language embeddings. Biases are present in grounded\nembeddings, and indeed seem to be equally or more significant than for\nungrounded embeddings. This is despite the fact that vision and language can\nsuffer from different biases, which one might hope could attenuate the biases\nin both. Multiple ways exist to generalize metrics measuring bias in word\nembeddings to this new setting. We introduce the space of generalizations\n(Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations\nanswer different yet important questions about how biases, language, and vision\ninteract. These metrics are used on a new dataset, the first for grounded bias,\ncreated by augmenting extending standard linguistic bias benchmarks with 10,228\nimages from COCO, Conceptual Captions, and Google Images. Dataset construction\nis challenging because vision datasets are themselves very biased. The presence\nof these biases in systems will begin to have real-world consequences as they\nare deployed, making carefully measuring bias and then mitigating it critical\nto building a fair society.",
      "doi": "arXiv:2002.08911v2",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Enhancing Visual Grounding in Vision-Language Pre-Training With Position-Guided Text Prompts.",
      "authors": [
        "Wang AJ",
        "Zhou P",
        "Shou MZ",
        "Yan S"
      ],
      "abstract": "Vision-Language Pre-Training (VLP) has demonstrated remarkable potential in aligning image and text pairs, paving the way for a wide range of cross-modal learning tasks. Nevertheless, we have observed that VLP models often fall short in terms of visual grounding and localization capabilities, which are crucial for many downstream tasks, such as visual reasoning. In response, we introduce a novel Position-guided Text Prompt (PTP) paradigm to bolster the visual grounding abilities of cross-modal models trained with VLP. In the VLP phase, PTP divides an image into N x N blocks and employs a widely-used object detector to identify objects within each block. PTP then reframes the visual grounding task as a fill-in-the-blank problem, encouraging the model to predict objects in given blocks or regress the blocks of a given object, exemplified by filling \"[P]\" or \"[O]\" in a PTP sentence such as \"The block [P] has a [O].\" This strategy enhances the visual grounding capabilities of VLP models, enabling them to better tackle various downstream tasks. Additionally, we integrate the seconda-order relationships between objects to further enhance the visual grounding capabilities of our proposed PTP paradigm. Incorporating PTP into several state-of-the-art VLP frameworks leads to consistently significant improvements across representative cross-modal learning model architectures and multiple benchmarks, such as zero-shot Flickr30 k Retrieval (+5.6 in average recall@1) for ViLT baseline, and COCO Captioning (+5.5 in CIDEr) for the state-of-the-art BLIP baseline. Furthermore, PTP attains comparable results with object-detector-based methods and a faster inference speed, as it discards its object detector during inference, unlike other approaches.",
      "doi": "https://doi.org/10.1109/tpami.2023.3343736",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer.",
      "authors": [
        "Deng J",
        "Yang Z",
        "Liu D",
        "Chen T",
        "Zhou W",
        "Zhang Y",
        "Li H",
        "Ouyang W"
      ],
      "abstract": "In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
      "doi": "https://doi.org/10.1109/tpami.2023.3296823",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Challenges and Prospects in Vision and Language Research.",
      "authors": [
        "Kafle K",
        "Shrestha R",
        "Kanan C"
      ],
      "abstract": "Language grounded image understanding tasks have often been proposed as a method for evaluating progress in artificial intelligence. Ideally, these tasks should test a plethora of capabilities that integrate computer vision, reasoning, and natural language understanding. However, the datasets and evaluation procedures used in these tasks are replete with flaws which allows the vision and language (V&L) algorithms to achieve a good performance without a robust understanding of vision and language. We argue for this position based on several recent studies in V&L literature and our own observations of dataset bias, robustness, and spurious correlations. Finally, we propose that several of these challenges can be mitigated by creation of carefully designed benchmarks.",
      "doi": "https://doi.org/10.3389/frai.2019.00028",
      "year": 2019,
      "source": "PubMed"
    },
    {
      "title": "Vision-Language Navigation Policy Learning and Adaptation.",
      "authors": [
        "Wang X",
        "Huang Q",
        "Celikyilmaz A",
        "Gao J",
        "Shen D",
        "Wang YF",
        "Wang WY",
        "Zhang L"
      ],
      "abstract": "Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10 percent on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7 to 11.7 percent).",
      "doi": "https://doi.org/10.1109/tpami.2020.2972281",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Creative Captioning: An AI Grand Challenge Based on the Dixit Board Game",
      "authors": [
        "Maithilee Kunda",
        "Irina Rabkina"
      ],
      "abstract": "We propose a new class of \"grand challenge\" AI problems that we call creative\ncaptioning---generating clever, interesting, or abstract captions for images,\nas well as understanding such captions. Creative captioning draws on core AI\nresearch areas of vision, natural language processing, narrative reasoning, and\nsocial reasoning, and across all these areas, it requires sophisticated uses of\ncommon sense and cultural knowledge. In this paper, we analyze several specific\nresearch problems that fall under creative captioning, using the popular board\ngame Dixit as both inspiration and proposed testing ground. We expect that\nDixit could serve as an engaging and motivating benchmark for creative\ncaptioning across numerous AI research communities for the coming 1-2 decades.",
      "doi": "arXiv:2010.00048v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "LineCap: Line Charts for Data Visualization Captioning Models",
      "authors": [
        "Anita Mahinpei",
        "Zona Kostic",
        "Chris Tanner"
      ],
      "abstract": "Data visualization captions help readers understand the purpose of a\nvisualization and are crucial for individuals with visual impairments. The\nprevalence of poor figure captions and the successful application of deep\nlearning approaches to image captioning motivate the use of similar techniques\nfor automated figure captioning. However, research in this field has been\nstunted by the lack of suitable datasets. We introduce LineCap, a novel figure\ncaptioning dataset of 3,528 figures, and we provide insights from curating this\ndataset and using end-to-end deep learning models for automated figure\ncaptioning.",
      "doi": "arXiv:2207.07243v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Dense Relational Captioning: Triple-Stream Networks for\n  Relationship-Based Captioning",
      "authors": [
        "Dong-Jin Kim",
        "Jinsoo Choi",
        "Tae-Hyun Oh",
        "In So Kweon"
      ],
      "abstract": "Our goal in this work is to train an image captioning model that generates\nmore dense and informative captions. We introduce \"relational captioning,\" a\nnovel image captioning task which aims to generate multiple captions with\nrespect to relational information between objects in an image. Relational\ncaptioning is a framework that is advantageous in both diversity and amount of\ninformation, leading to image understanding based on relationships. Part-of\nspeech (POS, i.e. subject-object-predicate categories) tags can be assigned to\nevery English word. We leverage the POS as a prior to guide the correct\nsequence of words in a caption. To this end, we propose a multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units for the\nrespective POS and jointly performs POS prediction and captioning. We\ndemonstrate more diverse and richer representations generated by the proposed\nmodel against several baselines and competing methods.",
      "doi": "arXiv:1903.05942v4",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal\n  Perception",
      "authors": [
        "Yanpeng Sun",
        "Jing Hao",
        "Ke Zhu",
        "Jiang-Jiang Liu",
        "Yuxiang Zhao",
        "Xiaofan Li",
        "Gang Zhang",
        "Zechao Li",
        "Jingdong Wang"
      ],
      "abstract": "Training Large Multimodality Models (LMMs) relies on descriptive image\ncaption that connects image and language. Existing methods either distill the\ncaption from the LMM models or construct the captions from the internet images\nor by human. We propose to leverage off-the-shelf visual specialists, which\nwere trained from annotated images initially not for image captioning, for\nenhancing the image caption.\n  Our approach, named DCE, explores object low-level and fine-grained\nattributes (e.g., depth, emotion and fine-grained categories) and object\nrelations (e.g., relative location and human-object-interaction (HOI)), and\ncombine the attributes into the descriptive caption. Experiments demonstrate\nthat such visual specialists are able to improve the performance for visual\nunderstanding tasks as well as reasoning that benefits from more accurate\nvisual understanding. We will release the source code and the pipeline so that\nother visual specialists are easily combined into the pipeline. The complete\nsource code of DCE pipeline and datasets will be available at\n\\url{https://github.com/syp2ysy/DCE}.",
      "doi": "arXiv:2412.14233v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Enhancing image caption generation through context-aware attention mechanism.",
      "authors": [
        "Bhuiyan A",
        "Hossain E",
        "Hoque MM",
        "Ali Akber Dewan M"
      ],
      "abstract": "Image captioning, the process of generating natural language descriptions based on image content, has garnered attention in AI research for its implications in scene understanding and human-computer interaction. While much prior research has focused on caption generation for English, addressing low-resource languages like Bengali presents challenges, particularly in producing coherent captions linking visual objects with corresponding words. This paper proposes a context-aware attention mechanism over semantic attention to accurately diagnose objects for image captioning in Bengali. The proposed architecture consists of an encoder and a decoder block. We chose ResNet-50 over the other pre-trained models for encoding the image features due to its ability to solve the vanishing gradient problem and recognize complex object features. For decoding generated captions, a bidirectional Gated Recurrent Unit (GRU) architecture combined with an attention mechanism captures contextual dependencies in both directions, resulting in more accurate captions. The paper also highlights the challenge of transferring knowledge between domains, especially with culturally specific images. Evaluation of three Bengali benchmark datasets, namely",
      "doi": "https://doi.org/10.1016/j.heliyon.2024.e36272",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Infrared Image Caption Based on Object-Oriented Attention.",
      "authors": [
        "Lv J",
        "Hui T",
        "Zhi Y",
        "Xu Y"
      ],
      "abstract": "With the ongoing development of image technology, the deployment of various intelligent applications on embedded devices has attracted increased attention in the industry. One such application is automatic image captioning for infrared images, which involves converting images into text. This practical task is widely used in night security, as well as for understanding night scenes and other scenarios. However, due to the differences in image features and the complexity of semantic information, generating captions for infrared images remains a challenging task. From the perspective of deployment and application, to improve the correlation between descriptions and objects, we introduced the YOLOv6 and LSTM as encoder-decoder structure and proposed infrared image caption based on object-oriented attention. Firstly, to improve the domain adaptability of the detector, we optimized the pseudo-label learning process. Secondly, we proposed the object-oriented attention method to address the alignment problem between complex semantic information and embedded words. This method helps select the most crucial features of the object region and guides the caption model in generating words that are more relevant to the object. Our methods have shown good performance on the infrared image and can produce words explicitly associated with the object regions located by the detector. The robustness and effectiveness of the proposed methods were demonstrated through evaluation on various datasets, along with other state-of-the-art methods. Our approach achieved BLUE-4 scores of 31.6 and 41.2 on KAIST and Infrared City and Town datasets, respectively. Our approach provides a feasible solution for the deployment of embedded devices in industrial applications.",
      "doi": "https://doi.org/10.3390/e25050826",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Transfer Learning via Unsupervised Task Discovery for Visual Question\n  Answering",
      "authors": [
        "Hyeonwoo Noh",
        "Taehoon Kim",
        "Jonghwan Mun",
        "Bohyung Han"
      ],
      "abstract": "We study how to leverage off-the-shelf visual and linguistic data to cope\nwith out-of-vocabulary answers in visual question answering task. Existing\nlarge-scale visual datasets with annotations such as image class labels,\nbounding boxes and region descriptions are good sources for learning rich and\ndiverse visual concepts. However, it is not straightforward how the visual\nconcepts can be captured and transferred to visual question answering models\ndue to missing link between question dependent answering models and visual data\nwithout question. We tackle this problem in two steps: 1) learning a task\nconditional visual classifier, which is capable of solving diverse\nquestion-specific visual recognition tasks, based on unsupervised task\ndiscovery and 2) transferring the task conditional visual classifier to visual\nquestion answering models. Specifically, we employ linguistic knowledge sources\nsuch as structured lexical database (e.g. WordNet) and visual descriptions for\nunsupervised task discovery, and transfer a learned task conditional visual\nclassifier as an answering unit in a visual question answering model. We\nempirically show that the proposed algorithm generalizes to out-of-vocabulary\nanswers successfully using the knowledge transferred from the visual dataset.",
      "doi": "arXiv:1810.02358v2",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue",
      "authors": [
        "Zipeng Xu",
        "Fangxiang Feng",
        "Xiaojie Wang",
        "Yushu Yang",
        "Huixing Jiang",
        "Zhongyuan Wang"
      ],
      "abstract": "A goal-oriented visual dialogue involves multi-turn interactions between two\nagents, Questioner and Oracle. During which, the answer given by Oracle is of\ngreat significance, as it provides golden response to what Questioner concerns.\nBased on the answer, Questioner updates its belief on target visual content and\nfurther raises another question. Notably, different answers drive into\ndifferent visual beliefs and future questions. However, existing methods always\nindiscriminately encode answers after much longer questions, resulting in a\nweak utilization of answers. In this paper, we propose an Answer-Driven Visual\nState Estimator (ADVSE) to impose the effects of different answers on visual\nstates. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture\nthe answer-driven effect on visual attention by sharpening question-related\nattention and adjusting it by answer-based logical operation at each turn. Then\nbased on the focusing attention, we get the visual state estimation by\nConditional Visual Information Fusion (CVIF), where overall information and\ndifference information are fused conditioning on the question-answer state. We\nevaluate the proposed ADVSE to both question generator and guesser tasks on the\nlarge-scale GuessWhat?! dataset and achieve the state-of-the-art performances\non both tasks. The qualitative results indicate that the ADVSE boosts the agent\nto generate highly efficient questions and obtains reliable visual attentions\nduring the reasonable question generation and guess processes.",
      "doi": "https://doi.org/10.1145/3394171.3413668",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Long-Form Answers to Visual Questions from Blind and Low Vision People",
      "authors": [
        "Mina Huh",
        "Fangyuan Xu",
        "Yi-Hao Peng",
        "Chongyan Chen",
        "Hansika Murugu",
        "Danna Gurari",
        "Eunsol Choi",
        "Amy Pavel"
      ],
      "abstract": "Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies.",
      "doi": "arXiv:2408.06303v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People",
      "authors": [
        "Danna Gurari",
        "Qing Li",
        "Abigale J. Stangl",
        "Anhong Guo",
        "Chi Lin",
        "Kristen Grauman",
        "Jiebo Luo",
        "Jeffrey P. Bigham"
      ],
      "abstract": "The study of algorithms to automatically answer visual questions currently is\nmotivated by visual question answering (VQA) datasets constructed in artificial\nVQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising\nfrom a natural VQA setting. VizWiz consists of over 31,000 visual questions\noriginating from blind people who each took a picture using a mobile phone and\nrecorded a spoken question about it, together with 10 crowdsourced answers per\nvisual question. VizWiz differs from the many existing VQA datasets because (1)\nimages are captured by blind photographers and so are often poor quality, (2)\nquestions are spoken and so are more conversational, and (3) often visual\nquestions cannot be answered. Evaluation of modern algorithms for answering\nvisual questions and deciding if a visual question is answerable reveals that\nVizWiz is a challenging dataset. We introduce this dataset to encourage a\nlarger community to develop more generalized algorithms that can assist blind\npeople.",
      "doi": "arXiv:1802.08218v4",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "Medical visual question answering: A survey.",
      "authors": [
        "Lin Z",
        "Zhang D",
        "Tao Q",
        "Shi D",
        "Haffari G",
        "Wu Q",
        "He M",
        "Ge Z"
      ],
      "abstract": "Medical Visual Question Answering (VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features. In the first part of this survey, we collect and discuss the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. We summarize and discuss their techniques, innovations, and potential improvements. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions. Our goal is to provide comprehensive and helpful information for researchers interested in the medical visual question answering field and encourage them to conduct further research in this field.",
      "doi": "https://doi.org/10.1016/j.artmed.2023.102611",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Re-Attention for Visual Question Answering.",
      "authors": [
        "Guo W",
        "Zhang Y",
        "Yang J",
        "Yuan X"
      ],
      "abstract": "A simultaneous understanding of questions and images is crucial in Visual Question Answering (VQA). While the existing models have achieved satisfactory performance by associating questions with key objects in images, the answers also contain rich information that can be used to describe the visual contents in images. In this paper, we propose a re-attention framework to utilize the information in answers for the VQA task. The framework first learns the initial attention weights for the objects by calculating the similarity of each word-object pair in the feature space. Then, the visual attention map is reconstructed by re-attending the objects in images based on the answer. Through keeping the initial visual attention map and the reconstructed one to be consistent, the learned visual attention map can be corrected by the answer information. Besides, we introduce a gate mechanism to automatically control the contribution of re-attention to model training based on the entropy of the learned initial visual attention maps. We conduct experiments on three benchmark datasets, and the results demonstrate the proposed model performs favorably against state-of-the-art methods.",
      "doi": "https://doi.org/10.1109/tip.2021.3097180",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning.",
      "authors": [
        "Liu B",
        "Zhan LM",
        "Xu L",
        "Wu XM"
      ],
      "abstract": "Medical visual question answering (Med-VQA) aims to accurately answer a clinical question presented with a medical image. Despite its enormous potential in healthcare services, the development of this technology is still in the initial stage. On the one hand, Med-VQA tasks are highly challenging due to the massive diversity of clinical questions that require different visual reasoning skills for different types of questions. On the other hand, medical images are complex in nature and very different from natural images, while current Med-VQA datasets are small-scale with a few hundred radiology images, making it difficult to train a well-performing visual feature extractor. This paper addresses above two critical issues. We propose a novel conditional reasoning mechanism with a question-conditioned reasoning component and a type-conditioned reasoning strategy to learn effective reasoning skills for different Med-VQA tasks adaptively. Further, we propose to pre-train a visual feature extractor for Med-VQA via contrastive learning on large amounts of unlabeled radiology images. The effectiveness of our proposals is validated by extensive experiments on existing Med-VQA benchmarks, which show significant improvement of our model in prediction accuracy over state-of-the-art methods. The source code and pre-training dataset are provided at https://github.com/Awenbocc/CPCR.",
      "doi": "https://doi.org/10.1109/tmi.2022.3232411",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Visual question answering based on local-scene-aware referring expression generation.",
      "authors": [
        "Kim JJ",
        "Lee DG",
        "Wu J",
        "Jung HG",
        "Lee SW"
      ],
      "abstract": "Visual question answering requires a deep understanding of both images and natural language. However, most methods mainly focus on visual concept; such as the relationships between various objects. The limited use of object categories combined with their relationships or simple question embedding is insufficient for representing complex scenes and explaining decisions. To address this limitation, we propose the use of text expressions generated for images, because such expressions have few structural constraints and can provide richer descriptions of images. The generated expressions can be incorporated with visual features and question embedding to obtain the question-relevant answer. A joint-embedding multi-head attention network is also proposed to model three different information modalities with co-attention. We quantitatively and qualitatively evaluated the proposed method on the VQA v2 dataset and compared it with state-of-the-art methods in terms of answer prediction. The quality of the generated expressions was also evaluated on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Experimental results demonstrate the effectiveness of the proposed method and reveal that it outperformed all of the competing methods in terms of both quantitative and qualitative results.",
      "doi": "https://doi.org/10.1016/j.neunet.2021.02.001",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question\n  Answering",
      "authors": [
        "Man Luo",
        "Yankai Zeng",
        "Pratyay Banerjee",
        "Chitta Baral"
      ],
      "abstract": "Knowledge-based visual question answering (VQA) requires answering questions\nwith external knowledge in addition to the content of images. One dataset that\nis mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold\nstandard knowledge corpus for retrieval. Existing work leverage different\nknowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge.\nBecause of varying knowledge bases, it is hard to fairly compare models'\nperformance. To address this issue, we collect a natural language knowledge\nbase that can be used for any VQA system. Moreover, we propose a Visual\nRetriever-Reader pipeline to approach knowledge-based VQA. The visual retriever\naims to retrieve relevant knowledge, and the visual reader seeks to predict\nanswers based on given knowledge. We introduce various ways to retrieve\nknowledge using text and images and two reader styles: classification and\nextraction. Both the retriever and reader are trained with weak supervision.\nOur experimental results show that a good retriever can significantly improve\nthe reader's performance on the OK-VQA challenge. The code and corpus are\nprovided in https://github.com/luomancs/retriever\\_reader\\_for\\_okvqa.git",
      "doi": "arXiv:2109.04014v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "A Comprehensive Survey of Knowledge-Based Vision Question Answering\n  Systems: The Lifecycle of Knowledge in Visual Reasoning Task",
      "authors": [
        "Jiaqi Deng",
        "Zonghan Wu",
        "Huan Huo",
        "Guandong Xu"
      ],
      "abstract": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications.",
      "doi": "arXiv:2504.17547v1",
      "year": 2025,
      "source": "arXiv"
    },
    {
      "title": "Visual Question Answering as Reading Comprehension",
      "authors": [
        "Hui Li",
        "Peng Wang",
        "Chunhua Shen",
        "Anton van den Hengel"
      ],
      "abstract": "Visual question answering (VQA) demands simultaneous comprehension of both\nthe image visual content and natural language questions. In some cases, the\nreasoning needs the help of common sense or general knowledge which usually\nappear in the form of text. Current methods jointly embed both the visual\ninformation and the textual feature into the same space. However, how to model\nthe complex interactions between the two different modalities is not an easy\ntask. In contrast to struggling on multimodal feature fusion, in this paper, we\npropose to unify all the input information by natural language so as to convert\nVQA into a machine reading comprehension problem. With this transformation, our\nmethod not only can tackle VQA datasets that focus on observation based\nquestions, but can also be naturally extended to handle knowledge-based VQA\nwhich requires to explore large-scale external knowledge base. It is a step\ntowards being able to exploit large volumes of text and natural language\nprocessing techniques to address VQA problem. Two types of models are proposed\nto deal with open-ended VQA and multiple-choice VQA respectively. We evaluate\nour models on three VQA benchmarks. The comparable performance with the\nstate-of-the-art demonstrates the effectiveness of the proposed method.",
      "doi": "arXiv:1811.11903v1",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual\n  Question Answering",
      "authors": [
        "Yuanze Lin",
        "Yujia Xie",
        "Dongdong Chen",
        "Yichong Xu",
        "Chenguang Zhu",
        "Lu Yuan"
      ],
      "abstract": "This paper revisits visual representation in knowledge-based visual question\nanswering (VQA) and demonstrates that using regional information in a better\nway can significantly improve the performance. While visual representation is\nextensively studied in traditional VQA, it is under-explored in knowledge-based\nVQA even though these two tasks share the common spirit, i.e., rely on visual\ninput to answer the question. Specifically, we observe that in most\nstate-of-the-art knowledge-based VQA methods: 1) visual features are extracted\neither from the whole image or in a sliding window manner for retrieving\nknowledge, and the important relationship within/among object regions is\nneglected; 2) visual features are not well utilized in the final answering\nmodel, which is counter-intuitive to some extent. Based on these observations,\nwe propose a new knowledge-based VQA method REVIVE, which tries to utilize the\nexplicit information of object regions not only in the knowledge retrieval\nstage but also in the answering model. The key motivation is that object\nregions and inherent relationship are important for knowledge-based VQA. We\nperform extensive experiments on the standard OK-VQA dataset and achieve new\nstate-of-the-art performance, i.e., 58.0% accuracy, surpassing previous\nstate-of-the-art method by a large margin (+3.6%). We also conduct detailed\nanalysis and show the necessity of regional information in different framework\ncomponents for knowledge-based VQA. Code is publicly available at\nhttps://github.com/yzleroy/REVIVE.",
      "doi": "arXiv:2206.01201v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Cross-Modal Knowledge Diffusion-Based Generation for Difference-Aware Medical VQA.",
      "authors": [
        "Lin Q",
        "He K",
        "Zhu Y",
        "Xu F",
        "Cambria E",
        "Feng M"
      ],
      "abstract": "Multimodal medical applications have garnered considerable attention due to their potential to offer comprehensive and robust support for medical assistance. Specifically, within this domain, difference-aware medical Visual Question Answering (VQA) has emerged as a topic of increasing interest that enables the recognition of changes in physical conditions over time when compared to previous states and provides customized suggestions accordingly. However, it is challenging because samples usually exhibit characteristics of complexity, diversity, and inherent noise. Besides, there is a need for multimodal knowledge understanding of the medical domain. The difference-aware setting requiring image comparison further intensifies these situations. To this end, we propose a cross-Modal knowlEdge diffusioN-baseD gEneration netwoRk (MENDER), where the diffusion mechanism with multi-step denoising and knowledge injection from global to local level are employed to tackle the aforementioned challenges, respectively. The diffusion process is to gradually generate answers with the sequence input of questions, random noises for the answer masks and virtual vision prompts of images. The strategy of answer nosing and knowledge cascading is specifically tailored for this task and is implemented during forward and reverse diffusion processes. Moreover, the visual and structure knowledge injection are proposed to learn virtual vision prompts to guide the diffusion process, where the former is realized using a pre-trained medical image-text network and the latter is modeled with spatial and semantic graph structures processed by the heterogeneous graph Transformer models. Experiment results demonstrate the effectiveness of MENDER for difference-aware medical VQA. Furthermore, it also exhibits notable performance in the low-resource setting and conventional medical VQA tasks.",
      "doi": "https://doi.org/10.1109/tip.2025.3558446",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "Advancing surgical VQA with scene graph knowledge.",
      "authors": [
        "Yuan K",
        "Kattel M",
        "Lavanchy JL",
        "Navab N",
        "Srivastav V",
        "Padoy N"
      ],
      "abstract": "The modern operating room is becoming increasingly complex, requiring innovative intra-operative support systems. While the focus of surgical data science has largely been on video analysis, integrating surgical computer vision with natural language capabilities is emerging as a necessity. Our work aims to advance visual question answering (VQA) in the surgical context with scene graph knowledge, addressing two main challenges in the current surgical VQA systems: removing question-condition bias in the surgical VQA dataset and incorporating scene-aware reasoning in the surgical VQA model design. First, we propose a surgical scene graph-based dataset, SSG-VQA, generated by employing segmentation and detection models on publicly available datasets. We build surgical scene graphs using spatial and action information of instruments and anatomies. These graphs are fed into a question engine, generating diverse QA pairs. We then propose SSG-VQA-Net, a novel surgical VQA model incorporating a lightweight Scene-embedded Interaction Module, which integrates geometric scene knowledge in the VQA model design by employing cross-attention between the textual and the scene features. Our comprehensive analysis shows that our SSG-VQA dataset provides a more complex, diverse, geometrically grounded, unbiased and surgical action-oriented dataset compared to existing surgical VQA datasets and SSG-VQA-Net outperforms existing methods across different question types and complexities. We highlight that the primary limitation in the current surgical VQA systems is the lack of scene knowledge to answer complex queries. We present a novel surgical VQA dataset and model and show that results can be significantly improved by incorporating geometric scene features in the VQA model design. We point out that the bottleneck of the current surgical visual question-answer model lies in learning the encoded representation rather than decoding the sequence. Our SSG-VQA dataset provides a diagnostic benchmark to test the scene understanding and reasoning capabilities of the model. The source code and the dataset will be made publicly available at: https://github.com/CAMMA-public/SSG-VQA .",
      "doi": "https://doi.org/10.1007/s11548-024-03141-y",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Rich Visual Knowledge-Based Augmentation Network for Visual Question Answering.",
      "authors": [
        "Zhang L",
        "Liu S",
        "Liu D",
        "Zeng P",
        "Li X",
        "Song J",
        "Gao L"
      ],
      "abstract": "Visual question answering (VQA) that involves understanding an image and paired questions develops very quickly with the boost of deep learning in relevant research fields, such as natural language processing and computer vision. Existing works highly rely on the knowledge of the data set. However, some questions require more professional cues other than the data set knowledge to answer questions correctly. To address such an issue, we propose a novel framework named a knowledge-based augmentation network (KAN) for VQA. We introduce object-related open-domain knowledge to assist the question answering. Concretely, we extract more visual information from images and introduce a knowledge graph to provide the necessary common sense or experience for the reasoning process. For these two augmented inputs, we design an attention module that can adjust itself according to the specific questions, such that the importance of external knowledge against detected objects can be balanced adaptively. Extensive experiments show that our KAN achieves state-of-the-art performance on three challenging VQA data sets, i.e., VQA v2, VQA-CP v2, and FVQA. In addition, our open-domain knowledge is also beneficial to VQA baselines. Code is available at https://github.com/yyyanglz/KAN.",
      "doi": "https://doi.org/10.1109/tnnls.2020.3017530",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "3D Question Answering.",
      "authors": [
        "Ye S",
        "Chen D",
        "Han S",
        "Liao J"
      ],
      "abstract": "Visual question answering (VQA) has experienced tremendous progress in recent years. However, most efforts have only focused on 2D image question-answering tasks. In this article, we extend VQA to its 3D counterpart, 3D question answering (3DQA), which can facilitate a machine's perception of 3D real-world scenarios. Unlike 2D image VQA, 3DQA takes the color point cloud as input and requires both appearance and 3D geometrical comprehension to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework \"3DQA-TR\", which consists of two encoders to exploit the appearance and geometry information, respectively. Finally, the multi-modal information about the appearance, geometry, and linguistic question can attend to each other via a 3D-linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset \"ScanQA\", which builds on the ScanNet dataset and contains over 10 K question-answer pairs for 806 scenes. To the best of our knowledge, ScanQA is the first large-scale dataset with natural-language questions and free-form answers in 3D environments that is fully human-annotated. We also use several visualizations and experiments to investigate the astonishing diversity of the collected questions and the significant differences between this task from 2D VQA and 3D captioning. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over state-of-the-art VQA frameworks and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate research in this direction. The code and data are available at http://shuquanye.com/3DQA_website/.",
      "doi": "https://doi.org/10.1109/tvcg.2022.3225327",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Exploring Human-like Attention Supervision in Visual Question Answering",
      "authors": [
        "Tingting Qiao",
        "Jianfeng Dong",
        "Duanqing Xu"
      ],
      "abstract": "Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA.",
      "doi": "arXiv:1709.06308v1",
      "year": 2017,
      "source": "arXiv"
    },
    {
      "title": "Dual Recurrent Attention Units for Visual Question Answering",
      "authors": [
        "Ahmed Osman",
        "Wojciech Samek"
      ],
      "abstract": "Visual Question Answering (VQA) requires AI models to comprehend data in two\ndomains, vision and text. Current state-of-the-art models use learned attention\nmechanisms to extract relevant information from the input domains to answer a\ncertain question. Thus, robust attention mechanisms are essential for powerful\nVQA models. In this paper, we propose a recurrent attention mechanism and show\nits benefits compared to the traditional convolutional approach. We perform two\nablation studies to evaluate recurrent attention. First, we introduce a\nbaseline VQA model with visual attention and test the performance difference\nbetween convolutional and recurrent attention on the VQA 2.0 dataset. Secondly,\nwe design an architecture for VQA which utilizes dual (textual and visual)\nRecurrent Attention Units (RAUs). Using this model, we show the effect of all\npossible combinations of recurrent and convolutional dual attention. Our single\nmodel outperforms the first place winner on the VQA 2016 challenge and to the\nbest of our knowledge, it is the second best performing single model on the VQA\n1.0 dataset. Furthermore, our model noticeably improves upon the winner of the\nVQA 2017 challenge. Moreover, we experiment replacing attention mechanisms in\nstate-of-the-art models with our RAUs and show increased performance.",
      "doi": "arXiv:1802.00209v3",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "Structured Triplet Learning with POS-tag Guided Attention for Visual\n  Question Answering",
      "authors": [
        "Zhe Wang",
        "Xiaoyi Liu",
        "Liangjian Chen",
        "Limin Wang",
        "Yu Qiao",
        "Xiaohui Xie",
        "Charless Fowlkes"
      ],
      "abstract": "Visual question answering (VQA) is of significant interest due to its\npotential to be a strong test of image understanding systems and to probe the\nconnection between language and vision. Despite much recent progress, general\nVQA is far from a solved problem. In this paper, we focus on the VQA\nmultiple-choice task, and provide some good practices for designing an\neffective VQA model that can capture language-vision interactions and perform\njoint reasoning. We explore mechanisms of incorporating part-of-speech (POS)\ntag guided attention, convolutional n-grams, triplet attention interactions\nbetween the image, question and candidate answer, and structured learning for\ntriplets based on image-question pairs. We evaluate our models on two popular\ndatasets: Visual7W and VQA Real Multiple Choice. Our final model achieves the\nstate-of-the-art performance of 68.2% on Visual7W, and a very competitive\nperformance of 69.6% on the test-standard split of VQA Real Multiple Choice.",
      "doi": "arXiv:1801.07853v1",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "Accuracy vs. Complexity: A Trade-off in Visual Question Answering Models",
      "authors": [
        "Moshiur R. Farazi",
        "Salman H. Khan",
        "Nick Barnes"
      ],
      "abstract": "Visual Question Answering (VQA) has emerged as a Visual Turing Test to\nvalidate the reasoning ability of AI agents. The pivot to existing VQA models\nis the joint embedding that is learned by combining the visual features from an\nimage and the semantic features from a given question. Consequently, a large\nbody of literature has focused on developing complex joint embedding strategies\ncoupled with visual attention mechanisms to effectively capture the interplay\nbetween these two modalities. However, modelling the visual and semantic\nfeatures in a high dimensional (joint embedding) space is computationally\nexpensive, and more complex models often result in trivial improvements in the\nVQA accuracy. In this work, we systematically study the trade-off between the\nmodel complexity and the performance on the VQA task. VQA models have a diverse\narchitecture comprising of pre-processing, feature extraction, multimodal\nfusion, attention and final classification stages. We specifically focus on the\neffect of \"multi-modal fusion\" in VQA models that is typically the most\nexpensive step in a VQA pipeline. Our thorough experimental evaluation leads us\nto two proposals, one optimized for minimal complexity and the other one\noptimized for state-of-the-art VQA performance.",
      "doi": "arXiv:2001.07059v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "MRA-Net: Improving VQA Via Multi-Modal Relation Attention Network.",
      "authors": [
        "Peng L",
        "Yang Y",
        "Wang Z",
        "Huang Z",
        "Shen HT"
      ],
      "abstract": "Visual Question Answering (VQA) is a task to answer natural language questions tied to the content of visual images. Most recent VQA approaches usually apply attention mechanism to focus on the relevant visual objects and/or consider the relations between objects via off-the-shelf methods in visual relation reasoning. However, they still suffer from several drawbacks. First, they mostly model the simple relations between objects, which results in many complicated questions cannot be answered correctly, because of failing to provide sufficient knowledge. Second, they seldom leverage the harmony cooperation of visual appearance feature and relation feature. To solve these problems, we propose a novel end-to-end VQA model, termed Multi-modal Relation Attention Network (MRA-Net). The proposed model explores both textual and visual relations to improve performance and interpretability. In specific, we devise 1) a self-guided word relation attention scheme, which explore the latent semantic relations between words; 2) two question-adaptive visual relation attention modules that can extract not only the fine-grained and precise binary relations between objects but also the more sophisticated trinary relations. Both kinds of question-related visual relations provide more and deeper visual semantics, thereby improving the visual reasoning ability of question answering. Furthermore, the proposed model also combines appearance feature with relation feature to reconcile the two types of features effectively. Extensive experiments on five large benchmark datasets, VQA-1.0, VQA-2.0, COCO-QA, VQA-CP v2, and TDIUC, demonstrate that our proposed model outperforms state-of-the-art approaches.",
      "doi": "https://doi.org/10.1109/tpami.2020.3004830",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "The multi-modal fusion in visual question answering: a review of attention mechanisms.",
      "authors": [
        "Lu S",
        "Liu M",
        "Yin L",
        "Yin Z",
        "Liu X",
        "Zheng W"
      ],
      "abstract": "Visual Question Answering (VQA) is a significant cross-disciplinary issue in the fields of computer vision and natural language processing that requires a computer to output a natural language answer based on pictures and questions posed based on the pictures. This requires simultaneous processing of multimodal fusion of text features and visual features, and the key task that can ensure its success is the attention mechanism. Bringing in attention mechanisms makes it better to integrate text features and image features into a compact multi-modal representation. Therefore, it is necessary to clarify the development status of attention mechanism, understand the most advanced attention mechanism methods, and look forward to its future development direction. In this article, we first conduct a bibliometric analysis of the correlation through CiteSpace, then we find and reasonably speculate that the attention mechanism has great development potential in cross-modal retrieval. Secondly, we discuss the classification and application of existing attention mechanisms in VQA tasks, analysis their shortcomings, and summarize current improvement methods. Finally, through the continuous exploration of attention mechanisms, we believe that VQA will evolve in a smarter and more human direction.",
      "doi": "https://doi.org/10.7717/peerj-cs.1400",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Histopathology in focus: a review on explainable multi-modal approaches for breast cancer diagnosis.",
      "authors": [
        "Abdullakutty F",
        "Akbari Y",
        "Al-Maadeed S",
        "Bouridane A",
        "Talaat IM",
        "Hamoudi R"
      ],
      "abstract": "Precision and timeliness in breast cancer detection are paramount for improving patient outcomes. Traditional diagnostic methods have predominantly relied on unimodal approaches, but recent advancements in medical data analytics have enabled the integration of diverse data sources beyond conventional imaging techniques. This review critically examines the transformative potential of integrating histopathology images with genomic data, clinical records, and patient histories to enhance diagnostic accuracy and comprehensiveness in multi-modal diagnostic techniques. It explores early, intermediate, and late fusion methods, as well as advanced deep multimodal fusion techniques, including encoder-decoder architectures, attention-based mechanisms, and graph neural networks. An overview of recent advancements in multimodal tasks such as Visual Question Answering (VQA), report generation, semantic segmentation, and cross-modal retrieval is provided, highlighting the utilization of generative AI and visual language models. Additionally, the review delves into the role of Explainable Artificial Intelligence (XAI) in elucidating the decision-making processes of sophisticated diagnostic algorithms, emphasizing the critical need for transparency and interpretability. By showcasing the importance of explainability, we demonstrate how XAI methods, including Grad-CAM, SHAP, LIME, trainable attention, and image captioning, enhance diagnostic precision, strengthen clinician confidence, and foster patient engagement. The review also discusses the latest XAI developments, such as X-VARs, LeGrad, LangXAI, LVLM-Interpret, and ex-ILP, to demonstrate their potential utility in multimodal breast cancer detection, while identifying key research gaps and proposing future directions for advancing the field.",
      "doi": "https://doi.org/10.3389/fmed.2024.1450103",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA",
      "authors": [
        "Badri Patro",
        "Anupriy",
        "Vinay Namboodiri"
      ],
      "abstract": "<jats:p>In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. Visualization of the results also confirms our hypothesis that attention maps improve using this form of supervision.</jats:p>",
      "doi": "https://doi.org/10.1609/aaai.v34i07.6858",
      "year": 2020,
      "source": "Crossref"
    },
    {
      "title": "Can Open Domain Question Answering Systems Answer Visual Knowledge\n  Questions?",
      "authors": [
        "Jiawen Zhang",
        "Abhijit Mishra",
        "Avinesh P. V. S",
        "Siddharth Patwardhan",
        "Sachin Agarwal"
      ],
      "abstract": "The task of Outside Knowledge Visual Question Answering (OKVQA) requires an\nautomatic system to answer natural language questions about pictures and images\nusing external knowledge. We observe that many visual questions, which contain\ndeictic referential phrases referring to entities in the image, can be\nrewritten as \"non-grounded\" questions and can be answered by existing\ntext-based question answering systems. This allows for the reuse of existing\ntext-based Open Domain Question Answering (QA) Systems for visual question\nanswering. In this work, we propose a potentially data-efficient approach that\nreuses existing systems for (a) image analysis, (b) question rewriting, and (c)\ntext-based question answering to answer such visual questions. Given an image\nand a question pertaining to that image (a visual question), we first extract\nthe entities present in the image using pre-trained object and scene\nclassifiers. Using these detected entities, the visual questions can be\nrewritten so as to be answerable by open domain QA systems. We explore two\nrewriting strategies: (1) an unsupervised method using BERT for masking and\nrewriting, and (2) a weakly supervised approach that combines adaptive\nrewriting and reinforcement learning techniques to use the implicit feedback\nfrom the QA system. We test our strategies on the publicly available OKVQA\ndataset and obtain a competitive performance with state-of-the-art models while\nusing only 10% of the training data.",
      "doi": "arXiv:2202.04306v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Co-VQA : Answering by Interactive Sub Question Sequence",
      "authors": [
        "Ruonan Wang",
        "Yuxi Qian",
        "Fangxiang Feng",
        "Xiaojie Wang",
        "Huixing Jiang"
      ],
      "abstract": "Most existing approaches to Visual Question Answering (VQA) answer questions\ndirectly, however, people usually decompose a complex question into a sequence\nof simple sub questions and finally obtain the answer to the original question\nafter answering the sub question sequence(SQS). By simulating the process, this\npaper proposes a conversation-based VQA (Co-VQA) framework, which consists of\nthree components: Questioner, Oracle, and Answerer. Questioner raises the sub\nquestions using an extending HRED model, and Oracle answers them one-by-one. An\nAdaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed,\nwhere the question-answer pair is used to update the visual representation\nsequentially. To perform supervised learning for each model, we introduce a\nwell-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2\ndatasets. Experimental results show that our method achieves state-of-the-art\non VQA-CP v2. Further analyses show that SQSs help build direct semantic\nconnections between questions and images, provide question-adaptive\nvariable-length reasoning chains, and with explicit interpretability as well as\nerror traceability.",
      "doi": "arXiv:2204.00879v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Bridge to Answer: Structure-aware Graph Interaction Network for Video\n  Question Answering",
      "authors": [
        "Jungin Park",
        "Jiyoung Lee",
        "Kwanghoon Sohn"
      ],
      "abstract": "This paper presents a novel method, termed Bridge to Answer, to infer correct\nanswers for questions about a given video by leveraging adequate graph\ninteractions of heterogeneous crossmodal graphs. To realize this, we learn\nquestion conditioned visual graphs by exploiting the relation between video and\nquestion to enable each visual node using question-to-visual interactions to\nencompass both visual and linguistic cues. In addition, we propose bridged\nvisual-to-visual interactions to incorporate two complementary visual\ninformation on appearance and motion by placing the question graph as an\nintermediate bridge. This bridged architecture allows reliable message passing\nthrough compositional semantics of the question to generate an appropriate\nanswer. As a result, our method can learn the question conditioned visual\nrepresentations attributed to appearance and motion that show powerful\ncapability for video question answering. Extensive experiments prove that the\nproposed method provides effective and superior performance than\nstate-of-the-art methods on several benchmarks.",
      "doi": "arXiv:2104.14085v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Focal Visual-Text Attention for Visual Question Answering",
      "authors": [
        "Junwei Liang",
        "Lu Jiang",
        "Liangliang Cao",
        "Li-Jia Li",
        "Alexander Hauptmann"
      ],
      "abstract": "Recent insights on language and vision with neural networks have been\nsuccessfully applied to simple single-image visual question answering. However,\nto tackle real-life question answering problems on multimedia collections such\nas personal photos, we have to look at whole collections with sequences of\nphotos or videos. When answering questions from a large collection, a natural\nproblem is to identify snippets to support the answer. In this paper, we\ndescribe a novel neural network called Focal Visual-Text Attention network\n(FVTA) for collective reasoning in visual question answering, where both visual\nand text sequence information such as images and text metadata are presented.\nFVTA introduces an end-to-end approach that makes use of a hierarchical process\nto dynamically determine what media and what time to focus on in the sequential\ndata to answer the question. FVTA can not only answer the questions well but\nalso provides the justifications which the system results are based upon to get\nthe answers. FVTA achieves state-of-the-art performance on the MemexQA dataset\nand competitive results on the MovieQA dataset.",
      "doi": "https://doi.org/10.1109/TPAMI.2018.2890628",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "Medical visual question answering based on question-type reasoning and semantic space constraint.",
      "authors": [
        "Wang M",
        "He X",
        "Liu L",
        "Qing L",
        "Chen H",
        "Liu Y",
        "Ren C"
      ],
      "abstract": "Medical visual question answering (Med-VQA) aims to accurately answer clinical questions about medical images. Despite its enormous potential for application in the medical domain, the current technology is still in its infancy. Compared with general visual question answering task, Med-VQA task involve more demanding challenges. First, clinical questions about medical images are usually diverse due to different clinicians and the complexity of diseases. Consequently, noise is inevitably introduced when extracting question features. Second, Med-VQA task have always been regarded as a classification problem for predefined answers, ignoring the relationships between candidate responses. Thus, the Med-VQA model pays equal attention to all candidate answers when predicting answers. In this paper, a novel Med-VQA framework is proposed to alleviate the above-mentioned problems. Specifically, we employed a question-type reasoning module severally to closed-ended and open-ended questions, thereby extracting the important information contained in the questions through an attention mechanism and filtering the noise to extract more valuable question features. To take advantage of the relational information between answers, we designed a semantic constraint space to calculate the similarity between the answers and assign higher attention to answers with high correlation. To evaluate the effectiveness of the proposed method, extensive experiments were conducted on a public dataset, namely VQA-RAD. Experimental results showed that the proposed method achieved better performance compared to other the state-of-the-art methods. The overall accuracy, closed-ended accuracy, and open-ended accuracy reached 74.1 %, 82.7 %, and 60.9 %, respectively. It is worth noting that the absolute accuracy of the proposed method improved by 5.5 % for closed-ended questions.",
      "doi": "https://doi.org/10.1016/j.artmed.2022.102346",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Multitask Learning for Visual Question Answering.",
      "authors": [
        "Ma J",
        "Liu J",
        "Lin Q",
        "Wu B",
        "Wang Y",
        "You Y"
      ],
      "abstract": "Visual question answering (VQA) is a task that machines should provide an accurate natural language answer given an image and a question about the image. Many studies have found that the current VQA methods are heavily driven by the surface correlation or statistical bias in the training data, and lack sufficient image grounding. To address this issue, we devise a novel end-to-end architecture that uses multitask learning to promote more sufficient image grounding and learn effective multimodality representations. The tasks consist of VQA and our proposed image cloze (IC) task requires machines to fill in the blanks accurately given an image and a textual description of the image. To ensure our model performs sufficient image grounding as much as possible, we propose a novel word-masking algorithm to develop the multimodal IC task based on the part-of-speech of words. Our model predicts the VQA answer and fills in the blanks after the multimodality representation learning that is shared by the two tasks. Experimental results show that our model achieves almost the equivalent, state-of-the-art, second-best performance on the VQA v2.0, VQA-changing priors (CP) v2, and grounded question answering (GQA) datasets, respectively, with fewer parameters and without additional data compared with baselines.",
      "doi": "https://doi.org/10.1109/tnnls.2021.3105284",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Interpretable Visual Question Answering by Reasoning on Dependency Trees.",
      "authors": [
        "Cao Q",
        "Liang X",
        "Li B",
        "Lin L"
      ],
      "abstract": "Collaborative reasoning for understanding image-question pairs is a very critical but underexplored topic in interpretable visual question answering systems. Although very recent studies have attempted to use explicit compositional processes to assemble multiple subtasks embedded in questions, their models heavily rely on annotations or handcrafted rules to obtain valid reasoning processes, which leads to either heavy workloads or poor performance on compositional reasoning. In this paper, to better align image and language domains in diverse and unrestricted cases, we propose a novel neural network model that performs global reasoning on a dependency tree parsed from the question; thus, our model is called a parse-tree-guided reasoning network (PTGRN). This network consists of three collaborative modules: i) an attention module that exploits the local visual evidence of each word parsed from the question, ii) a gated residual composition module that composes the previously mined evidence, and iii) a parse-tree-guided propagation module that passes the mined evidence along the parse tree. Thus, PTGRN is capable of building an interpretable visual question answering (VQA) system that gradually derives image cues following question-driven parse-tree reasoning. Experiments on relational datasets demonstrate the superiority of PTGRN over current state-of-the-art VQA methods, and the visualization results highlight the explainable capability of our reasoning system.",
      "doi": "https://doi.org/10.1109/tpami.2019.2943456",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Vision-Language Model for Visual Question Answering in Medical Imagery.",
      "authors": [
        "Bazi Y",
        "Rahhal MMA",
        "Bashmal L",
        "Zuair M"
      ],
      "abstract": "In the clinical and healthcare domains, medical images play a critical role. A mature medical visual question answering system (VQA) can improve diagnosis by answering clinical questions presented with a medical image. Despite its enormous potential in the healthcare industry and services, this technology is still in its infancy and is far from practical use. This paper introduces an approach based on a transformer encoder-decoder architecture. Specifically, we extract image features using the vision transformer (ViT) model, and we embed the question using a textual encoder transformer. Then, we concatenate the resulting visual and textual representations and feed them into a multi-modal decoder for generating the answer in an autoregressive way. In the experiments, we validate the proposed model on two VQA datasets for radiology images termed VQA-RAD and PathVQA. The model shows promising results compared to existing solutions. It yields closed and open accuracies of 84.99% and 72.97%, respectively, for VQA-RAD, and 83.86% and 62.37%, respectively, for PathVQA. Other metrics such as the BLUE score showing the alignment between the predicted and true answer sentences are also reported.",
      "doi": "https://doi.org/10.3390/bioengineering10030380",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Fine-grained Late-interaction Multi-modal Retrieval for Retrieval\n  Augmented Visual Question Answering",
      "authors": [
        "Weizhe Lin",
        "Jinghong Chen",
        "Jingbiao Mei",
        "Alexandru Coca",
        "Bill Byrne"
      ],
      "abstract": "Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to\nutilize knowledge from external knowledge bases to answer visually-grounded\nquestions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong\nframework to tackle KB-VQA, first retrieves related documents with Dense\nPassage Retrieval (DPR) and then uses them to answer questions. This paper\nproposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which\nsignificantly improves knowledge retrieval in RA-VQA. FLMR addresses two major\nlimitations in RA-VQA's retriever: (1) the image representations obtained via\nimage-to-text transforms can be incomplete and inaccurate and (2) relevance\nscores between queries and documents are computed with one-dimensional\nembeddings, which can be insensitive to finer-grained relevance. FLMR overcomes\nthese limitations by obtaining image representations that complement those from\nthe image-to-text transforms using a vision model aligned with an existing\ntext-based retriever through a simple alignment network. FLMR also encodes\nimages and questions using multi-dimensional embeddings to capture\nfiner-grained relevance between queries and documents. FLMR significantly\nimproves the original RA-VQA retriever's PRRecall@5 by approximately 8\\%.\nFinally, we equipped RA-VQA with two state-of-the-art large\nmulti-modal/language models to achieve $\\sim61\\%$ VQA score in the OK-VQA\ndataset.",
      "doi": "arXiv:2309.17133v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External\n  Knowledge",
      "authors": [
        "Kenneth Marino",
        "Mohammad Rastegari",
        "Ali Farhadi",
        "Roozbeh Mottaghi"
      ],
      "abstract": "Visual Question Answering (VQA) in its ideal form lets us study reasoning in\nthe joint space of vision and language and serves as a proxy for the AI task of\nscene understanding. However, most VQA benchmarks to date are focused on\nquestions such as simple counting, visual attributes, and object detection that\ndo not require reasoning or knowledge beyond what is in the image. In this\npaper, we address the task of knowledge-based visual question answering and\nprovide a benchmark, called OK-VQA, where the image content is not sufficient\nto answer the questions, encouraging methods that rely on external knowledge\nresources. Our new dataset includes more than 14,000 questions that require\nexternal knowledge to answer. We show that the performance of the\nstate-of-the-art VQA models degrades drastically in this new setting. Our\nanalysis shows that our knowledge-based VQA task is diverse, difficult, and\nlarge compared to previous knowledge-based VQA datasets. We hope that this\ndataset enables researchers to open up new avenues for research in this domain.\nSee http://okvqa.allenai.org to download and browse the dataset.",
      "doi": "arXiv:1906.00067v2",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via\n  Graph Representation Pretraining",
      "authors": [
        "Minjun Kim",
        "Seungwoo Song",
        "Youhan Lee",
        "Haneol Jang",
        "Kyungtae Lim"
      ],
      "abstract": "The current research direction in generative models, such as the recently\ndeveloped GPT4, aims to find relevant knowledge information for multimodal and\nmultilingual inputs to provide answers. Under these research circumstances, the\ndemand for multilingual evaluation of visual question answering (VQA) tasks, a\nrepresentative task of multimodal systems, has increased. Accordingly, we\npropose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that\ncan be extended to multilingualism. The proposed data include 17K images, 17K\nquestion-answer pairs for both Korean and English and 280K instances of\nknowledge information related to question-answer content. We also present a\nframework that can effectively inject knowledge information into a VQA system\nby pretraining the knowledge information of BOK-VQA data in the form of graph\nembeddings. Finally, through in-depth analysis, we demonstrated the actual\neffect of the knowledge information contained in the constructed training data\non VQA.",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29798",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual\n  Question Answering",
      "authors": [
        "Jialin Wu",
        "Raymond J. Mooney"
      ],
      "abstract": "Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a\ntwo-stage framework that first retrieves external knowledge given the visual\nquestion and then predicts the answer based on the retrieved content. However,\nthe retrieved knowledge is often inadequate. Retrievals are frequently too\ngeneral and fail to cover specific knowledge needed to answer the question.\nAlso, the naturally available supervision (whether the passage contains the\ncorrect answer) is weak and does not guarantee question relevancy. To address\nthese issues, we propose an Entity-Focused Retrieval (EnFoRe) model that\nprovides stronger supervision during training and recognizes question-relevant\nentities to help retrieve more specific knowledge. Experiments show that our\nEnFoRe model achieves superior retrieval performance on OK-VQA, the currently\nlargest outside-knowledge VQA dataset. We also combine the retrieved knowledge\nwith state-of-the-art VQA models, and achieve a new state-of-the-art\nperformance on OK-VQA.",
      "doi": "arXiv:2210.10176v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Interpretable medical image Visual Question Answering via multi-modal relationship graph learning.",
      "authors": [
        "Hu X",
        "Gu L",
        "Kobayashi K",
        "Liu L",
        "Zhang M",
        "Harada T",
        "Summers RM",
        "Zhu Y"
      ],
      "abstract": "Medical Visual Question Answering (VQA) is an important task in medical multi-modal Large Language Models (LLMs), aiming to answer clinically relevant questions regarding input medical images. This technique has the potential to improve the efficiency of medical professionals while relieving the burden on the public health system, particularly in resource-poor countries. However, existing medical VQA datasets are small and only contain simple questions (equivalent to classification tasks), which lack semantic reasoning and clinical knowledge. Our previous work proposed a clinical knowledge-driven image difference VQA benchmark using a rule-based approach (Hu et al., 2023). However, given the same breadth of information coverage, the rule-based approach shows an 85% error rate on extracted labels. We trained an LLM method to extract labels with 62% increased accuracy. We also comprehensively evaluated our labels with 2 clinical experts on 100 samples to help us fine-tune the LLM. Based on the trained LLM model, we proposed a large-scale medical VQA dataset, Medical-CXR-VQA, using LLMs focused on chest X-ray images. The questions involved detailed information, such as abnormalities, locations, levels, and types. Based on this dataset, we proposed a novel VQA method by constructing three different relationship graphs: spatial relationships, semantic relationships, and implicit relationship graphs on the image regions, questions, and semantic labels. We leveraged graph attention to learn the logical reasoning paths for different questions. These learned graph VQA reasoning paths can be further used for LLM prompt engineering and chain-of-thought, which are crucial for further fine-tuning and training multi-modal large language models. Moreover, we demonstrate that our approach has the qualities of evidence and faithfulness, which are crucial in the clinical field. The code and the dataset is available at https://github.com/Holipori/Medical-CXR-VQA.",
      "doi": "https://doi.org/10.1016/j.media.2024.103279",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "SPAN: Learning Similarity between Scene Graphs and Images with\n  Transformers",
      "authors": [
        "Yuren Cong",
        "Wentong Liao",
        "Bodo Rosenhahn",
        "Michael Ying Yang"
      ],
      "abstract": "Learning similarity between scene graphs and images aims to estimate a\nsimilarity score given a scene graph and an image. There is currently no\nresearch dedicated to this task, although it is critical for scene graph\ngeneration and downstream applications. Scene graph generation is\nconventionally evaluated by Recall$@K$ and mean Recall$@K$, which measure the\nratio of predicted triplets that appear in the human-labeled triplet set.\nHowever, such triplet-oriented metrics fail to demonstrate the overall semantic\ndifference between a scene graph and an image and are sensitive to annotation\nbias and noise. Using generated scene graphs in the downstream applications is\ntherefore limited. To address this issue, for the first time, we propose a\nScene graPh-imAge coNtrastive learning framework, SPAN, that can measure the\nsimilarity between scene graphs and images. Our novel framework consists of a\ngraph Transformer and an image Transformer to align scene graphs and their\ncorresponding images in the shared latent space. We introduce a novel graph\nserialization technique that transforms a scene graph into a sequence with\nstructural encodings. Based on our framework, we propose R-Precision measuring\nimage retrieval accuracy as a new evaluation metric for scene graph generation.\nWe establish new benchmarks on the Visual Genome and Open Images datasets.\nExtensive experiments are conducted to verify the effectiveness of SPAN, which\nshows great potential as a scene graph encoder.",
      "doi": "arXiv:2304.00590v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions",
      "authors": [
        "Johanna Wald",
        "Helisa Dhamo",
        "Nassir Navab",
        "Federico Tombari"
      ],
      "abstract": "Scene understanding has been of high interest in computer vision. It\nencompasses not only identifying objects in a scene, but also their\nrelationships within the given context. With this goal, a recent line of works\ntackles 3D semantic segmentation and scene layout prediction. In our work we\nfocus on scene graphs, a data structure that organizes the entities of a scene\nin a graph, where objects are nodes and their relationships modeled as edges.\nWe leverage inference on scene graphs as a way to carry out 3D scene\nunderstanding, mapping objects and their relationships. In particular, we\npropose a learned method that regresses a scene graph from the point cloud of a\nscene. Our novel architecture is based on PointNet and Graph Convolutional\nNetworks (GCN). In addition, we introduce 3DSSG, a semi-automatically generated\ndataset, that contains semantically rich scene graphs of 3D scenes. We show the\napplication of our method in a domain-agnostic retrieval task, where graphs\nserve as an intermediate representation for 3D-3D and 2D-3D matching.",
      "doi": "arXiv:2004.03967v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Scene Graph Generation With Hierarchical Context.",
      "authors": [
        "Ren G",
        "Ren L",
        "Liao Y",
        "Liu S",
        "Li B",
        "Han J",
        "Yan S"
      ],
      "abstract": "Scene graph generation has received increasing attention in recent years. Enhancing the predicate representations is an important entry point to this task. There are various methods to fully investigate the context of representation enhancement. In this brief, we analyze the decisive factors that can significantly affect the relation detection results. Our analysis shows that spatial correlations between objects, focused regions of objects, and global hints related to the relations have strong influences in relation prediction and contradiction elimination. Based on our analysis, we propose a hierarchical context network (HCNet) to generate a scene graph. HCNet consists of three contexts, including interaction context, depression context, and global context, which integrates information from pair, object, and graph levels. The experiments show that our method outperforms the state-of-the-art methods on the Visual Genome (VG) data set.",
      "doi": "https://doi.org/10.1109/tnnls.2020.2979270",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Unbiased Scene Graph Generation via Two-Stage Causal Modeling.",
      "authors": [
        "Sun S",
        "Zhi S",
        "Liao Q",
        "Heikkila J",
        "Liu L"
      ],
      "abstract": "Despite the impressive performance of recent unbiased Scene Graph Generation (SGG) methods, the current debiasing literature mainly focuses on the long-tailed distribution problem, whereas it overlooks another source of bias, i.e., semantic confusion, which makes the SGG model prone to yield false predictions for similar relationships. In this paper, we explore a debiasing procedure for the SGG task leveraging causal inference. Our central insight is that the Sparse Mechanism Shift (SMS) in causality allows independent intervention on multiple biases, thereby potentially preserving head category performance while pursuing the prediction of high-informative tail relationships. However, the noisy datasets lead to unobserved confounders for the SGG task, and thus the constructed causal models are always causal-insufficient to benefit from SMS. To remedy this, we propose Two-stage Causal Modeling (TsCM) for the SGG task, which takes the long-tailed distribution and semantic confusion as confounders to the Structural Causal Model (SCM) and then decouples the causal intervention into two stages. The first stage is causal representation learning, where we use a novel Population Loss (P-Loss) to intervene in the semantic confusion confounder. The second stage introduces the Adaptive Logit Adjustment (AL-Adjustment) to eliminate the long-tailed distribution confounder to complete causal calibration learning. These two stages are model agnostic and thus can be used in any SGG model that seeks unbiased predictions. Comprehensive experiments conducted on the popular SGG backbones and benchmarks show that our TsCM can achieve state-of-the-art performance in terms of mean recall rate. Furthermore, TsCM can maintain a higher recall rate than other debiasing methods, which indicates that our method can achieve a better tradeoff between head and tail relationships.",
      "doi": "https://doi.org/10.1109/tpami.2023.3285009",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "RelTR: Relation Transformer for Scene Graph Generation.",
      "authors": [
        "Cong Y",
        "Yang MY",
        "Rosenhahn B"
      ],
      "abstract": "Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by Detection Transformer, which excels in object detection, we view scene graph generation as a set prediction problem. In this article, we propose an end-to-end scene graph generation model Relation Transformer (RelTR), which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a fixed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, RelTR is a one-stage method that predicts sparse scene graphs directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome, Open Images V6, and VRD datasets demonstrate the superior performance and fast inference of our model.",
      "doi": "https://doi.org/10.1109/tpami.2023.3268066",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "SGT++: Improved Scene Graph-Guided Transformer for Surgical Report Generation.",
      "authors": [
        "Lin C",
        "Zhu Z",
        "Zhao Y",
        "Zhang Y",
        "He K",
        "Zhao Y"
      ],
      "abstract": "Automatically recording surgical procedures and generating surgical reports are crucial for alleviating surgeons' workload and enabling them to concentrate more on the operations. Despite some achievements, there still exist several issues for the previous works: 1) failure to model the interactive relationship between surgical instruments and tissue; and 2) neglect of fine-grained differences within different surgical images in the same surgery. To address these two issues, we propose an improved scene graph-guided Transformer, also named by SGT++, to generate more accurate surgical report, in which the complex interactions between surgical instruments and tissue are learnt from both explicit and implicit perspectives. Specifically, to facilitate the understanding of the surgical scene graph under a graph learning framework, a simple yet effective approach is proposed for homogenizing the input heterogeneous scene graph. For the homogeneous scene graph that contains explicit structured and fine-grained semantic relationships, we design an attention-induced graph transformer for node aggregation via an explicit relation-aware encoder. In addition, to characterize the implicit relationships about the instrument, tissue, and the interaction between them, the implicit relational attention is proposed to take full advantage of the prior knowledge from the interactional prototype memory. With the learnt explicit and implicit relation-aware representations, they are then coalesced to obtain the fused relation-aware representations contributing to generating reports. Some comprehensive experiments on two surgical datasets show that the proposed STG++ model achieves state-of-the-art results.",
      "doi": "https://doi.org/10.1109/tmi.2023.3335909",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Hyper-relationship Learning Network for Scene Graph Generation",
      "authors": [
        "Yibing Zhan",
        "Zhi Chen",
        "Jun Yu",
        "BaoSheng Yu",
        "Dacheng Tao",
        "Yong Luo"
      ],
      "abstract": "Generating informative scene graphs from images requires integrating and\nreasoning from various graph components, i.e., objects and relationships.\nHowever, current scene graph generation (SGG) methods, including the unbiased\nSGG methods, still struggle to predict informative relationships due to the\nlack of 1) high-level inference such as transitive inference between\nrelationships and 2) efficient mechanisms that can incorporate all interactions\nof graph components. To address the issues mentioned above, we devise a\nhyper-relationship learning network, termed HLN, for SGG. Specifically, the\nproposed HLN stems from hypergraphs and two graph attention networks (GATs) are\ndesigned to infer relationships: 1) the object-relationship GAT or OR-GAT to\nexplore interactions between objects and relationships, and 2) the\nhyper-relationship GAT or HR-GAT to integrate transitive inference of\nhyper-relationships, i.e., the sequential relationships between three objects\nfor transitive reasoning. As a result, HLN significantly improves the\nperformance of scene graph generation by integrating and reasoning from object\ninteractions, relationship interactions, and transitive inference of\nhyper-relationships. We evaluate HLN on the most popular SGG dataset, i.e., the\nVisual Genome dataset, and the experimental results demonstrate its great\nsuperiority over recent state-of-the-art methods. For example, the proposed HLN\nimproves the recall per relationship from 11.3\\% to 13.1\\%, and maintains the\nrecall per image from 19.8\\% to 34.9\\%. We will release the source code and\npretrained models on GitHub.",
      "doi": "arXiv:2202.07271v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection",
      "authors": [
        "Tim Salzmann",
        "Markus Ryll",
        "Alex Bewley",
        "Matthias Minderer"
      ],
      "abstract": "Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nablations, real-world qualitative examples, and analyses of zero-shot\nperformance.",
      "doi": "arXiv:2403.14270v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "An End-to-End Network for Generating Social Relationship Graphs",
      "authors": [
        "Arushi Goel",
        "Keng Teck Ma",
        "Cheston Tan"
      ],
      "abstract": "Socially-intelligent agents are of growing interest in artificial\nintelligence. To this end, we need systems that can understand social\nrelationships in diverse social contexts. Inferring the social context in a\ngiven visual scene not only involves recognizing objects, but also demands a\nmore in-depth understanding of the relationships and attributes of the people\ninvolved. To achieve this, one computational approach for representing human\nrelationships and attributes is to use an explicit knowledge graph, which\nallows for high-level reasoning. We introduce a novel end-to-end-trainable\nneural network that is capable of generating a Social Relationship Graph - a\nstructured, unified representation of social relationships and attributes -\nfrom a given input image. Our Social Relationship Graph Generation Network\n(SRG-GN) is the first to use memory cells like Gated Recurrent Units (GRUs) to\niteratively update the social relationship states in a graph using scene and\nattribute context. The neural network exploits the recurrent connections among\nthe GRUs to implement message passing between nodes and edges in the graph, and\nresults in significant improvement over previous methods for social\nrelationship recognition.",
      "doi": "arXiv:1903.09784v1",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Intrinsic Relationship Reasoning for Small Object Detection",
      "authors": [
        "Kui Fu",
        "Jia Li",
        "Lin Ma",
        "Kai Mu",
        "Yonghong Tian"
      ],
      "abstract": "The small objects in images and videos are usually not independent\nindividuals. Instead, they more or less present some semantic and spatial\nlayout relationships with each other. Modeling and inferring such intrinsic\nrelationships can thereby be beneficial for small object detection. In this\npaper, we propose a novel context reasoning approach for small object detection\nwhich models and infers the intrinsic semantic and spatial layout relationships\nbetween objects. Specifically, we first construct a semantic module to model\nthe sparse semantic relationships based on the initial regional features, and a\nspatial layout module to model the sparse spatial layout relationships based on\ntheir position and shape information, respectively. Both of them are then fed\ninto a context reasoning module for integrating the contextual information with\nrespect to the objects and their relationships, which is further fused with the\noriginal regional visual features for classification and regression.\nExperimental results reveal that the proposed approach can effectively boost\nthe small object detection performance.",
      "doi": "arXiv:2009.00833v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Understanding visual hallucinations: A new synthesis.",
      "authors": [
        "Collerton D",
        "Barnes J",
        "Diederich NJ",
        "Dudley R",
        "Ffytche D",
        "Friston K",
        "Goetz CG",
        "Goldman JG",
        "Jardri R",
        "Kulisevsky J",
        "Lewis SJG",
        "Nara S",
        "O'Callaghan C",
        "Onofrj M",
        "Pagonabarraga J",
        "Parr T",
        "Shine JM",
        "Stebbins G",
        "Taylor JP",
        "Tsuda I",
        "Weil RS"
      ],
      "abstract": "Despite decades of research, we do not definitively know how people sometimes see things that are not there. Eight models of complex visual hallucinations have been published since 2000, including Deafferentation, Reality Monitoring, Perception and Attention Deficit, Activation, Input, and Modulation, Hodological, Attentional Networks, Active Inference, and Thalamocortical Dysrhythmia Default Mode Network Decoupling. Each was derived from different understandings of brain organisation. To reduce this variability, representatives from each research group agreed an integrated Visual Hallucination Framework that is consistent with current theories of veridical and hallucinatory vision. The Framework delineates cognitive systems relevant to hallucinations. It allows a systematic, consistent, investigation of relationships between the phenomenology of visual hallucinations and changes in underpinning cognitive structures. The episodic nature of hallucinations highlights separate factors associated with the onset, persistence, and end of specific hallucinations suggesting a complex relationship between state and trait markers of hallucination risk. In addition to a harmonised interpretation of existing evidence, the Framework highlights new avenues of research, and potentially, new approaches to treating distressing hallucinations.",
      "doi": "https://doi.org/10.1016/j.neubiorev.2023.105208",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Multi-tailed vision transformer for efficient inference.",
      "authors": [
        "Wang Y",
        "Du B",
        "Wang W",
        "Xu C"
      ],
      "abstract": "Recently, Vision Transformer (ViT) has achieved promising performance in image recognition and gradually serves as a powerful backbone in various vision tasks. To satisfy the sequential input of Transformer, the tail of ViT first splits each image into a sequence of visual tokens with a fixed length. Then, the following self-attention layers construct the global relationship between tokens to produce useful representation for the downstream tasks. Empirically, representing the image with more tokens leads to better performance, yet the quadratic computational complexity of self-attention layer to the number of tokens could seriously influence the efficiency of ViT's inference. For computational reduction, a few pruning methods progressively prune uninformative tokens in the Transformer encoder, while leaving the number of tokens before the Transformer untouched. In fact, fewer tokens as the input for the Transformer encoder can directly reduce the following computational cost. In this spirit, we propose a Multi-Tailed Vision Transformer (MT-ViT) in the paper. MT-ViT adopts multiple tails to produce visual sequences of different lengths for the following Transformer encoder. A tail predictor is introduced to decide which tail is the most efficient for the image to produce accurate prediction. Both modules are optimized in an end-to-end fashion, with the Gumbel-Softmax trick. Experiments on ImageNet-1K demonstrate that MT-ViT can achieve a significant reduction on FLOPs with no degradation of the accuracy and outperform compared methods in both accuracy and FLOPs.",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106235",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "25th Annual Computational Neuroscience Meeting: CNS-2016.",
      "authors": [
        "Sharpee TO",
        "Destexhe A",
        "Kawato M",
        "Sekulić V",
        "Skinner FK",
        "Wójcik DK",
        "Chintaluri C",
        "Cserpán D",
        "Somogyvári Z",
        "Kim JK",
        "Kilpatrick ZP",
        "Bennett MR",
        "Josić K",
        "Elices I",
        "Arroyo D",
        "Levi R",
        "Rodriguez FB",
        "Varona P",
        "Hwang E",
        "Kim B",
        "Han HB",
        "Kim T",
        "McKenna JT",
        "Brown RE",
        "McCarley RW",
        "Choi JH",
        "Rankin J",
        "Popp PO",
        "Rinzel J",
        "Tabas A",
        "Rupp A",
        "Balaguer-Ballester E",
        "Maturana MI",
        "Grayden DB",
        "Cloherty SL",
        "Kameneva T",
        "Ibbotson MR",
        "Meffin H",
        "Koren V",
        "Lochmann T",
        "Dragoi V",
        "Obermayer K",
        "Psarrou M",
        "Schilstra M",
        "Davey N",
        "Torben-Nielsen B",
        "Steuber V",
        "Ju H",
        "Yu J",
        "Hines ML",
        "Chen L",
        "Yu Y",
        "Kim J",
        "Leahy W",
        "Shlizerman E",
        "Birgiolas J",
        "Gerkin RC",
        "Crook SM",
        "Viriyopase A",
        "Memmesheimer RM",
        "Gielen S",
        "Dabaghian Y",
        "DeVito J",
        "Perotti L",
        "Kim AJ",
        "Fenk LM",
        "Cheng C",
        "Maimon G",
        "Zhao C",
        "Widmer Y",
        "Sprecher S",
        "Senn W",
        "Halnes G",
        "Mäki-Marttunen T",
        "Keller D",
        "Pettersen KH",
        "Andreassen OA",
        "Einevoll GT",
        "Yamada Y",
        "Steyn-Ross ML",
        "Alistair Steyn-Ross D",
        "Mejias JF",
        "Murray JD",
        "Kennedy H",
        "Wang XJ",
        "Kruscha A",
        "Grewe J",
        "Benda J",
        "Lindner B",
        "Badel L",
        "Ohta K",
        "Tsuchimoto Y",
        "Kazama H",
        "Kahng B",
        "Tam ND",
        "Pollonini L",
        "Zouridakis G",
        "Soh J",
        "Kim D",
        "Yoo M",
        "Palmer SE",
        "Culmone V",
        "Bojak I",
        "Ferrario A",
        "Merrison-Hort R",
        "Borisyuk R",
        "Kim CS",
        "Tezuka T",
        "Joo P",
        "Rho YA",
        "Burton SD",
        "Bard Ermentrout G",
        "Jeong J",
        "Urban NN",
        "Marsalek P",
        "Kim HH",
        "Moon SH",
        "Lee DW",
        "Lee SB",
        "Lee JY",
        "Molkov YI",
        "Hamade K",
        "Teka W",
        "Barnett WH",
        "Kim T",
        "Markin S",
        "Rybak IA",
        "Forro C",
        "Dermutz H",
        "Demkó L",
        "Vörös J",
        "Babichev A",
        "Huang H",
        "Verduzco-Flores S",
        "Dos Santos F",
        "Andras P",
        "Metzner C",
        "Schweikard A",
        "Zurowski B",
        "Roach JP",
        "Sander LM",
        "Zochowski MR",
        "Skilling QM",
        "Ognjanovski N",
        "Aton SJ",
        "Zochowski M",
        "Wang SJ",
        "Ouyang G",
        "Guang J",
        "Zhang M",
        "Michael Wong KY",
        "Zhou C",
        "Robinson PA",
        "Sanz-Leon P",
        "Drysdale PM",
        "Fung F",
        "Abeysuriya RG",
        "Rennie CJ",
        "Zhao X",
        "Choe Y",
        "Yang HF",
        "Mi Y",
        "Lin X",
        "Wu S",
        "Liedtke J",
        "Schottdorf M",
        "Wolf F",
        "Yamamura Y",
        "Wickens JR",
        "Rumbell T",
        "Ramsey J",
        "Reyes A",
        "Draguljić D",
        "Hof PR",
        "Luebke J",
        "Weaver CM",
        "He H",
        "Yang X",
        "Ma H",
        "Xu Z",
        "Wang Y",
        "Baek K",
        "Morris LS",
        "Kundu P",
        "Voon V",
        "Agnes EJ",
        "Vogels TP",
        "Podlaski WF",
        "Giese M",
        "Kuravi P",
        "Vogels R",
        "Seeholzer A",
        "Podlaski W",
        "Ranjan R",
        "Vogels T",
        "Torres JJ",
        "Baroni F",
        "Latorre R",
        "Gips B",
        "Lowet E",
        "Roberts MJ",
        "de Weerd P",
        "Jensen O",
        "van der Eerden J",
        "Goodarzinick A",
        "Niry MD",
        "Valizadeh A",
        "Pariz A",
        "Parsi SS",
        "Warburton JM",
        "Marucci L",
        "Tamagnini F",
        "Brown J",
        "Tsaneva-Atanasova K",
        "Kleberg FI",
        "Triesch J",
        "Moezzi B",
        "Iannella N",
        "Schaworonkow N",
        "Plogmacher L",
        "Goldsworthy MR",
        "Hordacre B",
        "McDonnell MD",
        "Ridding MC",
        "Zapotocky M",
        "Smit D",
        "Fouquet C",
        "Trembleau A",
        "Dasgupta S",
        "Nishikawa I",
        "Aihara K",
        "Toyoizumi T",
        "Robb DT",
        "Mellen N",
        "Toporikova N",
        "Tang R",
        "Tang YY",
        "Liang G",
        "Kiser SA",
        "Howard JH Jr",
        "Goncharenko J",
        "Voronenko SO",
        "Ahamed T",
        "Stephens G",
        "Yger P",
        "Lefebvre B",
        "Spampinato GLB",
        "Esposito E",
        "et Olivier Marre MS",
        "Choi H",
        "Song MH",
        "Chung S",
        "Lee DD",
        "Sompolinsky H",
        "Phillips RS",
        "Smith J",
        "Chatzikalymniou AP",
        "Ferguson K",
        "Alex Cayco Gajic N",
        "Clopath C",
        "Angus Silver R",
        "Gleeson P",
        "Marin B",
        "Sadeh S",
        "Quintana A",
        "Cantarelli M",
        "Dura-Bernal S",
        "Lytton WW",
        "Davison A",
        "Li L",
        "Zhang W",
        "Wang D",
        "Song Y",
        "Park S",
        "Choi I",
        "Shin HS",
        "Choi H",
        "Pasupathy A",
        "Shea-Brown E",
        "Huh D",
        "Sejnowski TJ",
        "Vogt SM",
        "Kumar A",
        "Schmidt R",
        "Van Wert S",
        "Schiff SJ",
        "Veale R",
        "Scheutz M",
        "Lee SW",
        "Gallinaro J",
        "Rotter S",
        "Rubchinsky LL",
        "Cheung CC",
        "Ratnadurai-Giridharan S",
        "Shomali SR",
        "Ahmadabadi MN",
        "Shimazaki H",
        "Nader Rasuli S",
        "Zhao X",
        "Rasch MJ",
        "Wilting J",
        "Priesemann V",
        "Levina A",
        "Rudelt L",
        "Lizier JT",
        "Spinney RE",
        "Rubinov M",
        "Wibral M",
        "Bak JH",
        "Pillow J",
        "Zaho Y",
        "Park IM",
        "Kang J",
        "Park HJ",
        "Jang J",
        "Paik SB",
        "Choi W",
        "Lee C",
        "Song M",
        "Lee H",
        "Park Y",
        "Yilmaz E",
        "Baysal V",
        "Ozer M",
        "Saska D",
        "Nowotny T",
        "Chan HK",
        "Diamond A",
        "Herrmann CS",
        "Murray MM",
        "Ionta S",
        "Hutt A",
        "Lefebvre J",
        "Weidel P",
        "Duarte R",
        "Morrison A",
        "Lee JH",
        "Iyer R",
        "Mihalas S",
        "Koch C",
        "Petrovici MA",
        "Leng L",
        "Breitwieser O",
        "Stöckel D",
        "Bytschok I",
        "Martel R",
        "Bill J",
        "Schemmel J",
        "Meier K",
        "Esler TB",
        "Burkitt AN",
        "Kerr RR",
        "Tahayori B",
        "Nolte M",
        "Reimann MW",
        "Muller E",
        "Markram H",
        "Parziale A",
        "Senatore R",
        "Marcelli A",
        "Skiker K",
        "Maouene M",
        "Neymotin SA",
        "Seidenstein A",
        "Lakatos P",
        "Sanger TD",
        "Menzies RJ",
        "McLauchlan C",
        "van Albada SJ",
        "Kedziora DJ",
        "Neymotin S",
        "Kerr CC",
        "Suter BA",
        "Shepherd GMG",
        "Ryu J",
        "Lee SH",
        "Lee J",
        "Lee HJ",
        "Lim D",
        "Wang J",
        "Lee H",
        "Jung N",
        "Anh Quang L",
        "Maeng SE",
        "Lee TH",
        "Lee JW",
        "Park CH",
        "Ahn S",
        "Moon J",
        "Choi YS",
        "Kim J",
        "Jun SB",
        "Lee S",
        "Lee HW",
        "Jo S",
        "Jun E",
        "Yu S",
        "Goetze F",
        "Lai PY",
        "Kim S",
        "Kwag J",
        "Jang HJ",
        "Filipović M",
        "Reig R",
        "Aertsen A",
        "Silberberg G",
        "Bachmann C",
        "Buttler S",
        "Jacobs H",
        "Dillen K",
        "Fink GR",
        "Kukolja J",
        "Kepple D",
        "Giaffar H",
        "Rinberg D",
        "Shea S",
        "Koulakov A",
        "Bahuguna J",
        "Tetzlaff T",
        "Kotaleski JH",
        "Kunze T",
        "Peterson A",
        "Knösche T",
        "Kim M",
        "Kim H",
        "Park JS",
        "Yeon JW",
        "Kim SP",
        "Kang JH",
        "Lee C",
        "Spiegler A",
        "Petkoski S",
        "Palva MJ",
        "Jirsa VK",
        "Saggio ML",
        "Siep SF",
        "Stacey WC",
        "Bernar C",
        "Choung OH",
        "Jeong Y",
        "Lee YI",
        "Kim SH",
        "Jeong M",
        "Lee J",
        "Kwon J",
        "Kralik JD",
        "Jahng J",
        "Hwang DU",
        "Kwon JH",
        "Park SM",
        "Kim S",
        "Kim H",
        "Kim PS",
        "Yoon S",
        "Lim S",
        "Park C",
        "Miller T",
        "Clements K",
        "Ahn S",
        "Ji EH",
        "Issa FA",
        "Baek J",
        "Oba S",
        "Yoshimoto J",
        "Doya K",
        "Ishii S",
        "Mosqueiro TS",
        "Strube-Bloss MF",
        "Smith B",
        "Huerta R",
        "Hadrava M",
        "Hlinka J",
        "Bos H",
        "Helias M",
        "Welzig CM",
        "Harper ZJ",
        "Kim WS",
        "Shin IS",
        "Baek HM",
        "Han SK",
        "Richter R",
        "Vitay J",
        "Beuth F",
        "Hamker FH",
        "Toppin K",
        "Guo Y",
        "Graham BP",
        "Kale PJ",
        "Gollo LL",
        "Stern M",
        "Abbott LF",
        "Fedorov LA",
        "Giese MA",
        "Ardestani MH",
        "Faraji MJ",
        "Preuschoff K",
        "Gerstner W",
        "van Gendt MJ",
        "Briaire JJ",
        "Kalkman RK",
        "Frijns JHM",
        "Lee WH",
        "Frangou S",
        "Fulcher BD",
        "Tran PHP",
        "Fornito A",
        "Gliske SV",
        "Lim E",
        "Holman KA",
        "Fink CG",
        "Kim JS",
        "Mu S",
        "Briggman KL",
        "Sebastian Seung H",
        "the EyeWirers",
        "Wegener D",
        "Bohnenkamp L",
        "Ernst UA",
        "Devor A",
        "Dale AM",
        "Lines GT",
        "Edwards A",
        "Tveito A",
        "Hagen E",
        "Senk J",
        "Diesmann M",
        "Schmidt M",
        "Bakker R",
        "Shen K",
        "Bezgin G",
        "Hilgetag CC",
        "van Albada SJ",
        "Sun H",
        "Sourina O",
        "Huang GB",
        "Klanner F",
        "Denk C",
        "Glomb K",
        "Ponce-Alvarez A",
        "Gilson M",
        "Ritter P",
        "Deco G",
        "Witek MAG",
        "Clarke EF",
        "Hansen M",
        "Wallentin M",
        "Kringelbach ML",
        "Vuust P",
        "Klingbeil G",
        "De Schutter E",
        "Chen W",
        "Zang Y",
        "Hong S",
        "Takashima A",
        "Zamora C",
        "Gallimore AR",
        "Goldschmidt D",
        "Manoonpong P",
        "Karoly PJ",
        "Freestone DR",
        "Soundry D",
        "Kuhlmann L",
        "Paninski L",
        "Cook M",
        "Lee J",
        "Fishman YI",
        "Cohen YE",
        "Roberts JA",
        "Cocchi L",
        "Sweeney Y",
        "Lee S",
        "Jung WS",
        "Kim Y",
        "Jung Y",
        "Song YK",
        "Chavane F",
        "Soman K",
        "Muralidharan V",
        "Srinivasa Chakravarthy V",
        "Shivkumar S",
        "Mandali A",
        "Pragathi Priyadharsini B",
        "Mehta H",
        "Davey CE",
        "Brinkman BAW",
        "Kekona T",
        "Rieke F",
        "Buice M",
        "De Pittà M",
        "Berry H",
        "Brunel N",
        "Breakspear M",
        "Marsat G",
        "Drew J",
        "Chapman PD",
        "Daly KC",
        "Bradle SP",
        "Seo SB",
        "Su J",
        "Kavalali ET",
        "Blackwell J",
        "Shiau L",
        "Buhry L",
        "Basnayake K",
        "Lee SH",
        "Levy BA",
        "Baker CI",
        "Leleu T",
        "Philips RT",
        "Chhabria K"
      ],
      "abstract": "A1 Functional advantages of cell-type heterogeneity in neural circuits Tatyana O. Sharpee A2 Mesoscopic modeling of propagating waves in visual cortex Alain Destexhe A3 Dynamics and biomarkers of mental disorders Mitsuo Kawato F1 Precise recruitment of spiking output at theta frequencies requires dendritic h-channels in multi-compartment models of oriens-lacunosum/moleculare hippocampal interneurons Vladislav Sekulić, Frances K. Skinner F2 Kernel methods in reconstruction of current sources from extracellular potentials for single cells and the whole brains Daniel K. Wójcik, Chaitanya Chintaluri, Dorottya Cserpán, Zoltán Somogyvári F3 The synchronized periods depend on intracellular transcriptional repression mechanisms in circadian clocks. Jae Kyoung Kim, Zachary P. Kilpatrick, Matthew R. Bennett, Kresimir Josić O1 Assessing irregularity and coordination of spiking-bursting rhythms in central pattern generators Irene Elices, David Arroyo, Rafael Levi, Francisco B. Rodriguez, Pablo Varona O2 Regulation of top-down processing by cortically-projecting parvalbumin positive neurons in basal forebrain Eunjin Hwang, Bowon Kim, Hio-Been Han, Tae Kim, James T. McKenna, Ritchie E. Brown, Robert W. McCarley, Jee Hyun Choi O3 Modeling auditory stream segregation, build-up and bistability James Rankin, Pamela Osborn Popp, John Rinzel O4 Strong competition between tonotopic neural ensembles explains pitch-related dynamics of auditory cortex evoked fields Alejandro Tabas, André Rupp, Emili Balaguer-Ballester O5 A simple model of retinal response to multi-electrode stimulation Matias I. Maturana, David B. Grayden, Shaun L. Cloherty, Tatiana Kameneva, Michael R. Ibbotson, Hamish Meffin O6 Noise correlations in V4 area correlate with behavioral performance in visual discrimination task Veronika Koren, Timm Lochmann, Valentin Dragoi, Klaus Obermayer O7 Input-location dependent gain modulation in cerebellar nucleus neurons Maria Psarrou, Maria Schilstra, Neil Davey, Benjamin Torben-Nielsen, Volker Steuber O8 Analytic solution of cable energy function for cortical axons and dendrites Huiwen Ju, Jiao Yu, Michael L. Hines, Liang Chen, Yuguo Yu O9",
      "doi": "https://doi.org/10.1186/s12868-016-0283-6",
      "year": 2016,
      "source": "PubMed"
    },
    {
      "title": "Quantifying Visual Image Quality: A Bayesian View.",
      "authors": [
        "Duanmu Z",
        "Liu W",
        "Wang Z",
        "Wang Z"
      ],
      "abstract": "Image quality assessment (IQA) models aim to establish a quantitative relationship between visual images and their quality as perceived by human observers. IQA modeling plays a special bridging role between vision science and engineering practice, both as a test-bed for vision theories and computational biovision models and as a powerful tool that could potentially have a profound impact on a broad range of image processing, computer vision, and computer graphics applications for design, optimization, and evaluation purposes. The growth of IQA research has accelerated over the past two decades. In this review, we present an overview of IQA methods from a Bayesian perspective, with the goals of unifying a wide spectrum of IQA approaches under a common framework and providing useful references to fundamental concepts accessible to vision scientists and image processing practitioners. We discuss the implications of the successes and limitations of modern IQA methods for biological vision and the prospect for vision science to inform the design of future artificial vision systems. (The detailed model taxonomy can be found at",
      "doi": "https://doi.org/10.1146/annurev-vision-100419-120301",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "A computational model of serial and parallel processing in visual search",
      "authors": [
        "Rachel F. Heaton"
      ],
      "abstract": "The following is a dissertation aimed at understanding what the various\nphenomena in visual search teach us about the nature of human visual\nrepresentations and processes. I first review some of the major empirical\nfindings in the study of visual search. I next present a theory of visual\nsearch in terms of what I believe these findings suggest about the\nrepresentations and processes underlying ventral visual processing. These\nprinciples are instantiated in a computational model called CASPER (Concurrent\nAttention: Serial and Parallel Evaluation with Relations), originally developed\nby Hummel, that I have adapted to account for a range of phenomena in visual\nsearch. I then describe an extension of the CASPER model to account for our\nability to search for visual items defined not simply by the features composing\nthose items but by the spatial relations among those features. Seven\nexperiments (four main experiments and three replications) are described that\ntest CASPER's predictions about relational search. Finally, I evaluate the fit\nbetween CASPER's predictions and the empirical findings and show with three\nadditional simulations that CASPER can account for negative acceleration in\nsearch functions for relational stimuli if one postulates that the visual\nsystem is leveraging an emergent feature that bypasses relational processing.",
      "doi": "arXiv:2310.10061v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Tensor Composition Net for Visual Relationship Prediction",
      "authors": [
        "Yuting Qiang",
        "Yongxin Yang",
        "Xueting Zhang",
        "Yanwen Guo",
        "Timothy M. Hospedales"
      ],
      "abstract": "We present a novel Tensor Composition Net (TCN) to predict visual\nrelationships in images. Visual Relationship Prediction (VRP) provides a more\nchallenging test of image understanding than conventional image tagging and is\ndifficult to learn due to a large label-space and incomplete annotation. The\nkey idea of our TCN is to exploit the low-rank property of the visual\nrelationship tensor, so as to leverage correlations within and across objects\nand relations and make a structured prediction of all visual relationships in\nan image. To show the effectiveness of our model, we first empirically compare\nour model with Multi-Label Image Classification (MLIC) methods, eXtreme\nMulti-label Classification (XMC) methods, and VRD methods. We then show that\nthanks to our tensor (de)composition layer, our model can predict visual\nrelationships which have not been seen in the training dataset. We finally show\nour TCN's image-level visual relationship prediction provides a simple and\nefficient mechanism for relation-based image-retrieval even compared with VRD\nmethods.",
      "doi": "arXiv:2012.05473v2",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "DAVE: A Deep Audio-Visual Embedding for Dynamic Saliency Prediction",
      "authors": [
        "Hamed R. Tavakoli",
        "Ali Borji",
        "Esa Rahtu",
        "Juho Kannala"
      ],
      "abstract": "This paper studies audio-visual deep saliency prediction. It introduces a\nconceptually simple and effective Deep Audio-Visual Embedding for dynamic\nsaliency prediction dubbed ``DAVE\" in conjunction with our efforts towards\nbuilding an Audio-Visual Eye-tracking corpus named ``AVE\". Despite existing a\nstrong relation between auditory and visual cues for guiding gaze during\nperception, video saliency models only consider visual cues and neglect the\nauditory information that is ubiquitous in dynamic scenes. Here, we investigate\nthe applicability of audio cues in conjunction with visual ones in predicting\nsaliency maps using deep neural networks. To this end, the proposed model is\nintentionally designed to be simple. Two baseline models are developed on the\nsame architecture which consists of an encoder-decoder. The encoder projects\nthe input into a feature space followed by a decoder that infers saliency. We\nconduct an extensive analysis on different modalities and various aspects of\nmulti-model dynamic saliency prediction. Our results suggest that (1) audio is\na strong contributing cue for saliency prediction, (2) salient visible\nsound-source is the natural cause of the superiority of our Audio-Visual model,\n(3) richer feature representations for the input space leads to more powerful\npredictions even in absence of more sophisticated saliency decoders, and (4)\nAudio-Visual model improves over 53.54\\% of the frames predicted by the best\nVisual model (our baseline). Our endeavour demonstrates that audio is an\nimportant cue that boosts dynamic video saliency prediction and helps models to\napproach human performance. The code is available at\nhttps://github.com/hrtavakoli/DAVE",
      "doi": "arXiv:1905.10693v2",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual\n  Question Answering",
      "authors": [
        "Pan Lu",
        "Lei Ji",
        "Wei Zhang",
        "Nan Duan",
        "Ming Zhou",
        "Jianyong Wang"
      ],
      "abstract": "Recently, Visual Question Answering (VQA) has emerged as one of the most\nsignificant tasks in multimodal learning as it requires understanding both\nvisual and textual modalities. Existing methods mainly rely on extracting image\nand question features to learn their joint feature embedding via multimodal\nfusion or attention mechanism. Some recent studies utilize external\nVQA-independent models to detect candidate entities or attributes in images,\nwhich serve as semantic knowledge complementary to the VQA task. However, these\ncandidate entities or attributes might be unrelated to the VQA task and have\nlimited semantic capacities. To better utilize semantic knowledge in images, we\npropose a novel framework to learn visual relation facts for VQA. Specifically,\nwe build up a Relation-VQA (R-VQA) dataset based on the Visual Genome dataset\nvia a semantic similarity module, in which each data consists of an image, a\ncorresponding question, a correct answer and a supporting relation fact. A\nwell-defined relation detector is then adopted to predict visual\nquestion-related relation facts. We further propose a multi-step attention\nmodel composed of visual attention and semantic attention sequentially to\nextract related visual knowledge and semantic knowledge. We conduct\ncomprehensive experiments on the two benchmark datasets, demonstrating that our\nmodel achieves state-of-the-art performance and verifying the benefit of\nconsidering visual relation facts.",
      "doi": "https://doi.org/10.1145/3219819.3220036",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "Group Visual Relation Detection.",
      "authors": [
        "Yu F",
        "Zhang B",
        "Ren T",
        "Liu J",
        "Wu G",
        "Tang J"
      ],
      "abstract": "In this paper, we propose a novel visual relation detection task, named Group Visual Relation Detection (GVRD), for detecting visual relations whose subjects and/or objects are groups (GVRs), inspired by the observation that groups are common in image semantic representation. GVRD can be deemed as an evolution over the existing visual relation detection task that limits both subjects and objects of visual relations as individuals. We propose a Simultaneous Group Relation Prediction (SGRP) method that can simultaneously predict groups and predicates to address GVRD. SGRP contains an Entity Construction (EC) module, a Feature Extraction (FE) module, and a Group Relation Prediction (GRP) module. Specifically, the EC module constructs instances, group candidates, and phrase candidates; the FE module extracts visual, location and semantic features for these entities; and the GRP module simultaneously predicts groups and predicates, and generates the GVRs. Moreover, we construct a new dataset, named COCO-GVR, to facilitate solutions to GVRD task, which consists of 9,570 images from COCO dataset and 31,855 manually labeled GVRs. We test and validate the performance of SGRP by extensive experiments on COCO-GVR dataset. It shows that SGRP outperforms the baselines generated from the state-of-the-art visual relation detection and scene graph generation methods.",
      "doi": "https://doi.org/10.1109/tip.2025.3543114",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "Associative Prediction of Visual Shape in the Hippocampus.",
      "authors": [
        "Kok P",
        "Turk-Browne NB"
      ],
      "abstract": "Perception can be cast as a process of inference, in which bottom-up signals are combined with top-down predictions in sensory systems. In line with this, neural activity in sensory cortex is strongly modulated by prior expectations. Such top-down predictions often arise from cross-modal associations, such as when a sound (e.g., bell or bark) leads to an expectation of the visual appearance of the corresponding object (e.g., bicycle or dog). We hypothesized that the hippocampus, which rapidly learns arbitrary relationships between stimuli over space and time, may be involved in forming such associative predictions. We exposed male and female human participants to auditory cues predicting visual shapes, while measuring high-resolution fMRI signals in visual cortex and the hippocampus. Using multivariate reconstruction methods, we discovered a dissociation between these regions: representations in visual cortex were dominated by whichever shape was presented, whereas representations in the hippocampus reflected only which shape was predicted by the cue. The strength of hippocampal predictions correlated across participants with the amount of expectation-related facilitation in visual cortex. These findings help bridge the gap between memory and sensory systems in the human brain.",
      "doi": "https://doi.org/10.1523/jneurosci.0163-18.2018",
      "year": 2018,
      "source": "PubMed"
    },
    {
      "title": "Prediction of 10-2 Visual Field Loss Using Optical Coherence Tomography and 24-2 Visual Field Data.",
      "authors": [
        "Sullivan-Mee M",
        "Hedayat M",
        "Charry N",
        "Katiyar S",
        "Kee H",
        "Kimura B",
        "Pensyl D"
      ],
      "abstract": "Using standard glaucoma structural and functional tests, clinicians accurately predicted the presence/absence of 10-2 glaucomatous visual field (VF) loss in 90% of the eyes in this study. To investigate how well clinicians with variable experience can predict the presence and location of 10-2 VF loss using structural and functional data that are routinely obtained for glaucoma assessment. Within a test set of 416 eyes (210 subjects) who were diagnosed glaucoma suspect or primary open-angle glaucoma (with most eyes having mild disease), 6 clinicians were asked to predict the presence and hemispheric location of 10-2 VF loss using 24-2 VF and spectral-domain optical coherence tomography structural data. Prediction accuracies were calculated for each clinician and compared using the weighted κ-statistic. Receiver operating characteristic analyses were used to evaluate models for predicting 10-2 VF loss. Among the 6 clinicians, mean (range) accuracy, false negatives, and false positives for predicting presence/absence of 10-2 VF loss were 90% (87% to 92%), 4.7% (2.4% to 7.0%), and 5.4% (1.7% to 7.5%) respectively. The mean (range) weighted κ-statistic was 0.75 (0.64 to 0.83), suggesting good or very good inter-rater agreement between examiners. Mean accuracy for correctly predicting hemispheric location was 73% (range, 65% to 82%) with the most common error occurring in eyes with both superior and inferior 10-2 VF defects in which one hemisphere was correctly identified but the other missed. In this study, the presence/absence of 10-2 glaucomatous VF loss was highly predictable using standard functional and structural clinical metrics. These findings suggest that 10-2 VF testing is not needed to reliably recognize and confirm central VF involvement in most eyes with glaucoma. Whether error related to identifying second hemisphere involvement in 10-2 VF loss is important requires further study.",
      "doi": "https://doi.org/10.1097/ijg.0000000000001837",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Action prediction during real-time parent-infant interactions.",
      "authors": [
        "Monroy C",
        "Chen CH",
        "Houston D",
        "Yu C"
      ],
      "abstract": "Social interactions provide a crucial context for early learning and cognitive development during infancy. Action prediction-the ability to anticipate an observed action-facilitates successful, coordinated interaction and is an important social-cognitive skill in early development. However, current knowledge about infant action prediction comes largely from screen-based laboratory tasks. We know little about what infants' action prediction skills look like during real-time, free-flowing interactions with a social partner. In the current study, we used head-mounted eyetracking to quantify 9-month-old infants' visual anticipations of their parents' actions during free-flowing parent-child play. Our findings reveal that infants do anticipate their parents' actions during dynamic interactions at rates significantly higher than would be expected by chance. In addition, the frequency with which they do so is associated with child-led joint attention and hand-eye coordination. These findings are the first to reveal infants' action prediction behaviors in a more naturalistic context than prior screen-based studies, and they support the idea that action prediction is inherently linked to motor development and plays an important role in infants' social-cognitive development. A video abstract of this article can be viewed at https://www.youtube.com/watch?v=9HrmcicfiqE.",
      "doi": "https://doi.org/10.1111/desc.13042",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Image caption generation using Visual Attention Prediction and Contextual Spatial Relation Extraction",
      "authors": [
        "Reshmi Sasibhooshan",
        "Suresh Kumaraswamy",
        "Santhoshkumar Sasidharan"
      ],
      "abstract": "<jats:title>Abstract</jats:title><jats:p>Automatic caption generation with attention mechanisms aims at generating more descriptive captions containing coarser to finer semantic contents in the image. In this work, we use an encoder-decoder framework employing Wavelet transform based Convolutional Neural Network (WCNN) with two level discrete wavelet decomposition for extracting the visual feature maps highlighting the spatial, spectral and semantic details from the image. The Visual Attention Prediction Network (VAPN) computes both channel and spatial attention for obtaining visually attentive features. In addition to these, local features are also taken into account by considering the contextual spatial relationship between the different objects. The probability of the appropriate word prediction is achieved by combining the aforementioned architecture with Long Short Term Memory (LSTM) decoder network. Experiments are conducted on three benchmark datasets—Flickr8K, Flickr30K and MSCOCO datasets and the evaluation results prove the improved performance of the proposed model with CIDEr score of 124.2.</jats:p>",
      "doi": "https://doi.org/10.1186/s40537-023-00693-9",
      "year": 2023,
      "source": "Crossref"
    },
    {
      "title": "Scene Graph Lossless Compression with Adaptive Prediction for Objects\n  and Relations",
      "authors": [
        "Yufeng Zhang",
        "Weiyao Lin",
        "Wenrui Dai",
        "Huabin Liu",
        "Hongkai Xiong"
      ],
      "abstract": "The scene graph is a new data structure describing objects and their pairwise\nrelationship within image scenes. As the size of scene graph in vision\napplications grows, how to losslessly and efficiently store such data on disks\nor transmit over the network becomes an inevitable problem. However, the\ncompression of scene graph is seldom studied before because of the complicated\ndata structures and distributions. Existing solutions usually involve\ngeneral-purpose compressors or graph structure compression methods, which is\nweak at reducing redundancy for scene graph data. This paper introduces a new\nlossless compression framework with adaptive predictors for joint compression\nof objects and relations in scene graph data. The proposed framework consists\nof a unified prior extractor and specialized element predictors to adapt for\ndifferent data elements. Furthermore, to exploit the context information within\nand between graph elements, Graph Context Convolution is proposed to support\ndifferent graph context modeling schemes for different graph elements. Finally,\na learned distribution model is devised to predict numerical data under\ncomplicated conditional constraints. Experiments conducted on labeled or\ngenerated scene graphs proves the effectiveness of the proposed framework in\nscene graph lossless compression task.",
      "doi": "arXiv:2304.13359v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "SGRAM: Improving Scene Graph Parsing via Abstract Meaning Representation",
      "authors": [
        "Woo Suk Choi",
        "Yu-Jung Heo",
        "Byoung-Tak Zhang"
      ],
      "abstract": "Scene graph is structured semantic representation that can be modeled as a\nform of graph from images and texts. Image-based scene graph generation\nresearch has been actively conducted until recently, whereas text-based scene\ngraph generation research has not. In this paper, we focus on the problem of\nscene graph parsing from textual description of a visual scene. The core idea\nis to use abstract meaning representation (AMR) instead of the dependency\nparsing mainly used in previous studies. AMR is a graph-based semantic\nformalism of natural language which abstracts concepts of words in a sentence\ncontrary to the dependency parsing which considers dependency relationships on\nall words in a sentence. To this end, we design a simple yet effective\ntwo-stage scene graph parsing framework utilizing abstract meaning\nrepresentation, SGRAM (Scene GRaph parsing via Abstract Meaning\nrepresentation): 1) transforming a textual description of an image into an AMR\ngraph (Text-to-AMR) and 2) encoding the AMR graph into a Transformer-based\nlanguage model to generate a scene graph (AMR-to-SG). Experimental results show\nthe scene graphs generated by our framework outperforms the dependency\nparsing-based model by 11.61\\% and the previous state-of-the-art model using a\npre-trained Transformer language model by 3.78\\%. Furthermore, we apply SGRAM\nto image retrieval task which is one of downstream tasks for scene graph, and\nconfirm the effectiveness of scene graphs generated by our framework.",
      "doi": "arXiv:2210.08675v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Iterative Scene Graph Generation with Generative Transformers",
      "authors": [
        "Sanjoy Kundu",
        "Sathyanarayanan N. Aakur"
      ],
      "abstract": "Scene graphs provide a rich, structured representation of a scene by encoding\nthe entities (objects) and their spatial relationships in a graphical format.\nThis representation has proven useful in several tasks, such as question\nanswering, captioning, and even object detection, to name a few. Current\napproaches take a generation-by-classification approach where the scene graph\nis generated through labeling of all possible edges between objects in a scene,\nwhich adds computational overhead to the approach. This work introduces a\ngenerative transformer-based approach to generating scene graphs beyond link\nprediction. Using two transformer-based components, we first sample a possible\nscene graph structure from detected objects and their visual features. We then\nperform predicate classification on the sampled edges to generate the final\nscene graph. This approach allows us to efficiently generate scene graphs from\nimages with minimal inference overhead. Extensive experiments on the Visual\nGenome dataset demonstrate the efficiency of the proposed approach. Without\nbells and whistles, we obtain, on average, 20.7% mean recall (mR@100) across\ndifferent settings for scene graph generation (SGG), outperforming\nstate-of-the-art SGG approaches while offering competitive performance to\nunbiased SGG approaches.",
      "doi": "arXiv:2211.16636v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Sketching Image Gist: Human-Mimetic Hierarchical Scene Graph Generation",
      "authors": [
        "Wenbin Wang",
        "Ruiping Wang",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "abstract": "Scene graph aims to faithfully reveal humans' perception of image content.\nWhen humans analyze a scene, they usually prefer to describe image gist first,\nnamely major objects and key relations in a scene graph. This humans' inherent\nperceptive habit implies that there exists a hierarchical structure about\nhumans' preference during the scene parsing procedure. Therefore, we argue that\na desirable scene graph should be also hierarchically constructed, and\nintroduce a new scheme for modeling scene graph. Concretely, a scene is\nrepresented by a human-mimetic Hierarchical Entity Tree (HET) consisting of a\nseries of image regions. To generate a scene graph based on HET, we parse HET\nwith a Hybrid Long Short-Term Memory (Hybrid-LSTM) which specifically encodes\nhierarchy and siblings context to capture the structured information embedded\nin HET. To further prioritize key relations in the scene graph, we devise a\nRelation Ranking Module (RRM) to dynamically adjust their rankings by learning\nto capture humans' subjective perceptive habits from objective entity saliency\nand size. Experiments indicate that our method not only achieves\nstate-of-the-art performances for scene graph generation, but also is expert in\nmining image-specific relations which play a great role in serving downstream\ntasks.",
      "doi": "arXiv:2007.08760v1",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "Neural Belief Propagation for Scene Graph Generation.",
      "authors": [
        "Liu D",
        "Bober M",
        "Kittler J"
      ],
      "abstract": "Scene graph generation aims to interpret an input image by explicitly modelling the objects contained therein and their relationships. In existing methods the problem is predominantly solved by message passing neural network models. Unfortunately, in such models, the variational distributions generally ignore the structural dependencies among the output variables, and most of the scoring functions only consider pairwise dependencies. This can lead to inconsistent interpretations. In this article, we propose a novel neural belief propagation method seeking to replace the traditional mean field approximation with a structural Bethe approximation. To find a better bias-variance trade-off, higher-order dependencies among three or more output variables are also incorporated into the relevant scoring function. The proposed method achieves the state-of-the-art performance on various popular scene graph generation benchmarks.",
      "doi": "https://doi.org/10.1109/tpami.2023.3243306",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Relation Regularized Scene Graph Generation.",
      "authors": [
        "Guo Y",
        "Gao L",
        "Song J",
        "Wang P",
        "Sebe N",
        "Shen HT",
        "Li X"
      ],
      "abstract": "Scene graph generation (SGG) is built on top of detected objects to predict object pairwise visual relations for describing the image content abstraction. Existing works have revealed that if the links between objects are given as prior knowledge, the performance of SGG is significantly improved. Inspired by this observation, in this article, we propose a relation regularized network (R2-Net), which can predict whether there is a relationship between two objects and encode this relation into object feature refinement and better SGG. Specifically, we first construct an affinity matrix among detected objects to represent the probability of a relationship between two objects. Graph convolution networks (GCNs) over this relation affinity matrix are then used as object encoders, producing relation-regularized representations of objects. With these relation-regularized features, our R2-Net can effectively refine object labels and generate scene graphs. Extensive experiments are conducted on the visual genome dataset for three SGG tasks (i.e., predicate classification, scene graph classification, and scene graph detection), demonstrating the effectiveness of our proposed method. Ablation studies also verify the key roles of our proposed components in performance improvement.",
      "doi": "https://doi.org/10.1109/tcyb.2021.3052522",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Debiased Scene Graph Generation for Dual Imbalance Learning.",
      "authors": [
        "Zhou H",
        "Zhang J",
        "Luo T",
        "Yang Y",
        "Lei J"
      ],
      "abstract": "Scene graph generation (SGG) is one of the hottest topics in computer vision and has attracted many interests since it provides rich semantic information between objects. In practice, the SGG datasets are often dual imbalanced, presented as a large number of backgrounds and rarely few foregrounds, and highly skewed foreground relationships categories (i.e., the long-tailed distribution). How to tackle this dual imbalanced problem is crucial but rarely studied in literature. Existing methods only consider the long-tailed distribution of foregrounds classes and ignore the background-foreground imbalance in SGG, which results in a biased model and prevents it from being applied in the downstream tasks widely. To reduce its side effect and make the contributions of different categories equally, we propose a novel debiased SGG method (named DSDI) by incorporating biased resistance loss and causal intervention tree. We first deeply analyze the potential causes of dual imbalanced problem in SGG. Then, to learn more discriminate representation of the foreground by expanding the foreground features space, the biased resistance loss decouples the background classification from foreground relationship recognition. Meanwhile, a causal graph of content and context is designed to remove the context bias and learn unbiased relationship features via casual intervention tree. Extensive experimental results on two extremely imbalanced datasets: VG150 and VrR-VG, demonstrate our DSDI outperforms other state-of-the-art methods. All our models will be available in https://github.com/zhouhao0515/unbiasedSGG-DSDI.",
      "doi": "https://doi.org/10.1109/tpami.2022.3198965",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Constrained Structure Learning for Scene Graph Generation.",
      "authors": [
        "Liu D",
        "Bober M",
        "Kittler J"
      ],
      "abstract": "As a structured prediction task, scene graph generation aims to build a visually-grounded scene graph to explicitly model objects and their relationships in an input image. Currently, the mean field variational Bayesian framework is the de facto methodology used by the existing methods, in which the unconstrained inference step is often implemented by a message passing neural network. However, such formulation fails to explore other inference strategies, and largely ignores the more general constrained optimization models. In this paper, we present a constrained structure learning method, for which an explicit constrained variational inference objective is proposed. Instead of applying the ubiquitous message-passing strategy, a generic constrained optimization method - entropic mirror descent - is utilized to solve the constrained variational inference step. We validate the proposed generic model on various popular scene graph generation benchmarks and show that it outperforms the state-of-the-art methods.",
      "doi": "https://doi.org/10.1109/tpami.2023.3282889",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Structure Inference Net: Object Detection Using Scene-Level Context and\n  Instance-Level Relationships",
      "authors": [
        "Yong Liu",
        "Ruiping Wang",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "abstract": "Context is important for accurate visual recognition. In this work we propose\nan object detection algorithm that not only considers object visual appearance,\nbut also makes use of two kinds of context including scene contextual\ninformation and object relationships within a single image. Therefore, object\ndetection is regarded as both a cognition problem and a reasoning problem when\nleveraging these structured information. Specifically, this paper formulates\nobject detection as a problem of graph structure inference, where given an\nimage the objects are treated as nodes in a graph and relationships between the\nobjects are modeled as edges in such graph. To this end, we present a so-called\nStructure Inference Network (SIN), a detector that incorporates into a typical\ndetection framework (e.g. Faster R-CNN) with a graphical model which aims to\ninfer object state. Comprehensive experiments on PASCAL VOC and MS COCO\ndatasets indicate that scene context and object relationships truly improve the\nperformance of object detection with more desirable and reasonable outputs.",
      "doi": "arXiv:1807.00119v1",
      "year": 2018,
      "source": "arXiv"
    },
    {
      "title": "VrR-VG: Refocusing Visually-Relevant Relationships",
      "authors": [
        "Yuanzhi Liang",
        "Yalong Bai",
        "Wei Zhang",
        "Xueming Qian",
        "Li Zhu",
        "Tao Mei"
      ],
      "abstract": "Relationships encode the interactions among individual instances, and play a\ncritical role in deep visual scene understanding. Suffering from the high\npredictability with non-visual information, existing methods tend to fit the\nstatistical bias rather than ``learning'' to ``infer'' the relationships from\nimages. To encourage further development in visual relationships, we propose a\nnovel method to automatically mine more valuable relationships by pruning\nvisually-irrelevant ones. We construct a new scene-graph dataset named\nVisually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome.\nCompared with existing datasets, the performance gap between learnable and\nstatistical method is more significant in VrR-VG, and frequency-based analysis\ndoes not work anymore. Moreover, we propose to learn a relationship-aware\nrepresentation by jointly considering instances, attributes and relationships.\nBy applying the representation-aware feature learned on VrR-VG, the\nperformances of image captioning and visual question answering are\nsystematically improved with a large margin, which demonstrates the gain of our\ndataset and the features embedding schema. VrR-VG is available via\nhttp://vrr-vg.com/.",
      "doi": "arXiv:1902.00313v2",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Vectorizing World Buildings: Planar Graph Reconstruction by Primitive\n  Detection and Relationship Inference",
      "authors": [
        "Nelson Nauata",
        "Yasutaka Furukawa"
      ],
      "abstract": "This paper tackles a 2D architecture vectorization problem, whose task is to\ninfer an outdoor building architecture as a 2D planar graph from a single RGB\nimage. We provide a new benchmark with ground-truth annotations for 2,001\ncomplex buildings across the cities of Atlanta, Paris, and Las Vegas. We also\npropose a novel algorithm utilizing 1) convolutional neural networks (CNNs)\nthat detects geometric primitives and infers their relationships and 2) an\ninteger programming (IP) that assembles the information into a 2D planar graph.\nWhile being a trivial task for human vision, the inference of a graph structure\nwith an arbitrary topology is still an open problem for computer vision.\nQualitative and quantitative evaluations demonstrate that our algorithm makes\nsignificant improvements over the current state-of-the-art, towards an\nintelligent system at the level of human perception. We will share code and\ndata.",
      "doi": "arXiv:1912.05135v3",
      "year": 2019,
      "source": "arXiv"
    },
    {
      "title": "Fast Global Localization on Neural Radiance Field",
      "authors": [
        "Mangyu Kong",
        "Seongwon Lee",
        "Jaewon Lee",
        "Euntai Kim"
      ],
      "abstract": "Neural Radiance Fields (NeRF) presented a novel way to represent scenes,\nallowing for high-quality 3D reconstruction from 2D images. Following its\nremarkable achievements, global localization within NeRF maps is an essential\ntask for enabling a wide range of applications. Recently, Loc-NeRF demonstrated\na localization approach that combines traditional Monte Carlo Localization with\nNeRF, showing promising results for using NeRF as an environment map. However,\ndespite its advancements, Loc-NeRF encounters the challenge of a time-intensive\nray rendering process, which can be a significant limitation in practical\napplications. To address this issue, we introduce Fast Loc-NeRF, which\nleverages a coarse-to-fine approach to enable more efficient and accurate NeRF\nmap-based global localization. Specifically, Fast Loc-NeRF matches rendered\npixels and observed images on a multi-resolution from low to high resolution.\nAs a result, it speeds up the costly particle update process while maintaining\nprecise localization results. Additionally, to reject the abnormal particles,\nwe propose particle rejection weighting, which estimates the uncertainty of\nparticles by exploiting NeRF's characteristics and considers them in the\nparticle weighting process. Our Fast Loc-NeRF sets new state-of-the-art\nlocalization performances on several benchmarks, convincing its accuracy and\nefficiency.",
      "doi": "arXiv:2406.12202v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields",
      "authors": [
        "Yifan Wang",
        "Yi Gong",
        "Yuan Zeng"
      ],
      "abstract": "Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity\nscene reconstruction for novel view synthesis. However, NeRF requires hundreds\nof network evaluations per pixel to approximate a volume rendering integral,\nmaking it slow to train. Caching NeRFs into explicit data structures can\neffectively enhance rendering speed but at the cost of higher memory usage. To\naddress these issues, we present Hyb-NeRF, a novel neural radiance field with a\nmulti-resolution hybrid encoding that achieves efficient neural modeling and\nfast rendering, which also allows for high-quality novel view synthesis. The\nkey idea of Hyb-NeRF is to represent the scene using different encoding\nstrategies from coarse-to-fine resolution levels. Hyb-NeRF exploits\nmemory-efficiency learnable positional features at coarse resolutions and the\nfast optimization speed and local details of hash-based feature grids at fine\nresolutions. In addition, to further boost performance, we embed cone\ntracing-based features in our learnable positional encoding that eliminates\nencoding ambiguity and reduces aliasing artifacts. Extensive experiments on\nboth synthetic and real-world datasets show that Hyb-NeRF achieves faster\nrendering speed with better rending quality and even a lower memory footprint\nin comparison to previous state-of-the-art methods.",
      "doi": "arXiv:2311.12490v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale\n  Aerial Rendering",
      "authors": [
        "Xiaohan Zhang",
        "Yukui Qiu",
        "Zhenyu Sun",
        "Qi Liu"
      ],
      "abstract": "Recent progress in large-scale scene rendering has yielded Neural Radiance\nFields (NeRF)-based models with an impressive ability to synthesize scenes\nacross small objects and indoor scenes. Nevertheless, extending this idea to\nlarge-scale aerial rendering poses two critical problems. Firstly, a single\nNeRF cannot render the entire scene with high-precision for complex large-scale\naerial datasets since the sampling range along each view ray is insufficient to\ncover buildings adequately. Secondly, traditional NeRFs are infeasible to train\non one GPU to enable interactive fly-throughs for modeling massive images.\nInstead, existing methods typically separate the whole scene into multiple\nregions and train a NeRF on each region, which are unaccustomed to different\nflight trajectories and difficult to achieve fast rendering. To that end, we\npropose Aerial-NeRF with three innovative modifications for jointly adapting\nNeRF in large-scale aerial rendering: (1) Designing an adaptive spatial\npartitioning and selection method based on drones' poses to adapt different\nflight trajectories; (2) Using similarity of poses instead of (expert) network\nfor rendering speedup to determine which region a new viewpoint belongs to; (3)\nDeveloping an adaptive sampling approach for rendering performance improvement\nto cover the entire buildings at different heights. Extensive experiments have\nconducted to verify the effectiveness and efficiency of Aerial-NeRF, and new\nstate-of-the-art results have been achieved on two public large-scale aerial\ndatasets and presented SCUTic dataset. Note that our model allows us to perform\nrendering over 4 times as fast as compared to multiple competitors. Our\ndataset, code, and model are publicly available at https://drliuqi.github.io/.",
      "doi": "arXiv:2405.06214v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using\n  VDB Grid and Hierarchical Ray Traversal",
      "authors": [
        "Yoshio Kato",
        "Shuhei Tarashima"
      ],
      "abstract": "Transmittance estimators such as Occupancy Grid (OG) can accelerate the\ntraining and rendering of Neural Radiance Field (NeRF) by predicting important\nsamples that contributes much to the generated image. However, OG manages\noccupied regions in the form of the dense binary grid, in which there are many\nblocks with the same values that cause redundant examination of voxels'\nemptiness in ray-tracing. In our work, we introduce two techniques to improve\nthe efficiency of ray-tracing in trained OG without fine-tuning. First, we\nreplace the dense grids with VDB grids to reduce the spatial redundancy.\nSecond, we use hierarchical digital differential analyzer (HDDA) to efficiently\ntrace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF\n360 datasets show that our proposed method successfully accelerates rendering\nNeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in\naverage, compared to a fast implementation of OG, NerfAcc, without losing the\nquality of rendered images.",
      "doi": "arXiv:2404.10272v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Fast virtual view synthesis for an 8K 3D light-field display based on cutoff-NeRF and 3D voxel rendering.",
      "authors": [
        "Chen S",
        "Yan B",
        "Sang X",
        "Chen D",
        "Wang P",
        "Yang Z",
        "Guo X",
        "Zhong C"
      ],
      "abstract": "Three-dimensional (3D) light-field displays can provide an immersive visual experience, which has attracted significant attention. However, the generating of high-quality 3D light-field content in the real world is still a challenge because it is difficult to capture dense high-resolution viewpoints of the real world with the camera array. Novel view synthesis based on CNN can generate dense high-resolution viewpoints from sparse inputs but suffer from high-computational resource consumption, low rendering speed, and limited camera baseline. Here, a two-stage virtual view synthesis method based on cutoff-NeRF and 3D voxel rendering is presented, which can fast synthesize dense novel views with smooth parallax and 3D images with a resolution of 7680 × 4320 for the 3D light-field display. In the first stage, an image-based cutoff-NeRF is proposed to implicitly represent the distribution of scene content and improve the quality of the virtual view. In the second stage, a 3D voxel-based image rendering and coding algorithm is presented, which quantify the scene content distribution learned by cutoff-NeRF to render high-resolution virtual views fast and output high-resolution 3D images. Among them, a coarse-to-fine 3D voxel rendering method is proposed to improve the accuracy of voxel representation effectively. Furthermore, a 3D voxel-based off-axis pixel encoding method is proposed to speed up 3D image generation. Finally, a sparse views dataset is built by ourselves to analyze the effectiveness of the proposed method. Experimental results demonstrate the method's effectiveness, which can fast synthesize novel views and 3D images with high resolution in real 3D scenes and physical simulation environments. PSNR of the virtual view is about 29.75 dB, SSIM is about 0.88, and the synthetic 8K 3D image time is about 14.41s. We believe that our fast high-resolution virtual viewpoint synthesis method can effectively improve the application of 3D light field display.",
      "doi": "https://doi.org/10.1364/oe.473852",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "NeRFBuff: Fast Neural Rendering via Inter-frame Feature Buffering.",
      "authors": [
        "Liu A",
        "Liu Y",
        "Long X",
        "Wang P",
        "Lin C",
        "Luo P",
        "Wang W"
      ],
      "abstract": "Neural radiance fields (NeRF) have demonstrated impressive performance in novel view synthesis, but are still slow to render complex scenes at a high resolution. We introduce a novel method to boost the NeRF rendering speed by utilizing the temporal coherence between consecutive frames. Rather than computing features of each frame entirely from scratch, we reuse the coherent information (e.g., density and color) computed from the previous frames to help render the current frame, which significantly boosts rendering speed. To effectively manage the coherent information of previous frames, we introduce a history buffer with a multiple-plane structure, which is built online and updated from old frames to new frames. We name this buffer as multiple plane buffer (MPB). With this MPB, a new frame can be efficiently rendered using the warped features from previous frames. Extensive experiments on the NeRF-Synthetic, LLFF, and Mip-NeRF-360 datasets demonstrate that our method significantly boosts rendering efficiency and achieves 4× speedup on real-world scenes compared to the baseline methods while preserving competitive rendering quality.",
      "doi": "https://doi.org/10.1109/tvcg.2024.3393715",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "High-Fidelity and High-Efficiency Talking Portrait Synthesis With Detail-Aware Neural Radiance Fields.",
      "authors": [
        "Wang M",
        "Zhao S",
        "Dong X",
        "Shen J"
      ],
      "abstract": "In this paper, we propose a novel rendering framework based on neural radiance fields (NeRF) named HH-NeRF that can generate high-resolution audio-driven talking portrait videos with high fidelity and fast rendering. Specifically, our framework includes a detail-aware NeRF module and an efficient conditional super-resolution module. Firstly, a detail-aware NeRF is proposed to efficiently generate a high-fidelity low-resolution talking head, by using the encoded volume density estimation and audio-eye-aware color calculation. This module can capture natural eye blinks and high-frequency details, and maintain a similar rendering time as previous fast methods. Secondly, we present an efficient conditional super-resolution module on the dynamic scene to directly generate the high-resolution portrait with our low-resolution head. Incorporated with the prior information, such as depth map and audio features, our new proposed efficient conditional super resolution module can adopt a lightweight network to efficiently generate realistic and distinct high-resolution videos. Extensive experiments demonstrate that our method can generate more distinct and fidelity talking portraits on high resolution (900 × 900) videos compared to state-of-the-art methods. Our code is available at https://github.com/muyuWang/HHNeRF.",
      "doi": "https://doi.org/10.1109/tvcg.2024.3488960",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction.",
      "authors": [
        "Chen D",
        "Li H",
        "Ye W",
        "Wang Y",
        "Xie W",
        "Zhai S",
        "Wang N",
        "Liu H",
        "Bao H",
        "Zhang G"
      ],
      "abstract": "Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due to its high-quality rendering, and ultra-fast training and rendering speed. However, due to the unstructured and irregular nature of Gaussian point clouds, it is difficult to guarantee geometric reconstruction accuracy and multi-view consistency simply by relying on image reconstruction loss. Although many studies on surface reconstruction based on 3DGS have emerged recently, the quality of their meshes is generally unsatisfactory. To address this problem, we propose a fast planar-based Gaussian splatting reconstruction representation (PGSR) to achieve high-fidelity surface reconstruction while ensuring high-quality rendering. Specifically, we first introduce an unbiased depth rendering method, which directly renders the distance from the camera origin to the Gaussian plane and the corresponding normal map based on the Gaussian distribution of the point cloud, and divides the two to obtain the unbiased depth. We then introduce single-view geometric, multi-view photometric, and geometric regularization to preserve global geometric accuracy. We also propose a camera exposure compensation model to cope with scenes with large illumination variations. Experiments on indoor and outdoor scenes show that the proposed method achieves fast training and rendering while maintaining high-fidelity rendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based methods. Our code will be made publicly available, and more information can be found on our project page (https://zju3dv.github.io/pgsr/).",
      "doi": "https://doi.org/10.1109/tvcg.2024.3494046",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "A General Implicit Framework for Fast NeRF Composition and Rendering",
      "authors": [
        "Xinyu Gao",
        "Ziyi Yang",
        "Yunlu Zhao",
        "Yuxiang Sun",
        "Xiaogang Jin",
        "Changqing Zou"
      ],
      "abstract": "<jats:p>A variety of Neural Radiance Fields (NeRF) methods have recently achieved remarkable success in high render speed. However, current accelerating methods are specialized and incompatible with various implicit methods, preventing real-time composition over various types of NeRF works. Because NeRF relies on sampling along rays, it is possible to provide general guidance for acceleration. To that end, we propose a general implicit pipeline for composing NeRF objects quickly. Our method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration instead of depending on an explicit spatial structure.Our proposed method is the first to enable both the progressive and interactive composition of NeRF objects. Additionally, it also serves as a previewing plugin for a range of existing NeRF works.</jats:p>",
      "doi": "https://doi.org/10.1609/aaai.v38i3.27952",
      "year": 2024,
      "source": "Crossref"
    },
    {
      "title": "Remote Sensing Novel View Synthesis with Implicit Multiplane\n  Representations",
      "authors": [
        "Yongchang Wu",
        "Zhengxia Zou",
        "Zhenwei Shi"
      ],
      "abstract": "Novel view synthesis of remote sensing scenes is of great significance for\nscene visualization, human-computer interaction, and various downstream\napplications. Despite the recent advances in computer graphics and\nphotogrammetry technology, generating novel views is still challenging\nparticularly for remote sensing images due to its high complexity, view\nsparsity and limited view-perspective variations. In this paper, we propose a\nnovel remote sensing view synthesis method by leveraging the recent advances in\nimplicit neural representations. Considering the overhead and far depth imaging\nof remote sensing images, we represent the 3D space by combining implicit\nmultiplane images (MPI) representation and deep neural networks. The 3D scene\nis reconstructed under a self-supervised optimization paradigm through a\ndifferentiable multiplane renderer with multi-view input constraints. Images\nfrom any novel views thus can be freely rendered on the basis of the\nreconstructed model. As a by-product, the depth maps corresponding to the given\nviewpoint can be generated along with the rendering output. We refer to our\nmethod as Implicit Multiplane Images (ImMPI). To further improve the view\nsynthesis under sparse-view inputs, we explore the learning-based\ninitialization of remote sensing 3D scenes and proposed a neural network based\nPrior extractor to accelerate the optimization process. In addition, we propose\na new dataset for remote sensing novel view synthesis with multi-view\nreal-world google earth images. Extensive experiments demonstrate the\nsuperiority of the ImMPI over previous state-of-the-art methods in terms of\nreconstruction accuracy, visual fidelity, and time efficiency. Ablation\nexperiments also suggest the effectiveness of our methodology design. Our\ndataset and code can be found at https://github.com/wyc-Chang/ImMPI",
      "doi": "https://doi.org/10.1109/TGRS.2022.3197409",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Real-time High-resolution View Synthesis of Complex Scenes with Explicit\n  3D Visibility Reasoning",
      "authors": [
        "Tiansong Zhou",
        "Yebin Liu",
        "Xuangeng Chu",
        "Chengkun Cao",
        "Changyin Zhou",
        "Fei Yu",
        "Yu Li"
      ],
      "abstract": "Rendering photo-realistic novel-view images of complex scenes has been a\nlong-standing challenge in computer graphics. In recent years, great research\nprogress has been made on enhancing rendering quality and accelerating\nrendering speed in the realm of view synthesis. However, when rendering complex\ndynamic scenes with sparse views, the rendering quality remains limited due to\nocclusion problems. Besides, for rendering high-resolution images on dynamic\nscenes, the rendering speed is still far from real-time. In this work, we\npropose a generalizable view synthesis method that can render high-resolution\nnovel-view images of complex static and dynamic scenes in real-time from sparse\nviews. To address the occlusion problems arising from the sparsity of input\nviews and the complexity of captured scenes, we introduce an explicit 3D\nvisibility reasoning approach that can efficiently estimate the visibility of\nsampled 3D points to the input views. The proposed visibility reasoning\napproach is fully differentiable and can gracefully fit inside the volume\nrendering pipeline, allowing us to train our networks with only multi-view\nimages as supervision while refining geometry and texture simultaneously.\nBesides, each module in our pipeline is carefully designed to bypass the\ntime-consuming MLP querying process and enhance the rendering quality of\nhigh-resolution images, enabling us to render high-resolution novel-view images\nin real-time.Experimental results show that our method outperforms previous\nview synthesis methods in both rendering quality and speed, particularly when\ndealing with complex dynamic scenes with sparse views.",
      "doi": "arXiv:2402.12886v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical\n  Gaussians",
      "authors": [
        "Yiwen Wang",
        "Siyuan Chen",
        "Ran Yi"
      ],
      "abstract": "3D Gaussian Splatting is emerging as a state-of-the-art technique in novel\nview synthesis, recognized for its impressive balance between visual quality,\nspeed, and rendering efficiency. However, reliance on third-degree spherical\nharmonics for color representation introduces significant storage demands and\ncomputational overhead, resulting in a large memory footprint and slower\nrendering speed. We introduce SG-Splatting with Spherical Gaussians based color\nrepresentation, a novel approach to enhance rendering speed and quality in\nnovel view synthesis. Our method first represents view-dependent color using\nSpherical Gaussians, instead of three degree spherical harmonics, which largely\nreduces the number of parameters used for color representation, and\nsignificantly accelerates the rendering process. We then develop an efficient\nstrategy for organizing multiple Spherical Gaussians, optimizing their\narrangement to achieve a balanced and accurate scene representation. To further\nimprove rendering quality, we propose a mixed representation that combines\nSpherical Gaussians with low-degree spherical harmonics, capturing both high-\nand low-frequency color information effectively. SG-Splatting also has\nplug-and-play capability, allowing it to be easily integrated into existing\nsystems. This approach improves computational efficiency and overall visual\nfidelity, making it a practical solution for real-time applications.",
      "doi": "arXiv:2501.00342v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis",
      "authors": [
        "Lior Yariv",
        "Peter Hedman",
        "Christian Reiser",
        "Dor Verbin",
        "Pratul P. Srinivasan",
        "Richard Szeliski",
        "Jonathan T. Barron",
        "Ben Mildenhall"
      ],
      "abstract": "We present a method for reconstructing high-quality meshes of large unbounded\nreal-world scenes suitable for photorealistic novel view synthesis. We first\noptimize a hybrid neural volume-surface scene representation designed to have\nwell-behaved level sets that correspond to surfaces in the scene. We then bake\nthis representation into a high-quality triangle mesh, which we equip with a\nsimple and fast view-dependent appearance model based on spherical Gaussians.\nFinally, we optimize this baked representation to best reproduce the captured\nviewpoints, resulting in a model that can leverage accelerated polygon\nrasterization pipelines for real-time view synthesis on commodity hardware. Our\napproach outperforms previous scene representations for real-time rendering in\nterms of accuracy, speed, and power consumption, and produces high quality\nmeshes that enable applications such as appearance editing and physical\nsimulation.",
      "doi": "arXiv:2302.14859v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Brown and beige adipose tissue: a novel therapeutic strategy for obesity and type 2 diabetes mellitus.",
      "authors": [
        "Cheng L",
        "Wang J",
        "Dai H",
        "Duan Y",
        "An Y",
        "Shi L",
        "Lv Y",
        "Li H",
        "Wang C",
        "Ma Q",
        "Li Y",
        "Li P",
        "Du H",
        "Zhao B"
      ],
      "abstract": "Mammalian adipose tissue can be divided into two major types, namely, white adipose tissue (WAT) and brown adipose tissue (BAT). According to classical view, the main function of WAT is to store excess energy in the form of triglycerides, while BAT is a thermogenic tissue that acts a pivotal part in maintaining the core body temperature. White adipocytes display high plasticity and can transdifferentiate into beige adipocytes which have many similar morphological and functional properties with brown adipocytes under the stimulations of exercise, cold exposure and other factors. This phenomenon is also known as 'browning of WAT'. In addition to transdifferentiation, beige adipocytes can also come from de novo differentiation from tissue-resident progenitors. Activating BAT and inducing browning of WAT can accelerate the intake of glycolipids and reduce the insulin secretion requirement, which may be a new strategy to improve glycolipids metabolism and insulin resistance of obese and type 2 diabetes mellitus (T2DM) patients. This review mainly discusses the significance of brown and beige adipose tissues in the treatment of obesity and T2DM, and focuses on the effect of the browning agent on obesity and T2DM, which provides a brand-new theoretical reference for the prevention and treatment of obesity and T2DM.",
      "doi": "https://doi.org/10.1080/21623945.2020.1870060",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "New approach to diabetes care: from blood glucose to cardiovascular disease.",
      "authors": [
        "Aguiar C",
        "Duarte R",
        "Carvalho D"
      ],
      "abstract": "Diabetes, a metabolic disease with vascular consequences due to accelerated atherosclerosis, is one of the 21st century's most prevalent chronic diseases. Characterized by inability to produce or use insulin, leading to hyperglycemia and insulin deficiency, diabetes causes a variety of microvascular (such as retinopathy and kidney disease) and macrovascular complications (including myocardial infarction and stroke) which reduce the quality of life and life expectancy of individuals with diabetes. We describe the close relationship between diabetes, cardiovascular risk factors, and cardiovascular disease, and examine multifactorial approaches to diabetes treatment, including reducing cardiovascular risk in individuals with type 2 diabetes. Finally, we analyze new prospects for the treatment of type 2 diabetes, resulting from the development of novel antidiabetic drugs. The aim of this review is that the clinician should assume the crucial role of guiding individuals with diabetes in the control of their disease, in order to improve their quality of life and prognosis. In view of the currently available evidence, the emergence of new glucose-reducing therapies with proven cardiovascular benefit means that the best therapeutic strategy for diabetes must go beyond reducing hyperglycemia and aim to reduce cardiovascular risk.",
      "doi": "https://doi.org/10.1016/j.repc.2018.03.013",
      "year": 2019,
      "source": "PubMed"
    },
    {
      "title": "Rab family of small GTPases: an updated view on their regulation and functions.",
      "authors": [
        "Homma Y",
        "Hiragi S",
        "Fukuda M"
      ],
      "abstract": "The Rab family of small GTPases regulates intracellular membrane trafficking by orchestrating the biogenesis, transport, tethering, and fusion of membrane-bound organelles and vesicles. Like other small GTPases, Rabs cycle between two states, an active (GTP-loaded) state and an inactive (GDP-loaded) state, and their cycling is catalyzed by guanine nucleotide exchange factors (GEFs) and GTPase-activating proteins (GAPs). Because an active form of each Rab localizes on a specific organelle (or vesicle) and recruits various effector proteins to facilitate each step of membrane trafficking, knowing when and where Rabs are activated and what effectors Rabs recruit is crucial to understand their functions. Since the discovery of Rabs, they have been regarded as one of the central hubs for membrane trafficking, and numerous biochemical and genetic studies have revealed the mechanisms of Rab functions in recent years. The results of these studies have included the identification and characterization of novel GEFs, GAPs, and effectors, as well as post-translational modifications, for example, phosphorylation, of Rabs. Rab functions beyond the simple effector-recruiting model are also emerging. Furthermore, the recently developed CRISPR/Cas technology has enabled acceleration of knockout analyses in both animals and cultured cells and revealed previously unknown physiological roles of many Rabs. In this review article, we provide the most up-to-date and comprehensive lists of GEFs, GAPs, effectors, and knockout phenotypes of mammalian Rabs and discuss recent findings in regard to their regulation and functions.",
      "doi": "https://doi.org/10.1111/febs.15453",
      "year": 2021,
      "source": "PubMed"
    },
    {
      "title": "Exosomes in Parkinson's Disease.",
      "authors": [
        "Wu X",
        "Zheng T",
        "Zhang B"
      ],
      "abstract": "Exosomes, nano-sized extracellular vesicles secreted by most cell types, are found in all kinds of biological fluids and tissues, including the central nervous system (CNS). The proposed functions of these vesicles include roles in cell-cell signaling, removal of cellular debris, and transfer of pathogens between cells. Many studies have revealed that exosomes derived from the CNS occur in the cerebrospinal fluid and peripheral body fluids, and their contents are altered during disease, making them an appealing target for biomarker development in Parkinson's disease (PD). Exosomes have been shown to spread toxic α-synuclein (αsyn) between cells and induce apoptosis, which suggests a key mechanism underlying the spread of αsyn aggregates in the brain and the acceleration of pathology in PD. However, potential neuroprotective roles of exosomes in PD have also been reported. On the treatment side, as drug delivery vehicles, exosomes have been used to deliver small interfering RNAs and catalase to the brain, and have shown clear therapeutic effects in a mouse model of PD. These features of exosomes in PD make them extremely interesting from the point of view of developing novel diagnostic and therapeutic approaches.",
      "doi": "https://doi.org/10.1007/s12264-016-0092-z",
      "year": 2017,
      "source": "PubMed"
    },
    {
      "title": "MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance\n  Fields",
      "authors": [
        "Jaeyoung Chung",
        "Kanggeon Lee",
        "Sungyong Baik",
        "Kyoung Mu Lee"
      ],
      "abstract": "Hinged on the representation power of neural networks, neural radiance fields\n(NeRF) have recently emerged as one of the promising and widely applicable\nmethods for 3D object and scene representation. However, NeRF faces challenges\nin practical applications, such as large-scale scenes and edge devices with a\nlimited amount of memory, where data needs to be processed sequentially. Under\nsuch incremental learning scenarios, neural networks are known to suffer\ncatastrophic forgetting: easily forgetting previously seen data after training\nwith new data. We observe that previous incremental learning algorithms are\nlimited by either low performance or memory scalability issues. As such, we\ndevelop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF).\nMEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve\nas a memory that provides the pixel RGB values, given rays as queries. Upon the\nmotivation, our framework learns which rays to query NeRF to extract previous\npixel values. The extracted pixel values are then used to train NeRF in a\nself-distillation manner to prevent catastrophic forgetting. As a result,\nMEIL-NeRF demonstrates constant memory consumption and competitive performance.",
      "doi": "arXiv:2212.08328v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via\n  Algorithm-Hardware Co-Design",
      "authors": [
        "Yonggan Fu",
        "Zhifan Ye",
        "Jiayi Yuan",
        "Shunyao Zhang",
        "Sixu Li",
        "Haoran You",
        "Yingyan Celine Lin"
      ],
      "abstract": "Novel view synthesis is an essential functionality for enabling immersive\nexperiences in various Augmented- and Virtual-Reality (AR/VR) applications, for\nwhich generalizable Neural Radiance Fields (NeRFs) have gained increasing\npopularity thanks to their cross-scene generalization capability. Despite their\npromise, the real-device deployment of generalizable NeRFs is bottlenecked by\ntheir prohibitive complexity due to the required massive memory accesses to\nacquire scene features, causing their ray marching process to be\nmemory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware\nco-design framework dedicated to generalizable NeRF acceleration, which for the\nfirst time enables real-time generalizable NeRFs. On the algorithm side,\nGen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact\nthat different regions of a 3D scene contribute differently to the rendered\npixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF\nhighlights an accelerator micro-architecture to maximize the data reuse\nopportunities among different rays by making use of their epipolar geometric\nrelationship. Furthermore, our Gen-NeRF accelerator features a customized\ndataflow to enhance data locality during point-to-hardware mapping and an\noptimized scene feature storage strategy to minimize memory bank conflicts.\nExtensive experiments validate the effectiveness of our proposed Gen-NeRF\nframework in enabling real-time and generalizable novel view synthesis.",
      "doi": "arXiv:2304.11842v4",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "NeRFuser: Large-Scale Scene Representation by NeRF Fusion",
      "authors": [
        "Jiading Fang",
        "Shengjie Lin",
        "Igor Vasiljevic",
        "Vitor Guizilini",
        "Rares Ambrus",
        "Adrien Gaidon",
        "Gregory Shakhnarovich",
        "Matthew R. Walter"
      ],
      "abstract": "A practical benefit of implicit visual representations like Neural Radiance\nFields (NeRFs) is their memory efficiency: large scenes can be efficiently\nstored and shared as small neural nets instead of collections of images.\nHowever, operating on these implicit visual data structures requires extending\nclassical image-based vision techniques (e.g., registration, blending) from\nimage sets to neural fields. Towards this goal, we propose NeRFuser, a novel\narchitecture for NeRF registration and blending that assumes only access to\npre-generated NeRFs, and not the potentially large sets of images used to\ngenerate them. We propose registration from re-rendering, a technique to infer\nthe transformation between NeRFs based on images synthesized from individual\nNeRFs. For blending, we propose sample-based inverse distance weighting to\nblend visual information at the ray-sample level. We evaluate NeRFuser on\npublic benchmarks and a self-collected object-centric indoor dataset, showing\nthe robustness of our method, including to views that are challenging to render\nfrom the individual source NeRFs.",
      "doi": "arXiv:2305.13307v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Efficient Deformable Tissue Reconstruction via Orthogonal Neural Plane.",
      "authors": [
        "Yang C",
        "Wang K",
        "Wang Y",
        "Dou Q",
        "Yang X",
        "Shen W"
      ],
      "abstract": "Intraoperative imaging techniques for reconstructing deformable tissues in vivo are pivotal for advanced surgical systems. Existing methods either compromise on rendering quality or are excessively computationally intensive, often demanding dozens of hours to perform, which significantly hinders their practical application. In this paper, we introduce Fast Orthogonal Plane (Forplane), a novel, efficient framework based on neural radiance fields (NeRF) for the reconstruction of deformable tissues. We conceptualize surgical procedures as 4D volumes, and break them down into static and dynamic fields comprised of orthogonal neural planes. This factorization discretizes the four-dimensional space, leading to a decreased memory usage and faster optimization. A spatiotemporal importance sampling scheme is introduced to improve performance in regions with tool occlusion as well as large motions and accelerate training. An efficient ray marching method is applied to skip sampling among empty regions, significantly improving inference speed. Forplane accommodates both binocular and monocular endoscopy videos, demonstrating its extensive applicability and flexibility. Our experiments, carried out on two in vivo datasets, the EndoNeRF and Hamlyn datasets, demonstrate the effectiveness of our framework. In all cases, Forplane substantially accelerates both the optimization process (by over 100 times) and the inference process (by over 15 times) while maintaining or even improving the quality across a variety of non-rigid deformations. This significant performance improvement promises to be a valuable asset for future intraoperative surgical applications. The code of our project is now available at https://github.com/Loping151/ForPlane.",
      "doi": "https://doi.org/10.1109/tmi.2024.3388559",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "In silico analysis of alternative splicing events implicated in intracellular trafficking during B-lymphocyte differentiation.",
      "authors": [
        "Ostwaldt F",
        "Los B",
        "Heyd F"
      ],
      "abstract": "There are multiple regulatory layers that control intracellular trafficking and protein secretion, ranging from transcriptional to posttranslational mechanisms. Finely regulated trafficking and secretion is especially important for lymphocytes during activation and differentiation, as the quantity of secretory cargo increases once the activated cells start to produce and secrete large amounts of cytokines, cytotoxins, or antibodies. However, how the secretory machinery dynamically adapts its efficiency and specificity in general and specifically in lymphocytes remains incompletely understood. Here we present a systematic bioinformatics analysis to address RNA-based mechanisms that control intracellular trafficking and protein secretion during B-lymphocyte activation, and differentiation, with a focus on alternative splicing. Our",
      "doi": "https://doi.org/10.3389/fimmu.2022.1030409",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "CIPS-3D++: End-to-End Real-Time High-Resolution 3D-Aware GANs for GAN Inversion and Stylization.",
      "authors": [
        "Zhou P",
        "Xie L",
        "Ni B",
        "Tian Q"
      ],
      "abstract": "Style-based GANs achieve state-of-the-art results for generating high-quality images, but lack explicit and precise control over camera poses. Recently proposed NeRF-based GANs have made great progress towards 3D-aware image generation. However, the methods either rely on convolution operators which are not rotationally invariant, or utilize complex yet suboptimal training procedures to integrate both NeRF and CNN sub-structures, yielding un-robust, low-quality images with a large computational burden. This article presents an upgraded version called CIPS-3D++, aiming at high-robust, high-resolution and high-efficiency 3D-aware GANs. On the one hand, our basic model CIPS-3D, encapsulated in a style-based architecture, features a shallow NeRF-based 3D shape encoder as well as a deep MLP-based 2D image decoder, achieving robust image generation/editing with rotation-invariance. On the other hand, our proposed CIPS-3D++, inheriting the rotational invariance of CIPS-3D, together with geometric regularization and upsampling operations, encourages high-resolution high-quality image generation/editing with great computational efficiency. Trained on raw single-view images, without any bells and whistles, CIPS-3D++ sets new records for 3D-aware image synthesis, with an impressive FID of 3.2 on FFHQ at the 1024×1024 resolution. In the meantime, CIPS-3D++ runs efficiently and enjoys a low GPU memory footprint so that it can be trained end-to-end on high-resolution images directly, in contrast to previous alternate/progressive methods. Based on the infrastructure of CIPS-3D++, we propose a 3D-aware GAN inversion algorithm named FlipInversion, which can reconstruct the 3D object from a single-view image. We also provide a 3D-aware stylization method for real images based on CIPS-3D++ and FlipInversion. In addition, we analyze the problem of mirror symmetry suffered in training, and solve it by introducing an auxiliary discriminator for the NeRF network. Overall, CIPS-3D++ provides a strong base model that can serve as a testbed for transferring GAN-based image editing methods from 2D to 3D.",
      "doi": "https://doi.org/10.1109/tpami.2023.3285648",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Neural Controlled Differential Equations for Online Prediction Tasks",
      "authors": [
        "James Morrill",
        "Patrick Kidger",
        "Lingyi Yang",
        "Terry Lyons"
      ],
      "abstract": "Neural controlled differential equations (Neural CDEs) are a continuous-time\nextension of recurrent neural networks (RNNs), achieving state-of-the-art\n(SOTA) performance at modelling functions of irregular time series. In order to\ninterpret discrete data in continuous time, current implementations rely on\nnon-causal interpolations of the data. This is fine when the whole time series\nis observed in advance, but means that Neural CDEs are not suitable for use in\n\\textit{online prediction tasks}, where predictions need to be made in\nreal-time: a major use case for recurrent networks. Here, we show how this\nlimitation may be rectified. First, we identify several theoretical conditions\nthat interpolation schemes for Neural CDEs should satisfy, such as boundedness\nand uniqueness. Second, we use these to motivate the introduction of new\nschemes that address these conditions, offering in particular measurability\n(for online prediction), and smoothness (for speed). Third, we empirically\nbenchmark our online Neural CDE model on three continuous monitoring tasks from\nthe MIMIC-IV medical database: we demonstrate improved performance on all tasks\nagainst ODE benchmarks, and on two of the three tasks against SOTA non-ODE\nbenchmarks.",
      "doi": "arXiv:2106.11028v1",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Neural Scene Baking for Permutation Invariant Transparency Rendering\n  with Real-time Global Illumination",
      "authors": [
        "Ziyang Zhang",
        "Edgar Simo-Serra"
      ],
      "abstract": "Neural rendering provides a fundamentally new way to render photorealistic\nimages. Similar to traditional light-baking methods, neural rendering utilizes\nneural networks to bake representations of scenes, materials, and lights into\nlatent vectors learned from path-tracing ground truths. However, existing\nneural rendering algorithms typically use G-buffers to provide position,\nnormal, and texture information of scenes, which are prone to occlusion by\ntransparent surfaces, leading to distortions and loss of detail in the rendered\nimages. To address this limitation, we propose a novel neural rendering\npipeline that accurately renders the scene behind transparent surfaces with\nglobal illumination and variable scenes. Our method separates the G-buffers of\nopaque and transparent objects, retaining G-buffer information behind\ntransparent objects. Additionally, to render the transparent objects with\npermutation invariance, we designed a new permutation-invariant neural blending\nfunction. We integrate our algorithm into an efficient custom renderer to\nachieve real-time performance. Our results show that our method is capable of\nrendering photorealistic images with variable scenes and viewpoints, accurately\ncapturing complex transparent structures along with global illumination. Our\nrenderer can achieve real-time performance ($256\\times 256$ at 63 FPS and\n$512\\times 512$ at 32 FPS) on scenes with multiple variable transparent\nobjects.",
      "doi": "arXiv:2405.19056v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Volume Feature Rendering for Fast Neural Radiance Field Reconstruction",
      "authors": [
        "Kang Han",
        "Wei Xiang",
        "Lu Yu"
      ],
      "abstract": "Neural radiance fields (NeRFs) are able to synthesize realistic novel views\nfrom multi-view images captured from distinct positions and perspectives. In\nNeRF's rendering pipeline, neural networks are used to represent a scene\nindependently or transform queried learnable feature vector of a point to the\nexpected color or density. With the aid of geometry guides either in occupancy\ngrids or proposal networks, the number of neural network evaluations can be\nreduced from hundreds to dozens in the standard volume rendering framework.\nInstead of rendering yielded color after neural network evaluation, we propose\nto render the queried feature vectors of a ray first and then transform the\nrendered feature vector to the final pixel color by a neural network. This\nfundamental change to the standard volume rendering framework requires only one\nsingle neural network evaluation to render a pixel, which substantially lowers\nthe high computational complexity of the rendering framework attributed to a\nlarge number of neural network evaluations. Consequently, we can use a\ncomparably larger neural network to achieve a better rendering quality while\nmaintaining the same training and rendering time costs. Our model achieves the\nstate-of-the-art rendering quality on both synthetic and real-world datasets\nwhile requiring a training time of several minutes.",
      "doi": "arXiv:2305.17916v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "ENVIDR: Implicit Differentiable Renderer with Neural Environment\n  Lighting",
      "authors": [
        "Ruofan Liang",
        "Huiting Chen",
        "Chunlin Li",
        "Fan Chen",
        "Selvakumar Panneer",
        "Nandita Vijaykumar"
      ],
      "abstract": "Recent advances in neural rendering have shown great potential for\nreconstructing scenes from multiview images. However, accurately representing\nobjects with glossy surfaces remains a challenge for existing methods. In this\nwork, we introduce ENVIDR, a rendering and modeling framework for high-quality\nrendering and reconstruction of surfaces with challenging specular reflections.\nTo achieve this, we first propose a novel neural renderer with decomposed\nrendering components to learn the interaction between surface and environment\nlighting. This renderer is trained using existing physically based renderers\nand is decoupled from actual scene representations. We then propose an\nSDF-based neural surface model that leverages this learned neural renderer to\nrepresent general scenes. Our model additionally synthesizes indirect\nilluminations caused by inter-reflections from shiny surfaces by marching\nsurface-reflected rays. We demonstrate that our method outperforms state-of-art\nmethods on challenging shiny scenes, providing high-quality rendering of\nspecular reflections while also enabling material editing and scene relighting.",
      "doi": "arXiv:2303.13022v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Real-Time Neural Homogeneous Translucent Material Rendering Using Diffusion Blocks.",
      "authors": [
        "An D",
        "Kang L",
        "Xu K"
      ],
      "abstract": "Rendering realistic appearances of homogeneous translucent materials, such as milk and marble, poses challenges due to the complexity of subsurface scattering. In this paper, we present a neural method for real-time rendering of homogeneous translucent objects. Based on the observation that light propagation inside a highly scattered media is like a diffusion process [1], we propose a neural data structure named diffusion block to mimic the behavior of the diffusion process. The diffusion block is built upon a recent network structure named DiffusionNet [2] with a few modifications to adapt to our problem of translucent rendering. Our network is lightweight and efficient, leading to a real-time rendering method. Furthermore, our method supports dynamic material properties and diverse lighting conditions. Comparisons with state-of-the-art real-time translucent rendering methods demonstrate the superiority of our method in rendering quality.",
      "doi": "https://doi.org/10.1109/tvcg.2025.3548442",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "MNSS: Neural Supersampling Framework for Real-Time Rendering on Mobile Devices.",
      "authors": [
        "Yang S",
        "Zhao Y",
        "Luo Y",
        "Wang H",
        "Sun H",
        "Li C",
        "Cai B",
        "Jin X"
      ],
      "abstract": "Although neural supersampling has achieved great success in various applications for improving image quality, it is still difficult to apply it to a wide range of real-time rendering applications due to the high computational power demand. Most existing methods are computationally expensive and require high-performance hardware, preventing their use on platforms with limited hardware, such as smartphones. To this end, we propose a new supersampling framework for real-time rendering applications to reconstruct a high-quality image out of a low-resolution one, which is sufficiently lightweight to run on smartphones within a real-time budget. Our model takes as input the renderer-generated low resolution content and produces high resolution and anti-aliased results. To maximize sampling efficiency, we propose using an alternate sub-pixel sample pattern during the rasterization process. This allows us to create a relatively small reconstruction model while maintaining high image quality. By accumulating new samples into a high-resolution history buffer, an efficient history check and re-usage scheme is introduced to improve temporal stability. To our knowledge, this is the first research in pushing real-time neural supersampling on mobile devices. Due to the absence of training data, we present a new dataset containing 57 training and test sequences from three game scenes. Furthermore, based on the rendered motion vectors and a visual perception study, we introduce a new metric called inter-frame structural similarity (IF-SSIM) to quantitatively measure the temporal stability of rendered videos. Extensive evaluations demonstrate that our supersampling model outperforms existing or alternative solutions in both performance and temporal stability.",
      "doi": "https://doi.org/10.1109/tvcg.2023.3259141",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Real-Time Lighting Estimation for Augmented Reality via Differentiable Screen-Space Rendering.",
      "authors": [
        "Liu C",
        "Wang L",
        "Li Z",
        "Quan S",
        "Xu Y"
      ],
      "abstract": "Augmented Reality (AR) applications aim to provide realistic blending between the real-world and virtual objects. One of the important factors for realistic AR is the correct lighting estimation. In this article, we present a method that estimates the real-world lighting condition from a single image in real time, using information from an optional support plane provided by advanced AR frameworks (e.g., ARCore, ARKit, etc.). By analyzing the visual appearance of the real scene, our algorithm can predict the lighting condition from the input RGB photo. In the first stage, we use a deep neural network to decompose the scene into several components: lighting, normal, and Bidirectional Reflectance Distribution Function (BRDF). Then we introduce differentiable screen-space rendering, a novel approach to providing the supervisory signal for regressing lighting, normal, and BRDF jointly. We recover the most plausible real-world lighting condition using Spherical Harmonics and the main directional lighting. Through a variety of experimental results, we demonstrate that our method can provide improved results than prior works quantitatively and qualitatively, and it can enhance the real-time AR experiences.",
      "doi": "https://doi.org/10.1109/tvcg.2022.3141943",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "PowerNet: Learning-Based Real-Time Power-Budget Rendering.",
      "authors": [
        "Zhang Y",
        "Wang R",
        "Huo Y",
        "Hua W",
        "Bao H"
      ],
      "abstract": "With the prevalence of embedded GPUs on mobile devices, power-efficient rendering has become a widespread concern for graphics applications. Reducing the power consumption of rendering applications is critical for extending battery life. In this paper, we present a new real-time power-budget rendering system to meet this need by selecting the optimal rendering settings that maximize visual quality for each frame under a given power budget. Our method utilizes two independent neural networks trained entirely by synthesized datasets to predict power consumption and image quality under various workloads. This approach spares time-consuming precomputation or runtime periodic refitting and additional error computation. We evaluate the performance of the proposed framework on different platforms, two desktop PCs and two smartphones. Results show that compared to the previous state of the art, our system has less overhead and better flexibility. Existing rendering engines can integrate our system with negligible costs.",
      "doi": "https://doi.org/10.1109/tvcg.2021.3064367",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of\n  Real World Objects",
      "authors": [
        "Marco Toschi",
        "Riccardo De Matteo",
        "Riccardo Spezialetti",
        "Daniele De Gregorio",
        "Luigi Di Stefano",
        "Samuele Salti"
      ],
      "abstract": "In this paper, we focus on the problem of rendering novel views from a Neural\nRadiance Field (NeRF) under unobserved light conditions. To this end, we\nintroduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world\nobjects under one-light-at-time (OLAT) conditions, annotated with accurate\nground-truth camera and light poses. Our acquisition pipeline leverages two\nrobotic arms holding, respectively, a camera and an omni-directional point-wise\nlight source. We release a total of 20 scenes depicting a variety of objects\nwith complex geometry and challenging materials. Each scene includes 2000\nimages, acquired from 50 different points of views under 40 different OLAT\nconditions. By leveraging the dataset, we perform an ablation study on the\nrelighting capability of variants of the vanilla NeRF architecture and identify\na lightweight architecture that can render novel views of an object under novel\nlight conditions, which we use to establish a non-trivial baseline for the\ndataset. Dataset and benchmark are available at\nhttps://eyecan-ai.github.io/rene.",
      "doi": "arXiv:2304.10448v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple\n  Super-Resolution Pipeline",
      "authors": [
        "Chien-Yu Lin",
        "Qichen Fu",
        "Thomas Merth",
        "Karren Yang",
        "Anurag Ranjan"
      ],
      "abstract": "Super-resolution (SR) techniques have recently been proposed to upscale the\noutputs of neural radiance fields (NeRF) and generate high-quality images with\nenhanced inference speeds. However, existing NeRF+SR methods increase training\noverhead by using extra input features, loss functions, and/or expensive\ntraining procedures such as knowledge distillation. In this paper, we aim to\nleverage SR for efficiency gains without costly training or architectural\nchanges. Specifically, we build a simple NeRF+SR pipeline that directly\ncombines existing modules, and we propose a lightweight augmentation technique,\nrandom patch sampling, for training. Compared to existing NeRF+SR methods, our\npipeline mitigates the SR computing overhead and can be trained up to 23x\nfaster, making it feasible to run on consumer devices such as the Apple\nMacBook. Experiments show our pipeline can upscale NeRF outputs by 2-4x while\nmaintaining high quality, increasing inference speeds by up to 18x on an NVIDIA\nV100 GPU and 12.8x on an M1 Pro chip. We conclude that SR can be a simple but\neffective technique for improving the efficiency of NeRF models for consumer\ndevices.",
      "doi": "arXiv:2312.11537v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context",
      "authors": [
        "Binglun Wang",
        "Niladri Shekhar Dutt",
        "Niloy J. Mitra"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) have recently emerged as a popular option for\nphoto-realistic object capture due to their ability to faithfully capture\nhigh-fidelity volumetric content even from handheld video input. Although much\nresearch has been devoted to efficient optimization leading to real-time\ntraining and rendering, options for interactive editing NeRFs remain limited.\nWe present a very simple but effective neural network architecture that is fast\nand efficient while maintaining a low memory footprint. This architecture can\nbe incrementally guided through user-friendly image-based edits. Our\nrepresentation allows straightforward object selection via semantic feature\ndistillation at the training stage. More importantly, we propose a local\n3D-aware image context to facilitate view-consistent image editing that can\nthen be distilled into fine-tuned NeRFs, via geometric and appearance\nadjustments. We evaluate our setup on a variety of examples to demonstrate\nappearance and geometric edits and report 10-30x speedup over concurrent work\nfocusing on text-guided NeRF editing. Video results can be seen on our project\nwebpage at https://proteusnerf.github.io.",
      "doi": "arXiv:2310.09965v3",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "NeRF-RPN: A general framework for object detection in NeRFs",
      "authors": [
        "Benran Hu",
        "Junkai Huang",
        "Yichen Liu",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "abstract": "This paper presents the first significant object detection framework,\nNeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,\nNeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting\na novel voxel representation that incorporates multi-scale 3D neural volumetric\nfeatures, we demonstrate it is possible to regress the 3D bounding boxes of\nobjects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN\nis a general framework and can be applied to detect objects without class\nlabels. We experimented NeRF-RPN with various backbone architectures, RPN head\ndesigns and loss functions. All of them can be trained in an end-to-end manner\nto estimate high quality 3D bounding boxes. To facilitate future research in\nobject detection for NeRF, we built a new benchmark dataset which consists of\nboth synthetic and real-world data with careful labeling and clean up. Code and\ndataset are available at https://github.com/lyclyc52/NeRF_RPN.",
      "doi": "arXiv:2211.11646v3",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera\n  Trajectories",
      "authors": [
        "Peng Wang",
        "Yuan Liu",
        "Zhaoxi Chen",
        "Lingjie Liu",
        "Ziwei Liu",
        "Taku Komura",
        "Christian Theobalt",
        "Wenping Wang"
      ],
      "abstract": "This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF)\nfor novel view synthesis, which enables arbitrary input camera trajectories and\nonly costs a few minutes for training. Existing fast grid-based NeRF training\nframeworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed\nfor bounded scenes and rely on space warping to handle unbounded scenes.\nExisting two widely-used space-warping methods are only designed for the\nforward-facing trajectory or the 360-degree object-centric trajectory but\ncannot process arbitrary trajectories. In this paper, we delve deep into the\nmechanism of space warping to handle unbounded scenes. Based on our analysis,\nwe further propose a novel space-warping method called perspective warping,\nwhich allows us to handle arbitrary trajectories in the grid-based NeRF\nframework. Extensive experiments demonstrate that F2-NeRF is able to use the\nsame perspective warping to render high-quality images on two standard datasets\nand a new free trajectory dataset collected by us. Project page:\nhttps://totoro97.github.io/projects/f2-nerf.",
      "doi": "arXiv:2303.15951v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural\n  Radiance Fields",
      "authors": [
        "Takuhiro Kaneko"
      ],
      "abstract": "Neural radiance fields (NeRFs) have shown impressive results for novel view\nsynthesis. However, they depend on the repetitive use of a single-input\nsingle-output multilayer perceptron (SISO MLP) that maps 3D coordinates and\nview direction to the color and volume density in a sample-wise manner, which\nslows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF)\nthat reduces the number of MLPs running by replacing the SISO MLP with a MIMO\nMLP and conducting mappings in a group-wise manner. One notable challenge with\nthis approach is that the color and volume density of each point can differ\naccording to a choice of input coordinates in a group, which can lead to some\nnotable ambiguity. We also propose a self-supervised learning method that\nregularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this\nambiguity without using pretrained models. The results of a comprehensive\nexperimental evaluation including comparative and ablation studies are\npresented to show that MIMO-NeRF obtains a good trade-off between speed and\nquality with a reasonable training time. We then demonstrate that MIMO-NeRF is\ncompatible with and complementary to previous advancements in NeRFs by applying\nit to two representative fast NeRFs, i.e., a NeRF with sample reduction\n(DONeRF) and a NeRF with alternative representations (TensoRF).",
      "doi": "arXiv:2310.01821v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance\n  Fields via Semantic Distillation",
      "authors": [
        "Dadong Jiang",
        "Zhihui Ke",
        "Xiaobo Zhou",
        "Xidong Shi"
      ],
      "abstract": "This paper targets interactive object-level editing (e.g., deletion,\nrecoloring, transformation, composition) in dynamic scenes. Recently, some\nmethods aiming for flexible editing static scenes represented by neural\nradiance field (NeRF) have shown impressive synthesis quality, while similar\ncapabilities in time-variant dynamic scenes remain limited. To solve this\nproblem, we propose 4D-Editor, an interactive semantic-driven editing\nframework, allowing editing multiple objects in a dynamic NeRF with user\nstrokes on a single frame. We propose an extension to the original dynamic NeRF\nby incorporating a hybrid semantic feature distillation to maintain\nspatial-temporal consistency after editing. In addition, we design Recursive\nSelection Refinement that significantly boosts object segmentation accuracy\nwithin a dynamic NeRF to aid the editing process. Moreover, we develop\nMulti-view Reprojection Inpainting to fill holes caused by incomplete scene\ncapture after editing. Extensive experiments and editing examples on real-world\ndemonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs.\nProject page: https://patrickddj.github.io/4D-Editor",
      "doi": "arXiv:2310.16858v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models",
      "authors": [
        "Xingchen Zhou",
        "Ying He",
        "F. Richard Yu",
        "Jianqiang Li",
        "You Li"
      ],
      "abstract": "The emergence of Neural Radiance Fields (NeRF) has promoted the development\nof synthesized high-fidelity views of the intricate real world. However, it is\nstill a very demanding task to repaint the content in NeRF. In this paper, we\npropose a novel framework that can take RGB images as input and alter the 3D\ncontent in neural scenes. Our work leverages existing diffusion models to guide\nchanges in the designated 3D content. Specifically, we semantically select the\ntarget object and a pre-trained diffusion model will guide the NeRF model to\ngenerate new 3D objects, which can improve the editability, diversity, and\napplication range of NeRF. Experiment results show that our algorithm is\neffective for editing 3D objects in NeRF under different text prompts,\nincluding editing appearance, shape, and more. We validate our method on both\nreal-world datasets and synthetic-world datasets for these editing tasks.\nPlease visit https://starstesla.github.io/repaintnerf for a better view of our\nresults.",
      "doi": "arXiv:2306.05668v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields",
      "authors": [
        "Ning Wang",
        "Lefei Zhang",
        "Angel X Chang"
      ],
      "abstract": "Neural fields (NeRF) have emerged as a promising approach for representing\ncontinuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs\nposes a significant challenge for scene decomposition. To address this\nchallenge, we present a single model, Multi-Modal Decomposition NeRF\n(${M^2D}$NeRF), that is capable of both text-based and visual patch-based\nedits. Specifically, we use multi-modal feature distillation to integrate\nteacher features from pretrained visual and language models into 3D semantic\nfeature volumes, thereby facilitating consistent 3D editing. To enforce\nconsistency between the visual and language features in our 3D feature volumes,\nwe introduce a multi-modal similarity constraint. We also introduce a\npatch-based joint contrastive loss that helps to encourage object-regions to\ncoalesce in the 3D feature space, resulting in more precise boundaries.\nExperiments on various real-world scenes show superior performance in 3D scene\ndecomposition tasks compared to prior NeRF-based methods.",
      "doi": "arXiv:2405.05010v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing\n  Field",
      "authors": [
        "Chong Bao",
        "Yinda Zhang",
        "Bangbang Yang",
        "Tianxing Fan",
        "Zesong Yang",
        "Hujun Bao",
        "Guofeng Zhang",
        "Zhaopeng Cui"
      ],
      "abstract": "Despite the great success in 2D editing using user-friendly tools, such as\nPhotoshop, semantic strokes, or even text prompts, similar capabilities in 3D\nareas are still limited, either relying on 3D modeling skills or allowing\nediting within only a few categories. In this paper, we present a novel\nsemantic-driven NeRF editing approach, which enables users to edit a neural\nradiance field with a single image, and faithfully delivers edited novel views\nwith high fidelity and multi-view consistency. To achieve this goal, we propose\na prior-guided editing field to encode fine-grained geometric and texture\nediting in 3D space, and develop a series of techniques to aid the editing\nprocess, including cyclic constraints with a proxy mesh to facilitate geometric\nsupervision, a color compositing mechanism to stabilize semantic-driven texture\nediting, and a feature-cluster-based regularization to preserve the irrelevant\ncontent unchanged. Extensive experiments and editing examples on both\nreal-world and synthetic data demonstrate that our method achieves\nphoto-realistic 3D editing using only a single edited image, pushing the bound\nof semantic-driven editing in 3D real-world scenes. Our project webpage:\nhttps://zju3dv.github.io/sine/.",
      "doi": "arXiv:2303.13277v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Interactive NeRF Geometry Editing With Shape Priors.",
      "authors": [
        "Yuan YJ",
        "Sun YT",
        "Lai YK",
        "Ma Y",
        "Jia R",
        "Kobbelt L",
        "Gao L"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) have shown great potential for tasks like novel view synthesis of static 3D scenes. Since NeRFs are trained on a large number of input images, it is not trivial to change their content afterwards. Previous methods to modify NeRFs provide some control but they do not support direct shape deformation which is common for geometry representations like triangle meshes. In this paper, we present a NeRF geometry editing method that first extracts a triangle mesh representation of the geometry inside a NeRF. This mesh can be modified by any 3D modeling tool (we use ARAP mesh deformation). The mesh deformation is then extended into a volume deformation around the shape which establishes a mapping between ray queries to the deformed NeRF and the corresponding queries to the original NeRF. The basic shape editing mechanism is extended towards more powerful and more meaningful editing handles by generating box abstractions of the NeRF shapes which provide an intuitive interface to the user. By additionally assigning semantic labels, we can even identify and combine parts from different objects. We demonstrate the performance and quality of our method in a number of experiments on synthetic data as well as real captured scenes.",
      "doi": "https://doi.org/10.1109/tpami.2023.3315068",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations.",
      "authors": [
        "Tschernezki V",
        "Laina I",
        "Larlus D",
        "Vedaldi A"
      ],
      "abstract": "We present Neural Feature Fusion Fields (N3F), a method that improves dense 2D image feature extractors when the latter are applied to the analysis of multiple images reconstructible as a 3D scene. Given an image feature extractor, for example pre-trained using self-supervision, N3F uses it as a teacher to learn a student network defined in 3D space. The 3D student network is similar to a neural radiance field that distills said features and can be trained with the usual differentiable rendering machinery. As a consequence, N3F is readily applicable to most neural rendering formulations, including vanilla NeRF and its extensions to complex dynamic scenes. We show that our method not only enables semantic understanding in the context of scene-specific neural fields without the use of manual labels,but also consistently improves over the self-supervised 2D baselines. This is demonstrated by considering various tasks, such as 2D object retrieval, 3D segmentation, and scene editing, in diverse sequences, including long egocentric videos in the EPIC-KITCHENS benchmark. Project page: https://www.robots.ox.ac.uk/~vadim/n3f/.",
      "doi": "https://pubmed.ncbi.nlm.nih.gov/39404685/",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "Dynamic Scene Understanding Through Object-Centric Voxelization and Neural Rendering.",
      "authors": [
        "Zhao Y",
        "Hao Y",
        "Gao S",
        "Wang Y",
        "Yang X"
      ],
      "abstract": "Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.",
      "doi": "https://doi.org/10.1109/tpami.2025.3539866",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "Super-NeRF: View-consistent Detail Generation for NeRF super-resolution",
      "authors": [
        "Yuqi Han",
        "Tao Yu",
        "Xiaohang Yu",
        "Yuwang Wang",
        "Qionghai Dai"
      ],
      "abstract": "The neural radiance field (NeRF) achieved remarkable success in modeling 3D\nscenes and synthesizing high-fidelity novel views. However, existing NeRF-based\nmethods focus more on the make full use of the image resolution to generate\nnovel views, but less considering the generation of details under the limited\ninput resolution. In analogy to the extensive usage of image super-resolution,\nNeRF super-resolution is an effective way to generate the high-resolution\nimplicit representation of 3D scenes and holds great potential applications. Up\nto now, such an important topic is still under-explored. In this paper, we\npropose a NeRF super-resolution method, named Super-NeRF, to generate\nhigh-resolution NeRF from only low-resolution inputs. Given multi-view\nlow-resolution images, Super-NeRF constructs a consistency-controlling\nsuper-resolution module to generate view-consistent high-resolution details for\nNeRF. Specifically, an optimizable latent code is introduced for each\nlow-resolution input image to control the 2D super-resolution images to\nconverge to the view-consistent output. The latent codes of each low-resolution\nimage are optimized synergistically with the target Super-NeRF representation\nto fully utilize the view consistency constraint inherent in NeRF construction.\nWe verify the effectiveness of Super-NeRF on synthetic, real-world, and\nAI-generated NeRF datasets. Super-NeRF achieves state-of-the-art NeRF\nsuper-resolution performance on high-resolution detail generation and\ncross-view consistency.",
      "doi": "arXiv:2304.13518v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Creating Visual Effects with Neural Radiance Fields",
      "authors": [
        "Cyrus Vachha"
      ],
      "abstract": "We present a pipeline for integrating NeRFs into traditional compositing VFX\npipelines using Nerfstudio, an open-source framework for training and rendering\nNeRFs. Our approach involves using Blender, a widely used open-source 3D\ncreation software, to align camera paths and composite NeRF renders with meshes\nand other NeRFs, allowing for seamless integration of NeRFs into traditional\nVFX pipelines. Our NeRF Blender add-on allows for more controlled camera\ntrajectories of photorealistic scenes, compositing meshes and other\nenvironmental effects with NeRFs, and compositing multiple NeRFs in a single\nscene.This approach of generating NeRF aligned camera paths can be adapted to\nother 3D tool sets and workflows, enabling a more seamless integration of NeRFs\ninto visual effects and film production. Documentation can be found here:\nhttps://docs.nerf.studio/extensions/blender_addon.html",
      "doi": "arXiv:2401.08633v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Hallucinated Neural Radiance Fields in the Wild",
      "authors": [
        "Xingyu Chen",
        "Qi Zhang",
        "Xiaoyu Li",
        "Yue Chen",
        "Ying Feng",
        "Xuan Wang",
        "Jue Wang"
      ],
      "abstract": "Neural Radiance Fields (NeRF) has recently gained popularity for its\nimpressive novel view synthesis ability. This paper studies the problem of\nhallucinated NeRF: i.e., recovering a realistic NeRF at a different time of day\nfrom a group of tourism images. Existing solutions adopt NeRF with a\ncontrollable appearance embedding to render novel views under various\nconditions, but they cannot render view-consistent images with an unseen\nappearance. To solve this problem, we present an end-to-end framework for\nconstructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose\nan appearance hallucination module to handle time-varying appearances and\ntransfer them to novel views. Considering the complex occlusions of tourism\nimages, we introduce an anti-occlusion module to decompose the static subjects\nfor visibility accurately. Experimental results on synthetic data and real\ntourism photo collections demonstrate that our method can hallucinate the\ndesired appearances and render occlusion-free images from different views. The\nproject and supplementary materials are available at\nhttps://rover-xingyu.github.io/Ha-NeRF/.",
      "doi": "arXiv:2111.15246v3",
      "year": 2021,
      "source": "arXiv"
    },
    {
      "title": "Block-NeRF: Scalable Large Scene Neural View Synthesis",
      "authors": [
        "Matthew Tancik",
        "Vincent Casser",
        "Xinchen Yan",
        "Sabeek Pradhan",
        "Ben Mildenhall",
        "Pratul P. Srinivasan",
        "Jonathan T. Barron",
        "Henrik Kretzschmar"
      ],
      "abstract": "We present Block-NeRF, a variant of Neural Radiance Fields that can represent\nlarge-scale environments. Specifically, we demonstrate that when scaling NeRF\nto render city-scale scenes spanning multiple blocks, it is vital to decompose\nthe scene into individually trained NeRFs. This decomposition decouples\nrendering time from scene size, enables rendering to scale to arbitrarily large\nenvironments, and allows per-block updates of the environment. We adopt several\narchitectural changes to make NeRF robust to data captured over months under\ndifferent environmental conditions. We add appearance embeddings, learned pose\nrefinement, and controllable exposure to each individual NeRF, and introduce a\nprocedure for aligning appearance between adjacent NeRFs so that they can be\nseamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to\ncreate the largest neural scene representation to date, capable of rendering an\nentire neighborhood of San Francisco.",
      "doi": "arXiv:2202.05263v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields",
      "authors": [
        "Xiangyu Wang",
        "Jingsen Zhu",
        "Qi Ye",
        "Yuchi Huo",
        "Yunlong Ran",
        "Zhihua Zhong",
        "Jiming Chen"
      ],
      "abstract": "With the popularity of implicit neural representations, or neural radiance\nfields (NeRF), there is a pressing need for editing methods to interact with\nthe implicit 3D models for tasks like post-processing reconstructed scenes and\n3D content creation. While previous works have explored NeRF editing from\nvarious perspectives, they are restricted in editing flexibility, quality, and\nspeed, failing to offer direct editing response and instant preview. The key\nchallenge is to conceive a locally editable neural representation that can\ndirectly reflect the editing instructions and update instantly. To bridge the\ngap, we propose a new interactive editing method and system for implicit\nrepresentations, called Seal-3D, which allows users to edit NeRF models in a\npixel-level and free manner with a wide range of NeRF-like backbone and preview\nthe editing effects instantly. To achieve the effects, the challenges are\naddressed by our proposed proxy function mapping the editing instructions to\nthe original space of NeRF models in the teacher model and a two-stage training\nstrategy for the student model with local pretraining and global finetuning. A\nNeRF editing system is built to showcase various editing types. Our system can\nachieve compelling editing effects with an interactive speed of about 1 second.",
      "doi": "arXiv:2307.15131v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural\n  Radiance Fields",
      "authors": [
        "Zhentao Huang",
        "Yukun Shi",
        "Neil Bruce",
        "Minglun Gong"
      ],
      "abstract": "The widespread adoption of implicit neural representations, especially Neural\nRadiance Fields (NeRF), highlights a growing need for editing capabilities in\nimplicit 3D models, essential for tasks like scene post-processing and 3D\ncontent creation. Despite previous efforts in NeRF editing, challenges remain\ndue to limitations in editing flexibility and quality. The key issue is\ndeveloping a neural representation that supports local edits for real-time\nupdates. Current NeRF editing methods, offering pixel-level adjustments or\ndetailed geometry and color modifications, are mostly limited to static scenes.\nThis paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level\nediting in dynamic settings, specifically targeting the D-NeRF network. It\nallows for consistent edits across sequences by mapping editing actions to a\nspecific timeframe, freezing the deformation network responsible for dynamic\nscene representation, and using a teacher-student approach to integrate\nchanges.",
      "doi": "arXiv:2402.13510v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian\n  Representation",
      "authors": [
        "Sitian Shen",
        "Jing Xu",
        "Yuheng Yuan",
        "Xingyi Yang",
        "Qiuhong Shen",
        "Xinchao Wang"
      ],
      "abstract": "User-friendly 3D object editing is a challenging task that has attracted\nsignificant attention recently. The limitations of direct 3D object editing\nwithout 2D prior knowledge have prompted increased attention towards utilizing\n2D generative models for 3D editing. While existing methods like Instruct\nNeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly\ndue to semantic guided editing. In the realm of 3D representation, 3D Gaussian\nSplatting emerges as a promising approach for its efficiency and natural\nexplicit property, facilitating precise editing tasks. Building upon these\ninsights, we propose DragGaussian, a 3D object drag-editing framework based on\n3D Gaussian Splatting, leveraging diffusion models for interactive image\nediting with open-vocabulary input. This framework enables users to perform\ndrag-based editing on pre-trained 3D Gaussian object models, producing modified\n2D images through multi-view consistent editing. Our contributions include the\nintroduction of a new task, the development of DragGaussian for interactive\npoint-based 3D editing, and comprehensive validation of its effectiveness\nthrough qualitative and quantitative experiments.",
      "doi": "arXiv:2405.05800v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "RISE-Editing: Rotation-invariant neural point fields with interactive segmentation for fine-grained and efficient editing.",
      "authors": [
        "Wang Y",
        "Wang J",
        "Wang C",
        "Qi Y"
      ],
      "abstract": "Neural Radiance Fields (NeRF) have shown great potential for synthesizing novel views. Currently, despite the existence of some initial controllable and editable NeRF methods, they remain limited in terms of efficient and fine-grained editing capabilities, hinders the creative editing abilities and potential applications for NeRF. In this paper, we present the rotation-invariant neural point fields with interactive segmentation for fine-grained and efficient editing. Editing the implicit field presents a significant challenge, as varying the orientation of the corresponding explicit scaffold-whether point, mesh, volume, or other representations-may lead to a notable decline in rendering quality. By leveraging the complementary strengths of implicit NeRF-based representations and explicit point-based representations, we introduce a novel rotation-invariant neural point field representation. This representation enables the learning of local contents using Cartesian coordinates, leading to significant improvements in scene rendering quality after fine-grained editing. To achieve this rotation-invariant representation, we carefully design a Rotation-Invariant Neural Inverse Distance Weighting Interpolation (RNIDWI) module to aggregate the neural points. To enable more efficient and flexible cross-scene compositing, we disentangle the traditional NeRF representation into two components: a scene-agnostic rendering module and the scene-specific neural point fields. Furthermore, we present a multi-view ensemble learning strategy to lift the 2D inconsistent zero-shot segmentation results to 3D neural points field in real-time without post retraining. With simple click-based prompts on 2D images, user can efficiently segment the 3D neural point field and manipulate the corresponding neural points, enabling fine-grained editing of the implicit fields. Extensive experimental results demonstrate that our method offers enhanced editing capabilities and simplified editing process for users, delivers photorealistic rendering quality for novel views, and surpasses related methods in terms of the space-time efficiency and the types of editing functions they can achieve. The code is available at https://github.com/yuzewang1998/RISE-Editing.",
      "doi": "https://doi.org/10.1016/j.neunet.2025.107304",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "VQ-NeRF: Neural Reflectance Decomposition and Editing With Vector Quantization.",
      "authors": [
        "Zhong H",
        "Zhang J",
        "Liao J"
      ],
      "abstract": "We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy to predict the number of materials in a scene, which reduces redundancy in the material segmentation process. To improve usability, we also develop an interactive interface to further assist material editing. We evaluate our model on both computer-generated and real-world scenes, demonstrating its superior performance. To the best of our knowledge, our model is the first to enable discrete material editing in 3D scenes.",
      "doi": "https://doi.org/10.1109/tvcg.2023.3330518",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Exploration and Improvement of Nerf-based 3D Scene Editing Techniques",
      "authors": [
        "Shun Fang",
        "Ming Cui",
        "Xing Feng",
        "Yanan Zhang"
      ],
      "abstract": "NeRF's high-quality scene synthesis capability was quickly accepted by\nscholars in the years after it was proposed, and significant progress has been\nmade in 3D scene representation and synthesis. However, the high computational\ncost limits intuitive and efficient editing of scenes, making NeRF's\ndevelopment in the scene editing field facing many challenges. This paper\nreviews the preliminary explorations of scholars on NeRF in the scene or object\nediting field in recent years, mainly changing the shape and texture of scenes\nor objects in new synthesized scenes; through the combination of residual\nmodels such as GaN and Transformer with NeRF, the generalization ability of\nNeRF scene editing has been further expanded, including realizing real-time new\nperspective editing feedback, multimodal editing of text synthesized 3D scenes,\n4D synthesis performance, and in-depth exploration in light and shadow editing,\ninitially achieving optimization of indirect touch editing and detail\nrepresentation in complex scenes. Currently, most NeRF editing methods focus on\nthe touch points and materials of indirect points, but when dealing with more\ncomplex or larger 3D scenes, it is difficult to balance accuracy, breadth,\nefficiency, and quality. Overcoming these challenges may become the direction\nof future NeRF 3D scene editing technology.",
      "doi": "arXiv:2401.12456v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "GenN2N: Generative NeRF2NeRF Translation",
      "authors": [
        "Xiangyue Liu",
        "Han Xue",
        "Kunming Luo",
        "Ping Tan",
        "Li Yi"
      ],
      "abstract": "We present GenN2N, a unified NeRF-to-NeRF translation framework for various\nNeRF translation tasks such as text-driven NeRF editing, colorization,\nsuper-resolution, inpainting, etc. Unlike previous methods designed for\nindividual translation tasks with task-specific schemes, GenN2N achieves all\nthese NeRF editing tasks by employing a plug-and-play image-to-image translator\nto perform editing in the 2D domain and lifting 2D edits into the 3D NeRF\nspace. Since the 3D consistency of 2D edits may not be assured, we propose to\nmodel the distribution of the underlying 3D edits through a generative model\nthat can cover all possible edited NeRFs. To model the distribution of 3D\nedited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes\nimages while decoding NeRFs. The latent space is trained to align with a\nGaussian distribution and the NeRFs are supervised through an adversarial loss\non its renderings. To ensure the latent code does not depend on 2D viewpoints\nbut truly reflects the 3D edits, we also regularize the latent code through a\ncontrastive learning scheme. Extensive experiments on various editing tasks\nshow GenN2N, as a universal framework, performs as well or better than\ntask-specific specialists while possessing flexible generative power. More\nresults on our project page: https://xiangyueliu.github.io/GenN2N/",
      "doi": "arXiv:2404.02788v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "ED-NeRF: Efficient Text-Guided Editing of 3D Scene with Latent Space\n  NeRF",
      "authors": [
        "Jangho Park",
        "Gihyun Kwon",
        "Jong Chul Ye"
      ],
      "abstract": "Recently, there has been a significant advancement in text-to-image diffusion\nmodels, leading to groundbreaking performance in 2D image generation. These\nadvancements have been extended to 3D models, enabling the generation of novel\n3D objects from textual descriptions. This has evolved into NeRF editing\nmethods, which allow the manipulation of existing 3D objects through textual\nconditioning. However, existing NeRF editing techniques have faced limitations\nin their performance due to slow training speeds and the use of loss functions\nthat do not adequately consider editing. To address this, here we present a\nnovel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding\nreal-world scenes into the latent space of the latent diffusion model (LDM)\nthrough a unique refinement layer. This approach enables us to obtain a NeRF\nbackbone that is not only faster but also more amenable to editing compared to\ntraditional image space NeRF editing. Furthermore, we propose an improved loss\nfunction tailored for editing by migrating the delta denoising score (DDS)\ndistillation loss, originally used in 2D image editing to the three-dimensional\ndomain. This novel loss function surpasses the well-known score distillation\nsampling (SDS) loss in terms of suitability for editing purposes. Our\nexperimental results demonstrate that ED-NeRF achieves faster editing speed\nwhile producing improved output quality compared to state-of-the-art 3D editing\nmodels.",
      "doi": "arXiv:2310.02712v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "PaletteNeRF: Palette-based Color Editing for NeRFs",
      "authors": [
        "Qiling Wu",
        "Jianchao Tan",
        "Kun Xu"
      ],
      "abstract": "Neural Radiance Field (NeRF) is a powerful tool to faithfully generate novel\nviews for scenes with only sparse captured images. Despite its strong\ncapability for representing 3D scenes and their appearance, its editing ability\nis very limited. In this paper, we propose a simple but effective extension of\nvanilla NeRF, named PaletteNeRF, to enable efficient color editing on\nNeRF-represented scenes. Motivated by recent palette-based image decomposition\nworks, we approximate each pixel color as a sum of palette colors modulated by\nadditive weights. Instead of predicting pixel colors as in vanilla NeRFs, our\nmethod predicts additive weights. The underlying NeRF backbone could also be\nreplaced with more recent NeRF models such as KiloNeRF to achieve real-time\nediting. Experimental results demonstrate that our method achieves efficient,\nview-consistent, and artifact-free color editing on a wide range of\nNeRF-represented scenes.",
      "doi": "arXiv:2212.12871v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "LC-NeRF: Local Controllable Face Generation in Neural Radiance Field.",
      "authors": [
        "Zhou WY",
        "Yuan L",
        "Chen SY",
        "Gao L",
        "Hu SM"
      ],
      "abstract": "3D face generation has achieved high visual quality and 3D consistency thanks to the development of neural radiance fields (NeRF). However, these methods model the whole face as a neural radiance field, which limits the controllability of the local regions. In other words, previous methods struggle to independently control local regions, such as the mouth, nose, and hair. To improve local controllability in NeRF-based face generation, we propose LC-NeRF, which is composed of a Local Region Generators Module (LRGM) and a Spatial-Aware Fusion Module (SAFM), allowing for geometry and texture control of local facial regions. The LRGM models different facial regions as independent neural radiance fields and the SAFM is responsible for merging multiple independent neural radiance fields into a complete representation. Finally, LC-NeRF enables the modification of the latent code associated with each individual generator, thereby allowing precise control over the corresponding local region. Qualitative and quantitative evaluations show that our method provides better local controllability than state-of-the-art 3D-aware face generation methods. A perception study reveals that our method outperforms existing state-of-the-art methods in terms of image quality, face consistency, and editing effects. Furthermore, our method exhibits favorable performance in downstream tasks, including real image editing and text-driven facial image editing.",
      "doi": "https://doi.org/10.1109/tvcg.2023.3293653",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion\n  Model",
      "authors": [
        "Lu Yu",
        "Wei Xiang",
        "Kang Han"
      ],
      "abstract": "Recent research has demonstrated that the combination of pretrained diffusion\nmodels with neural radiance fields (NeRFs) has emerged as a promising approach\nfor text-to-3D generation. Simply coupling NeRF with diffusion models will\nresult in cross-view inconsistency and degradation of stylized view syntheses.\nTo address this challenge, we propose the Edit-DiffNeRF framework, which is\ncomposed of a frozen diffusion model, a proposed delta module to edit the\nlatent semantic space of the diffusion model, and a NeRF. Instead of training\nthe entire diffusion for each scene, our method focuses on editing the latent\nsemantic space in frozen pretrained diffusion models by the delta module. This\nfundamental change to the standard diffusion framework enables us to make\nfine-grained modifications to the rendered views and effectively consolidate\nthese instructions in a 3D scene via NeRF training. As a result, we are able to\nproduce an edited 3D scene that faithfully aligns to input text instructions.\nFurthermore, to ensure semantic consistency across different viewpoints, we\npropose a novel multi-view semantic consistency loss that extracts a latent\nsemantic embedding from the input view as a prior, and aim to reconstruct it in\ndifferent views. Our proposed method has been shown to effectively edit\nreal-world 3D scenes, resulting in 25% improvement in the alignment of the\nperformed 3D edits with text instructions compared to prior work.",
      "doi": "arXiv:2306.09551v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and\n  Relighting with Diffusion Models",
      "authors": [
        "Hao Zhang",
        "Yanbo Xu",
        "Tianyuan Dai",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "abstract": "The ability to create high-quality 3D faces from a single image has become\nincreasingly important with wide applications in video conferencing, AR/VR, and\nadvanced video editing in movie industries. In this paper, we propose Face\nDiffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality\nFace NeRFs from single images, complete with semantic editing and relighting\ncapabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly\ntrained 2D latent-diffusion model, allowing users to manipulate and construct\nFace NeRFs in zero-shot learning without the need for explicit 3D data. With\ncarefully designed illumination and identity preserving loss, as well as\nmulti-modal pre-training, FaceDNeRF offers users unparalleled control over the\nediting process enabling them to create and edit face NeRFs using just\nsingle-view images, text prompts, and explicit target lighting. The advanced\nfeatures of FaceDNeRF have been designed to produce more impressive results\nthan existing 2D editing approaches that rely on 2D segmentation maps for\neditable attributes. Experiments show that our FaceDNeRF achieves exceptionally\nrealistic results and unprecedented flexibility in editing compared with\nstate-of-the-art 3D face reconstruction and editing methods. Our code will be\navailable at https://github.com/BillyXYB/FaceDNeRF.",
      "doi": "arXiv:2306.00783v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "INFAMOUS-NeRF: ImproviNg FAce MOdeling Using Semantically-Aligned\n  Hypernetworks with Neural Radiance Fields",
      "authors": [
        "Andrew Hou",
        "Feng Liu",
        "Zhiyuan Ren",
        "Michel Sarkis",
        "Ning Bi",
        "Yiying Tong",
        "Xiaoming Liu"
      ],
      "abstract": "We propose INFAMOUS-NeRF, an implicit morphable face model that introduces\nhypernetworks to NeRF to improve the representation power in the presence of\nmany training subjects. At the same time, INFAMOUS-NeRF resolves the classic\nhypernetwork tradeoff of representation power and editability by learning\nsemantically-aligned latent spaces despite the subject-specific models, all\nwithout requiring a large pretrained model. INFAMOUS-NeRF further introduces a\nnovel constraint to improve NeRF rendering along the face boundary. Our\nconstraint can leverage photometric surface rendering and multi-view\nsupervision to guide surface color prediction and improve rendering near the\nsurface. Finally, we introduce a novel, loss-guided adaptive sampling method\nfor more effective NeRF training by reducing the sampling redundancy. We show\nquantitatively and qualitatively that our method achieves higher representation\npower than prior face modeling methods in both controlled and in-the-wild\nsettings. Code and models will be released upon publication.",
      "doi": "arXiv:2312.16197v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "BluNF: Blueprint Neural Field",
      "authors": [
        "Robin Courant",
        "Xi Wang",
        "Marc Christie",
        "Vicky Kalogeiton"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) have revolutionized scene novel view\nsynthesis, offering visually realistic, precise, and robust implicit\nreconstructions. While recent approaches enable NeRF editing, such as object\nremoval, 3D shape modification, or material property manipulation, the manual\nannotation prior to such edits makes the process tedious. Additionally,\ntraditional 2D interaction tools lack an accurate sense of 3D space, preventing\nprecise manipulation and editing of scenes. In this paper, we introduce a novel\napproach, called Blueprint Neural Field (BluNF), to address these editing\nissues. BluNF provides a robust and user-friendly 2D blueprint, enabling\nintuitive scene editing. By leveraging implicit neural representation, BluNF\nconstructs a blueprint of a scene using prior semantic and depth information.\nThe generated blueprint allows effortless editing and manipulation of NeRF\nrepresentations. We demonstrate BluNF's editability through an intuitive\nclick-and-change mechanism, enabling 3D manipulations, such as masking,\nappearance modification, and object removal. Our approach significantly\ncontributes to visual content creation, paving the way for further research in\nthis area.",
      "doi": "arXiv:2309.03933v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models",
      "authors": [
        "Xingchen Zhou",
        "Ying He",
        "F. Richard Yu",
        "Jianqiang Li",
        "You Li"
      ],
      "abstract": "<jats:p>The emergence of Neural Radiance Fields (NeRF) has promoted the development of synthesized high-fidelity views of the intricate real world. However, it is still a very demanding task to repaint the content in NeRF. In this paper, we propose a novel framework that can take RGB images as input and alter the 3D content in neural scenes. Our work leverages existing diffusion models to guide changes in the designated 3D content. Specifically, we semantically select the target object and a pre-trained diffusion model will guide the NeRF model to generate new 3D objects, which can improve the editability, diversity, and application range of NeRF. Experiment results show that our algorithm is effective for editing 3D objects in NeRF under different text prompts, including editing appearance, shape, and more. We validate our method on both real-world datasets and synthetic-world datasets for these editing tasks. Please visit https://repaintnerf.github.io for a better view of our results.</jats:p>",
      "doi": "https://doi.org/10.24963/ijcai.2023/201",
      "year": 2023,
      "source": "Crossref"
    },
    {
      "title": "Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent",
      "authors": [
        "Jianmeng Liu",
        "Yuyao Zhang",
        "Zeyuan Meng",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "abstract": "This paper explores promptable NeRF generation (e.g., text prompt or single\nimage prompt) for direct conditioning and fast generation of NeRF parameters\nfor the underlying 3D scenes, thus undoing complex intermediate steps while\nproviding full 3D generation with conditional control. Unlike previous\ndiffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,\nPrompt2NeRF-PIL is capable of generating a variety of 3D objects with a single\nforward pass, leveraging a pre-trained implicit latent space of NeRF\nparameters. Furthermore, in zero-shot tasks, our experiments demonstrate that\nthe NeRFs produced by our method serve as semantically informative\ninitializations, significantly accelerating the inference process of existing\nprompt-to-NeRF methods. Specifically, we will show that our approach speeds up\nthe text-to-NeRF model DreamFusion and the 3D reconstruction speed of the\nimage-to-NeRF method Zero-1-to-3 by 3 to 5 times.",
      "doi": "arXiv:2312.02568v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors",
      "authors": [
        "Dogyoon Lee",
        "Minhyeok Lee",
        "Chajin Shin",
        "Sangyoun Lee"
      ],
      "abstract": "Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D)\nreconstruction quality via the novel view synthesis from multi-view images and\npaired calibrated camera parameters. However, previous NeRF-based systems have\nbeen demonstrated under strictly controlled settings, with little attention\npaid to less ideal scenarios, including with the presence of noise such as\nexposure, illumination changes, and blur. In particular, though blur frequently\noccurs in real situations, NeRF that can handle blurred images has received\nlittle attention. The few studies that have investigated NeRF for blurred\nimages have not considered geometric and appearance consistency in 3D space,\nwhich is one of the most important factors in 3D reconstruction. This leads to\ninconsistency and the degradation of the perceptual quality of the constructed\nscene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for\nblurred images, which is constrained with two physical priors. These priors are\nderived from the actual blurring process during image acquisition by the\ncamera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency\nutilizing the physical priors and adaptive weight proposal to refine the color\ncomposition error in consideration of the relationship between depth and blur.\nWe present extensive experimental results for synthetic and real scenes with\ntwo types of blur: camera motion blur and defocus blur. The results demonstrate\nthat DP-NeRF successfully improves the perceptual quality of the constructed\nNeRF ensuring 3D geometric and appearance consistency. We further demonstrate\nthe effectiveness of our model with comprehensive ablation analysis.",
      "doi": "arXiv:2211.12046v4",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures",
      "authors": [
        "Gal Metzer",
        "Elad Richardson",
        "Or Patashnik",
        "Raja Giryes",
        "Daniel Cohen-Or"
      ],
      "abstract": "Text-guided image generation has progressed rapidly in recent years,\ninspiring major breakthroughs in text-guided shape generation. Recently, it has\nbeen shown that using score distillation, one can successfully text-guide a\nNeRF model to generate a 3D object. We adapt the score distillation to the\npublicly available, and computationally efficient, Latent Diffusion Models,\nwhich apply the entire diffusion process in a compact latent space of a\npretrained autoencoder. As NeRFs operate in image space, a naive solution for\nguiding them with latent score distillation would require encoding to the\nlatent space at each guidance step. Instead, we propose to bring the NeRF to\nthe latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we\nshow that while Text-to-3D models can generate impressive results, they are\ninherently unconstrained and may lack the ability to guide or enforce a\nspecific 3D structure. To assist and direct the 3D generation, we propose to\nguide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines\nthe coarse structure of the desired object. Then, we present means to integrate\nsuch a constraint directly into a Latent-NeRF. This unique combination of text\nand shape guidance allows for increased control over the generation process. We\nalso show that latent score distillation can be successfully applied directly\non 3D meshes. This allows for generating high-quality textures on a given\ngeometry. Our experiments validate the power of our different forms of guidance\nand the efficiency of using latent rendering. Implementation is available at\nhttps://github.com/eladrich/latent-nerf",
      "doi": "arXiv:2211.07600v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh\n  Rasterization",
      "authors": [
        "Jianwei Feng",
        "Prateek Singhal"
      ],
      "abstract": "Style transfer for human face has been widely researched in recent years.\nMajority of the existing approaches work in 2D image domain and have 3D\ninconsistency issue when applied on different viewpoints of the same face. In\nthis paper, we tackle the problem of 3D face style transfer which aims at\ngenerating stylized novel views of a 3D human face with multi-view consistency.\nWe propose to use a neural radiance field (NeRF) to represent 3D human face and\ncombine it with 2D style transfer to stylize the 3D face. We find that directly\ntraining a NeRF on stylized images from 2D style transfer brings in 3D\ninconsistency issue and causes blurriness. On the other hand, training a NeRF\njointly with 2D style transfer objectives shows poor convergence due to the\nidentity and head pose gap between style image and content image. It also poses\nchallenge in training time and memory due to the need of volume rendering for\nfull image to apply style transfer loss functions. We therefore propose a\nhybrid framework of NeRF and mesh rasterization to combine the benefits of high\nfidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our\nframework consists of three stages: 1. Training a NeRF model on input face\nimages to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF\nmodel and optimizing it with style transfer objectives via differentiable\nrasterization; 3. Training a new color network in NeRF conditioned on a style\nembedding to enable arbitrary style transfer to the 3D face. Experiment results\nshow that our approach generates high quality face style transfer with great 3D\nconsistency, while also enabling a flexible style control.",
      "doi": "arXiv:2311.13168v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "LAENeRF: Local Appearance Editing for Neural Radiance Fields",
      "authors": [
        "Lukas Radl",
        "Michael Steiner",
        "Andreas Kurz",
        "Markus Steinberger"
      ],
      "abstract": "Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest\ntowards editable implicit 3D representations has surged over the last years.\nHowever, editing implicit or hybrid representations as used for NeRFs is\ndifficult due to the entanglement of appearance and geometry encoded in the\nmodel parameters. Despite these challenges, recent research has shown first\npromising steps towards photorealistic and non-photorealistic appearance edits.\nThe main open issues of related work include limited interactivity, a lack of\nsupport for local edits and large memory requirements, rendering them less\nuseful in practice. We address these limitations with LAENeRF, a unified\nframework for photorealistic and non-photorealistic appearance editing of\nNeRFs. To tackle local editing, we leverage a voxel grid as starting point for\nregion selection. We learn a mapping from expected ray terminations to final\noutput color, which can optionally be supervised by a style loss, resulting in\na framework which can perform photorealistic and non-photorealistic appearance\nediting of selected regions. Relying on a single point per ray for our mapping,\nwe limit memory requirements and enable fast optimization. To guarantee\ninteractivity, we compose the output color using a set of learned, modifiable\nbase colors, composed with additive layer mixing. Compared to concurrent work,\nLAENeRF enables recoloring and stylization while keeping processing time low.\nFurthermore, we demonstrate that our approach surpasses baseline methods both\nquantitatively and qualitatively.",
      "doi": "arXiv:2312.09913v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Point Resampling and Ray Transformation Aid to Editable NeRF Models",
      "authors": [
        "Zhenyang Li",
        "Zilong Chen",
        "Feifan Qu",
        "Mingqing Wang",
        "Yizhou Zhao",
        "Kai Zhang",
        "Yifan Peng"
      ],
      "abstract": "In NeRF-aided editing tasks, object movement presents difficulties in\nsupervision generation due to the introduction of variability in object\npositions. Moreover, the removal operations of certain scene objects often lead\nto empty regions, presenting challenges for NeRF models in inpainting them\neffectively. We propose an implicit ray transformation strategy, allowing for\ndirect manipulation of the 3D object's pose by operating on the neural-point in\nNeRF rays. To address the challenge of inpainting potential empty regions, we\npresent a plug-and-play inpainting module, dubbed differentiable neural-point\nresampling (DNR), which interpolates those regions in 3D space at the original\nray locations within the implicit space, thereby facilitating object removal &\nscene inpainting tasks. Importantly, employing DNR effectively narrows the gap\nbetween ground truth and predicted implicit features, potentially increasing\nthe mutual information (MI) of the features across rays. Then, we leverage DNR\nand ray transformation to construct a point-based editable NeRF pipeline\nPR^2T-NeRF. Results primarily evaluated on 3D object removal & inpainting tasks\nindicate that our pipeline achieves state-of-the-art performance. In addition,\nour pipeline supports high-quality rendering visualization for diverse editing\noperations without necessitating extra supervision.",
      "doi": "arXiv:2405.07306v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "LC-NeRF: Local Controllable Face Generation in Neural Randiance Field",
      "authors": [
        "Wenyang Zhou",
        "Lu Yuan",
        "Shuyu Chen",
        "Lin Gao",
        "Shimin Hu"
      ],
      "abstract": "3D face generation has achieved high visual quality and 3D consistency thanks\nto the development of neural radiance fields (NeRF). Recently, to generate and\nedit 3D faces with NeRF representation, some methods are proposed and achieve\ngood results in decoupling geometry and texture. The latent codes of these\ngenerative models affect the whole face, and hence modifications to these codes\ncause the entire face to change. However, users usually edit a local region\nwhen editing faces and do not want other regions to be affected. Since changes\nto the latent code affect global generation results, these methods do not allow\nfor fine-grained control of local facial regions. To improve local\ncontrollability in NeRF-based face editing, we propose LC-NeRF, which is\ncomposed of a Local Region Generators Module and a Spatial-Aware Fusion Module,\nallowing for local geometry and texture control of local facial regions.\nQualitative and quantitative evaluations show that our method provides better\nlocal editing than state-of-the-art face editing methods. Our method also\nperforms well in downstream tasks, such as text-driven facial image editing.",
      "doi": "arXiv:2302.09486v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories",
      "authors": [
        "Susung Hong",
        "Johanna Karras",
        "Ricardo Martin-Brualla",
        "Ira Kemelmacher-Shlizerman"
      ],
      "abstract": "Recent advancements in text-based diffusion models have accelerated progress\nin 3D reconstruction and text-based 3D editing. Although existing 3D editing\nmethods excel at modifying color, texture, and style, they struggle with\nextensive geometric or appearance changes, thus limiting their applications. To\nthis end, we propose Perturb-and-Revise, which makes possible a variety of NeRF\nediting. First, we perturb the NeRF parameters with random initializations to\ncreate a versatile initialization. The level of perturbation is determined\nautomatically through analysis of the local loss landscape. Then, we revise the\nedited NeRF via generative trajectories. Combined with the generative process,\nwe impose identity-preserving gradients to refine the edited NeRF. Extensive\nexperiments demonstrate that Perturb-and-Revise facilitates flexible,\neffective, and consistent editing of color, appearance, and geometry in 3D. For\n360{\\deg} results, please visit our project page:\nhttps://susunghong.github.io/Perturb-and-Revise.",
      "doi": "arXiv:2412.05279v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points.",
      "authors": [
        "Zheng C",
        "Lin W",
        "Xu F"
      ],
      "abstract": "Neural radiance fields (NeRF) achieve highly photo-realistic novel-view synthesis, but it's a challenging problem to edit the scenes modeled by NeRF-based methods, especially for dynamic scenes. We propose editable neural radiance fields that enable end-users to easily edit dynamic scenes and support topological changes. Input with an image sequence from a single camera, our network is trained automatically and models topologically varying dynamics using our picked-out surface key points. Then end-users can edit the scene by easily dragging the key points to desired new positions. To achieve this, we propose a scene analysis method to detect and initialize key points by considering the dynamics in the scene, and a weighted key points strategy to model topologically varying dynamics by joint key points and weights optimization. Our method supports intuitive multi-dimensional (up to 3D) editing and can generate novel scenes that are unseen in the input sequence. Experiments demonstrate that our method achieves high-quality editing on various dynamic scenes and outperforms the state-of-the-art.",
      "doi": "https://doi.org/10.1109/tpami.2024.3366148",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Correspondence Distillation from NeRF-based GAN",
      "authors": [
        "Yushi Lan",
        "Chen Change Loy",
        "Bo Dai"
      ],
      "abstract": "The neural radiance field (NeRF) has shown promising results in preserving\nthe fine details of objects and scenes. However, unlike mesh-based\nrepresentations, it remains an open problem to build dense correspondences\nacross different NeRFs of the same category, which is essential in many\ndownstream tasks. The main difficulties of this problem lie in the implicit\nnature of NeRF and the lack of ground-truth correspondence annotations. In this\npaper, we show it is possible to bypass these challenges by leveraging the rich\nsemantics and structural priors encapsulated in a pre-trained NeRF-based GAN.\nSpecifically, we exploit such priors from three aspects, namely 1) a dual\ndeformation field that takes latent codes as global structural indicators, 2) a\nlearning objective that regards generator features as geometric-aware local\ndescriptors, and 3) a source of infinite object-specific NeRF samples. Our\nexperiments demonstrate that such priors lead to 3D dense correspondence that\nis accurate, smooth, and robust. We also show that established dense\ncorrespondence across NeRFs can effectively enable many NeRF-based downstream\napplications such as texture transfer.",
      "doi": "arXiv:2212.09735v2",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated\n  Objects",
      "authors": [
        "Wenxiao Cai",
        "Xinyue Lei",
        "Xinyu He",
        "Junming Leo Chen",
        "Yangang Wang"
      ],
      "abstract": "We present Knowledge NeRF to synthesize novel views for dynamic scenes.\nReconstructing dynamic 3D scenes from few sparse views and rendering them from\narbitrary perspectives is a challenging problem with applications in various\ndomains. Previous dynamic NeRF methods learn the deformation of articulated\nobjects from monocular videos. However, qualities of their reconstructed scenes\nare limited. To clearly reconstruct dynamic scenes, we propose a new framework\nby considering two frames at a time.We pretrain a NeRF model for an articulated\nobject.When articulated objects moves, Knowledge NeRF learns to generate novel\nviews at the new state by incorporating past knowledge in the pretrained NeRF\nmodel with minimal observations in the present state. We propose a projection\nmodule to adapt NeRF for dynamic scenes, learning the correspondence between\npretrained knowledge base and current states. Experimental results demonstrate\nthe effectiveness of our method in reconstructing dynamic 3D scenes with 5\ninput images in one state. Knowledge NeRF is a new pipeline and promising\nsolution for novel view synthesis in dynamic articulated objects. The data and\nimplementation are publicly available at\nhttps://github.com/RussRobin/Knowledge_NeRF.",
      "doi": "arXiv:2404.00674v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Shielding the Unseen: Privacy Protection through Poisoning NeRF with\n  Spatial Deformation",
      "authors": [
        "Yihan Wu",
        "Brandon Y. Feng",
        "Heng Huang"
      ],
      "abstract": "In this paper, we introduce an innovative method of safeguarding user privacy\nagainst the generative capabilities of Neural Radiance Fields (NeRF) models.\nOur novel poisoning attack method induces changes to observed views that are\nimperceptible to the human eye, yet potent enough to disrupt NeRF's ability to\naccurately reconstruct a 3D scene. To achieve this, we devise a bi-level\noptimization algorithm incorporating a Projected Gradient Descent (PGD)-based\nspatial deformation. We extensively test our approach on two common NeRF\nbenchmark datasets consisting of 29 real-world scenes with high-quality images.\nOur results compellingly demonstrate that our privacy-preserving method\nsignificantly impairs NeRF's performance across these benchmark datasets.\nAdditionally, we show that our method is adaptable and versatile, functioning\nacross various perturbation strengths and NeRF architectures. This work offers\nvaluable insights into NeRF's vulnerabilities and emphasizes the need to\naccount for such potential privacy risks when developing robust 3D scene\nreconstruction algorithms. Our study contributes to the larger conversation\nsurrounding responsible AI and generative machine learning, aiming to protect\nuser privacy and respect creative ownership in the digital age.",
      "doi": "arXiv:2310.03125v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Nerfies: Deformable Neural Radiance Fields",
      "authors": [
        "Keunhong Park",
        "Utkarsh Sinha",
        "Jonathan T. Barron",
        "Sofien Bouaziz",
        "Dan B Goldman",
        "Steven M. Seitz",
        "Ricardo Martin-Brualla"
      ],
      "abstract": "We present the first method capable of photorealistically reconstructing\ndeformable scenes using photos/videos captured casually from mobile phones. Our\napproach augments neural radiance fields (NeRF) by optimizing an additional\ncontinuous volumetric deformation field that warps each observed point into a\ncanonical 5D NeRF. We observe that these NeRF-like deformation fields are prone\nto local minima, and propose a coarse-to-fine optimization method for\ncoordinate-based models that allows for more robust optimization. By adapting\nprinciples from geometry processing and physical simulation to NeRF-like\nmodels, we propose an elastic regularization of the deformation field that\nfurther improves robustness. We show that our method can turn casually captured\nselfie photos/videos into deformable NeRF models that allow for photorealistic\nrenderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We\nevaluate our method by collecting time-synchronized data using a rig with two\nmobile phones, yielding train/validation images of the same pose at different\nviewpoints. We show that our method faithfully reconstructs non-rigidly\ndeforming scenes and reproduces unseen views with high fidelity.",
      "doi": "arXiv:2011.12948v5",
      "year": 2020,
      "source": "arXiv"
    },
    {
      "title": "NeRF-Art: Text-Driven Neural Radiance Fields Stylization.",
      "authors": [
        "Wang C",
        "Jiang R",
        "Chai M",
        "He M",
        "Chen D",
        "Liao J"
      ],
      "abstract": "As a powerful representation of 3D scenes, the neural radiance field (NeRF) enables high-quality novel view synthesis from multi-view images. Stylizing NeRF, however, remains challenging, especially in simulating a text-guided style with both the appearance and the geometry altered simultaneously. In this paper, we present NeRF-Art, a text-guided NeRF stylization approach that manipulates the style of a pre-trained NeRF model with a simple text prompt. Unlike previous approaches that either lack sufficient geometry deformations and texture details or require meshes to guide the stylization, our method can shift a 3D scene to the target style characterized by desired geometry and appearance variations without any mesh guidance. This is achieved by introducing a novel global-local contrastive learning strategy, combined with the directional constraint to simultaneously control both the trajectory and the strength of the target style. Moreover, we adopt a weight regularization method to effectively suppress cloudy artifacts and geometry noises which arise easily when the density field is transformed during geometry stylization. Through extensive experiments on various styles, we demonstrate that our method is effective and robust regarding both single-view stylization quality and cross-view consistency.",
      "doi": "https://doi.org/10.1109/tvcg.2023.3283400",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "[Short sagittal osteotomy].",
      "authors": [
        "Paulus C",
        "Kater W"
      ],
      "abstract": "Bilateral sagittal split osteotomy has become the standard mandibular surgery for the treatment of dento-facial deformities. Even patients with less important deformities may undergo surgery. The morbidity must be as low as possible. We describe a technique with reduced split surfaces. The osseous section follows an oblique line since the thorn of Spix below and outside towards the supra-angular region. This section is completed by an osteotomy of the posterior border of the mandible. This split never reaches the inferior alveolar nerf tunnel. The protection of the alveolar nerve is increased what decreases considerably the risk of nervous complications of this intervention. The majority of the mandibular movements are possible by this technique with the exception of the important advancements and the increase of the height of the ramus.",
      "doi": "https://doi.org/10.1051/orthodfr/2015036",
      "year": 2015,
      "source": "PubMed"
    },
    {
      "title": "MPS-NeRF: Generalizable 3D Human Rendering From Multiview Images.",
      "authors": [
        "Gao X",
        "Yang J",
        "Kim J",
        "Peng S",
        "Liu Z",
        "Tong X"
      ],
      "abstract": "There has been rapid progress recently on 3D human rendering, including novel view synthesis and pose animation, based on the advances of neural radiance fields (NeRF). However, most existing methods focus on person-specific training and their training typically requires multi-view videos. This paper deals with a new challenging task - rendering novel views and novel poses for a person unseen in training, using only multiview still images as input without videos. For this task, we propose a simple yet surprisingly effective method to train a generalizable NeRF with multiview images as conditional input. The key ingredient is a dedicated representation combining a canonical NeRF and a volume deformation scheme. Using a canonical space enables our method to learn shared properties of human and easily generalize to different people. Volume deformation is used to connect the canonical space with input and target images and query image features for radiance and density prediction. We leverage the parametric 3D human model fitted on the input images to derive the deformation, which works quite well in practice when combined with our canonical NeRF. The experiments on both real and synthetic data with the novel view synthesis and pose animation tasks collectively demonstrate the efficacy of our method.",
      "doi": "https://doi.org/10.1109/tpami.2022.3205910",
      "year": 2022,
      "source": "PubMed"
    },
    {
      "title": "De‐NeRF: Ultra‐high‐definition NeRF with deformable net alignment",
      "authors": [
        "Jianing Hou",
        "Runjie Zhang",
        "Zhongqi Wu",
        "Weiliang Meng",
        "Xiaopeng Zhang",
        "Jianwei Guo"
      ],
      "abstract": "<jats:title>Abstract</jats:title><jats:p>Neural Radiance Field (NeRF) can render complex 3D scenes with viewpoint‐dependent effects. However, less work has been devoted to exploring its limitations in high‐resolution environments, especially when upscaled to ultra‐high resolution (e.g., 4k). Specifically, existing NeRF‐based methods face severe limitations in reconstructing high‐resolution real scenes, for example, a large number of parameters, misalignment of the input data, and over‐smoothing of details. In this paper, we present a novel and effective framework, called <jats:italic>De‐NeRF</jats:italic>, based on NeRF and deformable convolutional network, to achieve high‐fidelity view synthesis in ultra‐high resolution scenes: (1) marrying the deformable convolution unit which can solve the problem of misaligned input of the high‐resolution data. (2) Presenting a density sparse voxel‐based approach which can greatly reduce the training time while rendering results with higher accuracy. Compared to existing high‐resolution NeRF methods, our approach improves the rendering quality of high‐frequency details and achieves better visual effects in 4K high‐resolution scenes.</jats:p>",
      "doi": "https://doi.org/10.1002/cav.2240",
      "year": 2024,
      "source": "Crossref"
    },
    {
      "title": "Matching Query Image Against Selected NeRF Feature for Efficient and\n  Scalable Localization",
      "authors": [
        "Huaiji Zhou",
        "Bing Wang",
        "Changhao Chen"
      ],
      "abstract": "Neural implicit representations such as NeRF have revolutionized 3D scene\nrepresentation with photo-realistic quality. However, existing methods for\nvisual localization within NeRF representations suffer from inefficiency and\nscalability issues, particularly in large-scale environments. This work\nproposes MatLoc-NeRF, a novel matching-based localization framework using\nselected NeRF features. It addresses efficiency by employing a learnable\nfeature selection mechanism that identifies informative NeRF features for\nmatching with query images. This eliminates the need for all NeRF features or\nadditional descriptors, leading to faster and more accurate pose estimation. To\ntackle large-scale scenes, MatLoc-NeRF utilizes a pose-aware scene partitioning\nstrategy. It ensures that only the most relevant NeRF sub-block generates key\nfeatures for a specific pose. Additionally, scene segmentation and a place\npredictor provide fast coarse initial pose estimation. Evaluations on public\nlarge-scale datasets demonstrate that MatLoc-NeRF achieves superior efficiency\nand accuracy compared to existing NeRF-based localization methods.",
      "doi": "arXiv:2406.11766v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "LLaNA: Large Language and NeRF Assistant",
      "authors": [
        "Andrea Amaduzzi",
        "Pierluigi Zama Ramirez",
        "Giuseppe Lisanti",
        "Samuele Salti",
        "Luigi Di Stefano"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated an excellent\nunderstanding of images and 3D data. However, both modalities have shortcomings\nin holistically capturing the appearance and geometry of objects. Meanwhile,\nNeural Radiance Fields (NeRFs), which encode information within the weights of\na simple Multi-Layer Perceptron (MLP), have emerged as an increasingly\nwidespread modality that simultaneously encodes the geometry and photorealistic\nappearance of objects. This paper investigates the feasibility and\neffectiveness of ingesting NeRF into MLLM. We create LLaNA, the first\ngeneral-purpose NeRF-language assistant capable of performing new tasks such as\nNeRF captioning and Q\\&A. Notably, our method directly processes the weights of\nthe NeRF's MLP to extract information about the represented objects without the\nneed to render images or materialize 3D data structures. Moreover, we build a\ndataset of NeRFs with text annotations for various NeRF-language tasks with no\nhuman intervention. Based on this dataset, we develop a benchmark to evaluate\nthe NeRF understanding capability of our method. Results show that processing\nNeRF weights performs favourably against extracting 2D or 3D representations\nfrom NeRFs.",
      "doi": "arXiv:2406.11840v2",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "HyperFields: Towards Zero-Shot Generation of NeRFs from Text",
      "authors": [
        "Sudarshan Babu",
        "Richard Liu",
        "Avery Zhou",
        "Michael Maire",
        "Greg Shakhnarovich",
        "Rana Hanocka"
      ],
      "abstract": "We introduce HyperFields, a method for generating text-conditioned Neural\nRadiance Fields (NeRFs) with a single forward pass and (optionally) some\nfine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns\na smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF\ndistillation training, which distills scenes encoded in individual NeRFs into\none dynamic hypernetwork. These techniques enable a single network to fit over\na hundred unique scenes. We further demonstrate that HyperFields learns a more\ngeneral map between text and NeRFs, and consequently is capable of predicting\nnovel in-distribution and out-of-distribution scenes -- either zero-shot or\nwith a few finetuning steps. Finetuning HyperFields benefits from accelerated\nconvergence thanks to the learned general map, and is capable of synthesizing\nnovel scenes 5 to 10 times faster than existing neural optimization-based\nmethods. Our ablation experiments show that both the dynamic architecture and\nNeRF distillation are critical to the expressivity of HyperFields.",
      "doi": "arXiv:2310.17075v3",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Fast High Dynamic Range Radiance Fields for Dynamic Scenes",
      "authors": [
        "Guanjun Wu",
        "Taoran Yi",
        "Jiemin Fang",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "abstract": "Neural Radiances Fields (NeRF) and their extensions have shown great success\nin representing 3D scenes and synthesizing novel-view images. However, most\nNeRF methods take in low-dynamic-range (LDR) images, which may lose details,\nespecially with nonuniform illumination. Some previous NeRF methods attempt to\nintroduce high-dynamic-range (HDR) techniques but mainly target static scenes.\nTo extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF\nframework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images\ncaptured with various exposures. A learnable exposure mapping function is\nconstructed to obtain adaptive exposure values for each image. Based on the\nmonotonically increasing prior, a camera response function is designed for\nstable learning. With the proposed model, high-quality novel-view images at any\ntime point can be rendered with any desired exposure. We further construct a\ndataset containing multiple dynamic scenes captured with diverse exposures for\nevaluation. All the datasets and code are available at\n\\url{https://guanjunwu.github.io/HDR-HexPlane/}.",
      "doi": "arXiv:2401.06052v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Dynamic Appearance Particle Neural Radiance Field",
      "authors": [
        "Ancheng Lin",
        "Yusheng Xiang",
        "Jun Li",
        "Mukesh Prasad"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) have shown great potential in modeling 3D\nscenes. Dynamic NeRFs extend this model by capturing time-varying elements,\ntypically using deformation fields. The existing dynamic NeRFs employ a similar\nEulerian representation for both light radiance and deformation fields. This\nleads to a close coupling of appearance and motion and lacks a physical\ninterpretation. In this work, we propose Dynamic Appearance Particle Neural\nRadiance Field (DAP-NeRF), which introduces particle-based representation to\nmodel the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists\nof the superposition of a static field and a dynamic field. The dynamic field\nis quantized as a collection of appearance particles, which carries the visual\ninformation of a small dynamic element in the scene and is equipped with a\nmotion model. All components, including the static field, the visual features\nand the motion models of particles, are learned from monocular videos without\nany prior geometric knowledge of the scene. We develop an efficient\ncomputational framework for the particle-based model. We also construct a new\ndataset to evaluate motion modeling. Experimental results show that DAP-NeRF is\nan effective technique to capture not only the appearance but also the\nphysically meaningful motions in a 3D dynamic scene. Code is available at:\nhttps://github.com/Cenbylin/DAP-NeRF.",
      "doi": "https://doi.org/10.1109/TCSVT.2025.3540792",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "NeRF-OR: neural radiance fields for operating room scene reconstruction from sparse-view RGB-D videos.",
      "authors": [
        "Gerats BGA",
        "Wolterink JM",
        "Broeders IAMJ"
      ],
      "abstract": "RGB-D cameras in the operating room (OR) provide synchronized views of complex surgical scenes. Assimilation of this multi-view data into a unified representation allows for downstream tasks such as object detection and tracking, pose estimation, and action recognition. Neural radiance fields (NeRFs) can provide continuous representations of complex scenes with limited memory footprint. However, existing NeRF methods perform poorly in real-world OR settings, where a small set of cameras capture the room from entirely different vantage points. In this work, we propose NeRF-OR, a method for 3D reconstruction of dynamic surgical scenes in the OR. Where other methods for sparse-view datasets use either time-of-flight sensor depth or dense depth estimated from color images, NeRF-OR uses a combination of both. The depth estimations mitigate the missing values that occur in sensor depth images due to reflective materials and object boundaries. We propose to supervise with surface normals calculated from the estimated depths, because these are largely scale invariant. We fit NeRF-OR to static surgical scenes in the 4D-OR dataset and show that its representations are geometrically accurate, where state of the art collapses to sub-optimal solutions. Compared to earlier work, NeRF-OR grasps fine scene details while training 30 Our results show that NeRF-OR allows for novel view synthesis with videos captured by a small number of cameras with entirely different vantage points, which is the typical camera setting in the OR. Code is available via: github.com/Beerend/NeRF-OR .",
      "doi": "https://doi.org/10.1007/s11548-024-03261-5",
      "year": 2025,
      "source": "PubMed"
    },
    {
      "title": "LTM-NeRF: Embedding 3D Local Tone Mapping in HDR Neural Radiance Field.",
      "authors": [
        "Huang X",
        "Zhang Q",
        "Feng Y",
        "Li H",
        "Wang Q"
      ],
      "abstract": "Recent advances in Neural Radiance Fields (NeRF) have provided a new geometric primitive for novel view synthesis. High Dynamic Range NeRF (HDR NeRF) can render novel views with a higher dynamic range. However, effectively displaying the scene contents of HDR NeRF on diverse devices with limited dynamic range poses a significant challenge. To address this, we present LTM-NeRF, a method designed to recover HDR NeRF and support 3D local tone mapping. LTM-NeRF allows for the synthesis of HDR views, tone-mapped views, and LDR views under different exposure settings, using only the multi-view multi-exposure LDR inputs for supervision. Specifically, we propose a differentiable Camera Response Function (CRF) module for HDR NeRF reconstruction, globally mapping the scene's HDR radiance to LDR pixels. Moreover, we introduce a Neural Exposure Field (NeEF) to represent the spatially varying exposure time of an HDR NeRF to achieve 3D local tone mapping, for compatibility with various displays. Comprehensive experiments demonstrate that our method can not only synthesize HDR views and exposure-varying LDR views accurately but also render locally tone-mapped views naturally.",
      "doi": "https://doi.org/10.1109/tpami.2024.3448620",
      "year": 2024,
      "source": "PubMed"
    },
    {
      "title": "Recursive-NeRF: An Efficient and Dynamically Growing NeRF.",
      "authors": [
        "Yang GW",
        "Zhou WY",
        "Peng HY",
        "Liang D",
        "Mu TJ",
        "Hu SM"
      ],
      "abstract": "View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on public datasets and a large-scale scene dataset we collected shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.",
      "doi": "https://doi.org/10.1109/tvcg.2022.3204608",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model\n  Retrieval",
      "authors": [
        "Xin Wen",
        "Xuening Zhu",
        "Renjiao Yi",
        "Zhifeng Wang",
        "Chenyang Zhu",
        "Kai Xu"
      ],
      "abstract": "Reconstructing from multi-view images is a longstanding problem in 3D vision,\nwhere neural radiance fields (NeRFs) have shown great potential and get\nrealistic rendered images of novel views. Currently, most NeRF methods either\nrequire accurate camera poses or a large number of input images, or even both.\nReconstructing NeRF from few-view images without poses is challenging and\nhighly ill-posed. To address this problem, we propose CAD-NeRF, a method\nreconstructed from less than 10 images without any known poses. Specifically,\nwe build a mini library of several CAD models from ShapeNet and render them\nfrom many random views. Given sparse-view input images, we run a model and pose\nretrieval from the library, to get a model with similar shapes, serving as the\ndensity supervision and pose initializations. Here we propose a multi-view pose\nretrieval method to avoid pose conflicts among views, which is a new and unseen\nproblem in uncalibrated NeRF methods. Then, the geometry of the object is\ntrained by the CAD guidance. The deformation of the density field and camera\nposes are optimized jointly. Then texture and density are trained and\nfine-tuned as well. All training phases are in self-supervised manners.\nComprehensive evaluations of synthetic and real images show that CAD-NeRF\nsuccessfully learns accurate densities with a large deformation from retrieved\nCAD models, showing the generalization abilities.",
      "doi": "https://doi.org/10.1007/s11704-024-40417-7",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Flow supervision for Deformable NeRF",
      "authors": [
        "Chaoyang Wang",
        "Lachlan Ewen MacDonald",
        "Laszlo A. Jeni",
        "Simon Lucey"
      ],
      "abstract": "In this paper we present a new method for deformable NeRF that can directly\nuse optical flow as supervision. We overcome the major challenge with respect\nto the computationally inefficiency of enforcing the flow constraints to the\nbackward deformation field, used by deformable NeRFs. Specifically, we show\nthat inverting the backward deformation function is actually not needed for\ncomputing scene flows between frames. This insight dramatically simplifies the\nproblem, as one is no longer constrained to deformation functions that can be\nanalytically inverted. Instead, thanks to the weak assumptions required by our\nderivation based on the inverse function theorem, our approach can be extended\nto a broad class of commonly used backward deformation field. We present\nresults on monocular novel view synthesis with rapid object motion, and\ndemonstrate significant improvements over baselines without flow supervision.",
      "doi": "arXiv:2303.16333v1",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation",
      "authors": [
        "Xian Liu",
        "Yinghao Xu",
        "Qianyi Wu",
        "Hang Zhou",
        "Wayne Wu",
        "Bolei Zhou"
      ],
      "abstract": "Animating high-fidelity video portrait with speech audio is crucial for\nvirtual reality and digital entertainment. While most previous studies rely on\naccurate explicit structural information, recent works explore the implicit\nscene representation of Neural Radiance Fields (NeRF) for realistic generation.\nIn order to capture the inconsistent motions as well as the semantic difference\nbetween human head and torso, some work models them via two individual sets of\nNeRF, leading to unnatural results. In this work, we propose Semantic-aware\nSpeaking Portrait NeRF (SSP-NeRF), which creates delicate audio-driven\nportraits using one unified set of NeRF. The proposed model can handle the\ndetailed local facial semantics and the global head-torso relationship through\ntwo semantic-aware modules. Specifically, we first propose a Semantic-Aware\nDynamic Ray Sampling module with an additional parsing branch that facilitates\naudio-driven volume rendering. Moreover, to enable portrait rendering in one\nunified neural radiance field, a Torso Deformation module is designed to\nstabilize the large-scale non-rigid torso motions. Extensive evaluations\ndemonstrate that our proposed approach renders more realistic video portraits\ncompared to previous methods. Project page:\nhttps://alvinliu0.github.io/projects/SSP-NeRF",
      "doi": "arXiv:2201.07786v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "[Congenital abnormalities of the optic disc].",
      "authors": [
        "Denis D",
        "Hugo J",
        "Beylerian M",
        "Ramtohul P",
        "Aziz A",
        "Matonti F",
        "Lebranchu P"
      ],
      "abstract": "Congenital abnormalities of the optic disc are not uncommon in clinical practice and should be recognized. Size abnormalities of the optic disc include optic disc aplasia, hypoplasia, megalopapilla, and optic disc cupping in prematurity. Among congenital excavations of the optic disc head, morning glory disc anomaly and optic disc pit can be complicated by serous retinal detachment; the papillorenal disc is an association of bilateral optic disc cupping and renal hypoplasia which should be ruled out; optic disc coloboma is caused by an abnormal closure of the embryonic fissure and can be complicated by choroidal neovascularization and retinal detachment. Other abnormalities that will be discussed are congenital tilted disc syndrome, duplicity of the optic disc head, congenital pigmentation of the optic disc head and myelinated retinal nerve fibers. All of these abnormalities can be associated with syndromes and neurological diseases, as well as other potentially blinding ophthalmological defects which can be secondarily complicated by amblyopia, strabismus and nystagmus. Thus, they should be recognized in order to plan for appropriate follow-up.",
      "doi": "https://doi.org/10.1016/j.jfo.2018.09.011",
      "year": 2019,
      "source": "PubMed"
    },
    {
      "title": "Accessory subscapularis muscle - A forgotten variation?",
      "authors": [
        "Pires LAS",
        "Souza CFC",
        "Teixeira AR",
        "Leite TFO",
        "Babinski MA",
        "Chagas CAA"
      ],
      "abstract": "The quadrangular space is a space in the axilla bounded by the inferior margin of the teres minor muscle, the superior margin of the teres major muscle, the lateral margin of the long head of the triceps brachii muscle and the surgical neck of the humerus, medially. The axillary nerve (C5-C6) and the posterior circumflex humeral artery and veins pass through this space in order to supply their territories. The subscapularis muscle is situated into the scapular fossa and inserts itself into the lesser tubercle of the humerus, thus helping stabilize the shoulder joint. A supernumerary muscle known as accessory subscapularis muscle originates from the anterior surface of the muscle and usually inserts itself into the shoulder joint. It is a rare variation with few reports of its existence and incidence. We present a case of the accessory subscapularis muscle in a male cadaver fixated with a 10% formalin solution. The muscle passed anteriorly to the axillary nerve, thus, predisposing an individual to quadrangular space compression syndrome. We perform a review of the literature and address its clinical, anthropological and anatomical significance.",
      "doi": "https://doi.org/10.1016/j.morpho.2017.04.003",
      "year": 2017,
      "source": "PubMed"
    },
    {
      "title": "[Biomechanics of the lamina cribrosa: A determining factor in glaucomatous neuropathy. A review of the literature].",
      "authors": [
        "Claudel H",
        "Bastelica P",
        "Hamard P",
        "Labbé A",
        "Baudouin C"
      ],
      "abstract": "Glaucoma is a chronic optic neuropathy characterized by progressive sclero-laminar remodeling. The main factor at the origin of these deformations is the intraocular pressure (IOP), the effect of which varies according to the biomechanical properties of the individual lamina cribrosa (LC). In this environment, the LC represents a malleable zone of weakness within a rigid corneoscleral shell. It is a dynamic structure whose movements play a key role in the pathogenesis of glaucoma: displacing it posteriorly, in addition to contributing to the characteristic appearance of glaucomatous cupping, would increase constriction on the nerve fibers and the laminar capillaries. Often incorrectly considered permanent in adults, these deformations have a certain degree of reversibility, which is currently better characterized thanks to progress in imaging techniques. The occurrence of anterior displacement and laminar thickening following a reduction in IOP could thus constitute a good prognostic factor by reducing mechanical stress on this region. These changes would tend to reduce laminar pore tortuosity and shear forces, which are probably key mechanisms of axonal loss in glaucoma.",
      "doi": "https://doi.org/10.1016/j.jfo.2023.05.026",
      "year": 2023,
      "source": "PubMed"
    },
    {
      "title": "Dynamic NeRF: A Review",
      "authors": [
        "Jinwei Lin"
      ],
      "abstract": "Neural Radiance Field(NeRF) is an novel implicit method to achieve the 3D\nreconstruction and representation with a high resolution. After the first\nresearch of NeRF is proposed, NeRF has gained a robust developing power and is\nbooming in the 3D modeling, representation and reconstruction areas. However\nthe first and most of the followed research projects based on NeRF is static,\nwhich are weak in the practical applications. Therefore, more researcher are\ninterested and focused on the study of dynamic NeRF that is more feasible and\nuseful in practical applications or situations. Compared with the static NeRF,\nimplementing the Dynamic NeRF is more difficult and complex. But Dynamic is\nmore potential in the future even is the basic of Editable NeRF. In this\nreview, we made a detailed and abundant statement for the development and\nimportant implementation principles of Dynamci NeRF. The analysis of main\nprinciple and development of Dynamic NeRF is from 2021 to 2023, including the\nmost of the Dynamic NeRF projects. What is more, with colorful and novel\nspecial designed figures and table, We also made a detailed comparison and\nanalysis of different features of various of Dynamic. Besides, we analyzed and\ndiscussed the key methods to implement a Dynamic NeRF. The volume of the\nreference papers is large. The statements and comparisons are multidimensional.\nWith a reading of this review, the whole development history and most of the\nmain design method or principles of Dynamic NeRF can be easy understood and\ngained.",
      "doi": "arXiv:2405.08609v1",
      "year": 2024,
      "source": "arXiv"
    },
    {
      "title": "Removing Objects From Neural Radiance Fields",
      "authors": [
        "Silvan Weder",
        "Guillermo Garcia-Hernando",
        "Aron Monszpart",
        "Marc Pollefeys",
        "Gabriel Brostow",
        "Michael Firman",
        "Sara Vicente"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) are emerging as a ubiquitous scene\nrepresentation that allows for novel view synthesis. Increasingly, NeRFs will\nbe shareable with other people. Before sharing a NeRF, though, it might be\ndesirable to remove personal information or unsightly objects. Such removal is\nnot easily achieved with the current NeRF editing frameworks. We propose a\nframework to remove objects from a NeRF representation created from an RGB-D\nsequence. Our NeRF inpainting method leverages recent work in 2D image\ninpainting and is guided by a user-provided mask. Our algorithm is underpinned\nby a confidence based view selection procedure. It chooses which of the\nindividual 2D inpainted images to use in the creation of the NeRF, so that the\nresulting inpainted NeRF is 3D consistent. We show that our method for NeRF\nediting is effective for synthesizing plausible inpaintings in a multi-view\ncoherent manner. We validate our approach using a new and still-challenging\ndataset for the task of NeRF inpainting.",
      "doi": "arXiv:2212.11966v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "NAS-NeRF: Generative Neural Architecture Search for Neural Radiance\n  Fields",
      "authors": [
        "Saeejith Nair",
        "Yuhao Chen",
        "Mohammad Javad Shafiee",
        "Alexander Wong"
      ],
      "abstract": "Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but\ntheir high computational complexity limits deployability. While existing\nneural-based solutions strive for efficiency, they use one-size-fits-all\narchitectures regardless of scene complexity. The same architecture may be\nunnecessarily large for simple scenes but insufficient for complex ones. Thus,\nthere is a need to dynamically optimize the neural network component of NeRFs\nto achieve a balance between computational complexity and specific targets for\nsynthesis quality. We introduce NAS-NeRF, a generative neural architecture\nsearch strategy that generates compact, scene-specialized NeRF architectures by\nbalancing architecture complexity and target synthesis quality metrics. Our\nmethod incorporates constraints on target metrics and budgets to guide the\nsearch towards architectures tailored for each scene. Experiments on the\nBlender synthetic dataset show the proposed NAS-NeRF can generate architectures\nup to 5.74$\\times$ smaller, with 4.19$\\times$ fewer FLOPs, and 1.93$\\times$\nfaster on a GPU than baseline NeRFs, without suffering a drop in SSIM.\nFurthermore, we illustrate that NAS-NeRF can also achieve architectures up to\n23$\\times$ smaller, with 22$\\times$ fewer FLOPs, and 4.7$\\times$ faster than\nbaseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made\npublicly available at https://saeejithnair.github.io/NAS-NeRF.",
      "doi": "arXiv:2309.14293v3",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "SNeRF: Stylized Neural Implicit Representations for 3D Scenes",
      "authors": [
        "Thu Nguyen-Phuoc",
        "Feng Liu",
        "Lei Xiao"
      ],
      "abstract": "This paper presents a stylized novel view synthesis method. Applying\nstate-of-the-art stylization methods to novel views frame by frame often causes\njittering artifacts due to the lack of cross-view consistency. Therefore, this\npaper investigates 3D scene stylization that provides a strong inductive bias\nfor consistent novel view synthesis. Specifically, we adopt the emerging neural\nradiance fields (NeRF) as our choice of 3D scene representation for their\ncapability to render high-quality novel views for a variety of scenes. However,\nas rendering a novel view from a NeRF requires a large number of samples,\ntraining a stylized NeRF requires a large amount of GPU memory that goes beyond\nan off-the-shelf GPU capacity. We introduce a new training method to address\nthis problem by alternating the NeRF and stylization optimization steps. Such a\nmethod enables us to make full use of our hardware memory capacity to both\ngenerate images at higher resolution and adopt more expressive image style\ntransfer methods. Our experiments show that our method produces stylized NeRFs\nfor a wide range of content, including indoor, outdoor and dynamic scenes, and\nsynthesizes high-quality novel views with cross-view consistency.",
      "doi": "arXiv:2207.02363v1",
      "year": 2022,
      "source": "arXiv"
    },
    {
      "title": "Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields",
      "authors": [
        "Shangzan Zhang",
        "Sida Peng",
        "Yinji ShenTu",
        "Qing Shuai",
        "Tianrun Chen",
        "Kaicheng Yu",
        "Hujun Bao",
        "Xiaowei Zhou"
      ],
      "abstract": "Recently, the editing of neural radiance fields (NeRFs) has gained\nconsiderable attention, but most prior works focus on static scenes while\nresearch on the appearance editing of dynamic scenes is relatively lacking. In\nthis paper, we propose a novel framework to edit the local appearance of\ndynamic NeRFs by manipulating pixels in a single frame of training video.\nSpecifically, to locally edit the appearance of dynamic NeRFs while preserving\nunedited regions, we introduce a local surface representation of the edited\nregion, which can be inserted into and rendered along with the original NeRF\nand warped to arbitrary other frames through a learned invertible motion\nrepresentation network. By employing our method, users without professional\nexpertise can easily add desired content to the appearance of a dynamic scene.\nWe extensively evaluate our approach on various scenes and show that our\napproach achieves spatially and temporally consistent editing results. Notably,\nour approach is versatile and applicable to different variants of dynamic NeRF\nrepresentations.",
      "doi": "arXiv:2307.12909v2",
      "year": 2023,
      "source": "arXiv"
    },
    {
      "title": "Neural Radiance Fields (NeRF) for 3D Reconstruction of Monocular Endoscopic Video in Sinus Surgery.",
      "authors": [
        "Ruthberg JS",
        "Bly R",
        "Gunderson N",
        "Chen P",
        "Alighezi M",
        "Seibel EJ",
        "Abuzeid WM"
      ],
      "abstract": "To validate the use of neural radiance fields (NeRF), a state-of-the-art computer vision technique, for rapid, high-fidelity 3-dimensional (3D) reconstruction in endoscopic sinus surgery (ESS). An experimental cadaveric pilot study. Academic medical center. Complete bilateral endoscopic sinus surgery was performed on 3 cadaveric specimens, followed by postsurgical nasal endoscopy using a 0° rigid endoscope. NeRF was utilized to generate 3D reconstructions from the monocular endoscopic video feed. Reconstructions were calibrated, scaled, and then co-registered to postoperative computed tomography (CT) image sets to assess accuracy. Reconstruction error was determined by comparing ethmoid sinus measurements on NeRF reconstructions and CT image sets. NeRF-based 3D scene reconstructions were successfully generated and co-registered to corresponding CT images for 5 out of 6 cadaveric nasal cavity sides. The mean reconstruction errors and standard error of the mean (SEM) for ethmoid length and height were 0.17 (SEM 0.59) and 0.70 (SEM 0.44) mm, respectively. NeRF demonstrates significant potential for dynamic, high-fidelity 3D surgical field reconstruction in ESS, offering submillimeter accuracy comparable to postoperative CT data in cadaveric specimens. This innovative approach may ultimately augment dynamic real-time intraoperative navigation through co-registration of the 3D reconstruction with preoperative imaging to potentially reduce the risk of injury to critical structures, optimize surgical completeness and, thereby, improve surgical outcomes. Further refinement and validation in live surgical settings are necessary to fully realize its clinical utility.",
      "doi": "https://doi.org/10.1002/ohn.1105",
      "year": 2025,
      "source": "PubMed"
    }
  ]
}